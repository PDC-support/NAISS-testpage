{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to PDC Support!","text":"<p>This section provides documentation for help on how to use PDC resources. If you cannot find the information you need in the official documentation, or through a global web search please contact us. We are always glad to help, and we also appreciate suggestions for improving our documentation.</p> <p>Quick Start How to apply for an allocation How to apply for a PDC account How to get development support How to login Available software </p> <p>Site Map FAQ Contact support </p>"},{"location":"site_map/","title":"Quick start guide","text":"<ul> <li>Quick Start<ul> <li>Brief description</li> <li>How to log in</li> <li>Storage</li> <li>Transfer of files</li> <li>The Lmod module system</li> <li>The Cray programming environment</li> <li>Build your first program</li> <li>How to use EasyBuild</li> <li>Submit a batch job to the queue</li> <li>References</li> </ul> </li> </ul>"},{"location":"site_map/#introduction","title":"Introduction","text":"<ul> <li>Introduction<ul> <li>Basic Information<ul> <li>Clusters, nodes, processors, and cores</li> <li>Deciding whether you need PDC for your work</li> <li>Who may use PDC\u2019s HPC services</li> <li>Account and Time Allocation</li> <li>How much resources will be needed  corehours</li> </ul> </li> <li>Clusters at PDC</li> <li>Basic Linux for new HPC users<ul> <li>Using commands in the shell</li> <li>Useful Shell commands</li> <li>Further information</li> </ul> </li> <li>Glossary</li> </ul> </li> </ul>"},{"location":"site_map/#requesting-resources","title":"Requesting resources","text":"<ul> <li>Getting Access<ul> <li>Getting compute time</li> <li>Apply for a new Time Allocation via NAISS</li> <li>Joining an existing Time Allocation</li> <li>Check your existing Time Allocation</li> <li>Applying for an account<ul> <li>Apply via a SUPR account</li> <li>Apply via PDC webpage</li> <li>Request class access</li> </ul> </li> </ul> </li> <li>Development support</li> </ul>"},{"location":"site_map/#how-to-login","title":"How to Login","text":"<ul> <li>How to log in with SSH keys<ul> <li>How SSH key pairs work</li> <li>How to create SSH key pairs</li> <li>Authentication process details</li> <li>In the login portal</li> <li>Log in to PDC resources</li> <li>Users which do not have a SUPR account</li> <li>Configuring ssh keys and kerberos login</li> <li>Debugging</li> </ul> </li> <li>Generating SSH keys<ul> <li>Linux</li> <li>Windows</li> <li>macOS</li> </ul> </li> <li>How to log in with kerberos<ul> <li>General information about Kerberos</li> <li>Login nodes</li> <li>Step by step Login tutorial</li> <li>Troubleshooting login problems</li> <li>How to configure kerberos</li> <li>How to reset your password</li> </ul> </li> <li>Interactive HPC at PDC<ul> <li>Please note that at the moment Thinlinc is not in production yet, only in pilot phase!</li> <li>How ThinLinc can help in my computation?</li> <li>Kerberos Authentication</li> <li>SSH key based authentication</li> <li>Graphical application on Dardel</li> <li>Launching Jupyter Lab and Jupyter Notebook</li> <li>Disconnect or Logout</li> </ul> </li> </ul>"},{"location":"site_map/#how-to-run-jobs","title":"How to Run Jobs","text":"<ul> <li>How to Run Jobs<ul> <li>How jobs are scheduled</li> <li>Dardel compute nodes</li> <li>Dardel partitions</li> <li>How to submit jobs<ul> <li>Job scripts<ul> <li>Job script examples</li> <li>Job arrays</li> <li>Short jobs</li> </ul> </li> <li>Run interactively</li> <li>Running on Dardel</li> </ul> </li> </ul> </li> <li>Job Statistics<ul> <li>Viewing information about account usage</li> <li>Specifying what information is shown on the PDC system usage page</li> <li>To find information about a particular project account</li> <li>Displaying account usage information as a graph or table</li> <li>Viewing usage information per day or per month</li> <li>Viewing usage information for specific PDC systems</li> <li>To clear all current selections</li> </ul> </li> </ul>"},{"location":"site_map/#available-software","title":"Available Software","text":"<ul> <li>How to use module to load different softwares into your environment<ul> <li>Cray Programming Environment</li> <li>What softwares are installed</li> <li>What softwares are present in my environment</li> <li>How to manage software into your environment</li> <li>What parameters are set by a specific module</li> </ul> </li> <li>Instructions for using singularity at PDC<ul> <li>Security</li> <li>Performance</li> <li>Installation of singularity</li> <li>How to use singularity on your local computer</li> <li>How to remote build a singularity image on the cluster</li> <li>Running singularity images at PDC</li> </ul> </li> <li>Available Software</li> </ul>"},{"location":"site_map/#data-management","title":"Data Management","text":"<ul> <li>Data Management<ul> <li>Where to store my data</li> <li>PDC environmental variables</li> <li>Klemming<ul> <li>Performance considerations</li> <li>Managing access permissions</li> </ul> </li> <li>Swestore<ul> <li>Swestore-dCache access from PDC transfer node</li> <li>Security considerations when using rclone</li> </ul> </li> <li>File transfer<ul> <li>Some important consideration before using Swestore</li> <li>Swestore-dCache access from PDC transfer node</li> <li>Security considerations when using rclone</li> <li>Using rclone to access OneDrive</li> <li>Nodes for file operations</li> </ul> </li> </ul> </li> </ul>"},{"location":"site_map/#software-development","title":"Software Development","text":"<ul> <li>Compilers and libraries<ul> <li>The Cray Programming Environment</li> <li>Compiler wrappers</li> <li>Cray scientific and math libraries</li> <li>Cray message passing toolkit</li> <li>Compiler and linker flags</li> <li>Build examples</li> <li>References</li> </ul> </li> <li>Building for AMD GPUs<ul> <li>The AMD ROCm development platform</li> <li>Compiler and linker flags environment variables</li> <li>Build and run examples</li> <li>References, general information</li> </ul> </li> <li>Allinea Forge<ul> <li>ARM (Allinea) map</li> </ul> </li> <li>Downloadable example for compiling and submitting</li> <li>References</li> <li>Installing software using EasyBuild<ul> <li>For local installations</li> <li>How is EasyBuild configured</li> <li>How to install software using EasyBuild</li> <li>dry-run installation of software</li> <li>How to search for software using EasyBuild</li> <li>How install dependent software</li> <li>How to build easyconfig files</li> <li>Final results</li> </ul> </li> <li>Installing software using Spack<ul> <li>Setting the environment</li> <li>Finding and listing available software</li> <li>Installing software using spack</li> <li>Installing non-downloadable software</li> <li>Garbage collection</li> <li>How to execute your software</li> </ul> </li> </ul>"},{"location":"site_map/#industry","title":"Industry","text":"<ul> <li>Practical information for industry projects<ul> <li>SCANIA</li> </ul> </li> </ul>"},{"location":"site_map/#courses","title":"Courses","text":"<ul> <li>General instructions for PDC courses</li> <li>PRACE Deep Learning workshop at PDC</li> <li>Introduction to PDC</li> <li>PDC Summer School</li> <li>PDC/PRACE workshop \u201cHPC Tools for the Modern Era\u201d</li> </ul>"},{"location":"site_map/#pdc-blog","title":"PDC Blog","text":"<ul> <li>PDC Blog</li> </ul>"},{"location":"site_map/#frequently-asked-questions-faq","title":"Frequently Asked Questions  FAQ","text":"<ul> <li>Frequently Asked Questions (FAQ)<ul> <li>Filesystem</li> <li>Kerberos</li> <li>Login</li> <li>Running</li> <li>Other</li> </ul> </li> </ul>"},{"location":"site_map/#contact-support","title":"Contact Support","text":"<ul> <li>Contact Support</li> </ul>"},{"location":"basics/introduction/","title":"Introduction","text":""},{"location":"basics/introduction/#basic-information","title":"Basic Information","text":""},{"location":"basics/introduction/#clusters-nodes-processors-and-cores","title":"Clusters  nodes  processors  and cores","text":"<p>Like most high-performance computing facilities, PDC mainly features clusters. A computer cluster is in the broad sense terminology for a supercomputer, consisting of a set of connected computers working together so that they can be viewed as a single system.</p> <p>A node is the individual computer part of each cluster. Nodes are analogous to the computers we use everyday.</p> <p>And each node in turn has processors made up of computing entities called cores.</p> <p>Note</p> <p>For a more technical overview, please visit the Resources page.</p> <p></p>"},{"location":"basics/introduction/#deciding-whether-you-need-pdc-for-your-work","title":"Deciding whether you need PDC for your work","text":"<p>The key feature of PDC systems (or most HPC resources in general) is large-scale parallelization of computations. PDC resources are used in a wide-range of scientific disciplines. If you want to find out if you work could benefit from PDC resources, check if:</p> <ul> <li>your application require large computional resources</li> <li>your application require large memory resources</li> <li>your application require GPUs</li> <li>your application can be parallelized</li> </ul> <p>If you decide to use PDC resources, welcome on board! PDC provides you with:</p> <ul> <li>supercomputer systems for large simulations and calculations</li> <li>systems for processing data before and after simulations or calculations</li> <li>software for simulation and modelling</li> <li>short term storage for large volumes of data</li> <li>assistance with using PDC\u2019s computing and storage resources</li> <li>experts in different research fields to assist you with using and/or scaling software</li> </ul>"},{"location":"basics/introduction/#who-may-use-pdc-s-hpc-services","title":"Who may use PDC s HPC services","text":"<p>PDC\u2019s resources are available for both business and academic research. Companies wanting to use PDC\u2019s systems should contact business-unit@pdc.kth.se to discuss their requirements. Academic researchers (from Sweden or Europe) may apply for free time/storage allocations at PDC. Swedish academic researchers and their collaborators will need to apply for an allocation of time/storage for their projects via NAISS.</p>"},{"location":"basics/introduction/#account-and-time-allocation","title":"Account and Time Allocation","text":"<p>Before starting using PDC resources, you will need to get an account. Each user account must belong to one or more Time Allocations, since we allocate resources and manage job queueing based on the Time Allocation you belong to and not based on your individual user account. Each Time Allocation includes the following information:</p> <ol> <li>list of users belonging to that Time Allocation</li> <li>number of corehours allocated per 30 days period for all members of the Time Allocation</li> <li>an expiration date for the Time Allocation</li> <li>list of clusters available for running jobs</li> </ol> <p>When you submit jobs in a cluster, you should belong to at least one Time Allocation, or the submission will fail. Using Time Allocations allows us to:</p> <ol> <li>Prioritize your submitted jobs compared to other user\u2019s jobs.</li> <li>Keep track of compute-time used within the last 30 days by users and projects.</li> </ol>"},{"location":"basics/introduction/#how-much-resources-will-be-needed-corehours","title":"How much resources will be needed  corehours","text":"<p>At PDC, we allocate compute-time on our systems in corehours and you are granted an amount of corehours on a particular system. Corehours equals the number of cores used in how many hours. A time allocation gives you a certain amount of corehours evenly split over the months of the time allocation.</p>"},{"location":"basics/introduction/#clusters-at-pdc","title":"Clusters at PDC","text":"<p>You can find information about the clusters at PDC at https://www.pdc.kth.se/hpc-services/computing-systems</p>"},{"location":"basics/introduction/#basic-linux-for-new-hpc-users","title":"Basic Linux for new HPC users","text":"<p>Working with PDC resources requires a basic understanding of Linux systems. Some of our users are new to using Linux-based systems and have asked for introductory materials. Here is a collection for the basic command-line operations needed to get started with PDC.</p>"},{"location":"basics/introduction/#using-commands-in-the-shell","title":"Using commands in the shell","text":"<p>The shell is the program from which the user controls everything in a text-interface. When you login to a PDC system remotely, you are already in the shell window of the system. If you login to your own system, you are probably on a graphical screen. From there, search for terminal or Ctrl+Alt+T to enter the shell terminal. In the shell, you can start typing commands to perform some action. The default shell on the PDC clusters is bash.</p>"},{"location":"basics/introduction/#useful-shell-commands","title":"Useful Shell commands","text":"<p>Upon login we are greeted with the shell</p> <pre><code>ssh user@dardel.pdc.kth.se\nLast login: Fri Aug 8 10:14:59 2017 from example.com\nuser@dardel-login-1:~&gt; _\n</code></pre>"},{"location":"basics/introduction/#bash-files-and-directories","title":"Bash  Files and directories","text":"<ul> <li>Command pwd tells me where I am. After login I am in the \u201chome\u201ddirectory   <pre><code>pwd\n/cfs/klemming/home/u/user\n</code></pre></li> <li>I can change the directory with cd <pre><code>cd Private\n/Private\npwd\n/cfs/klemming/home/u/user/Private\n</code></pre></li> <li>I can go one level up with cd ..</li> <li>I can return to my HOME folder with cd</li> <li>List the contents with ls -l <pre><code>user@machine:~/tmp/talks\nls -l\ntotal 237\ndrwx------ 3 user csc-users 2048 Aug 17 15:21 img\n-rw------- 1 user csc-users 18084 Aug 17 15:21 pdc-env.html\n-rw------- 1 user csc-users 222051 Aug 17 15:22 remark-latest.min.js\n</code></pre></li> </ul>"},{"location":"basics/introduction/#bash-creating-directories-and-files","title":"Bash  creating directories and files","text":"<ul> <li>We create a new directory called results and change to it   <pre><code>mkdir results\ncd results\n</code></pre></li> <li>Creating and editing files</li> <li>Textfiles can be edited on your local computer and then transferred.</li> <li>Textfiles can also be edited locally using text editor like nano/emacs/vim.</li> </ul>"},{"location":"basics/introduction/#copying-moving-renaming-and-deleting","title":"Copying  moving renaming and deleting","text":"<pre><code># copy file\ncp draft.txt backup.txt\n# recursively copy directory\ncp -r results backup\n# move rename file\nmv draft.txt draft_2.txt\n# move rename directory\nmv results backup\n# move directory one level up\nmv results ..\n# remove file\nrm draft.txt\n# remove directory and all its contents\nrm -r results\n</code></pre>"},{"location":"basics/introduction/#file-permissions-with-chmod","title":"File permissions with chmod","text":"<p>In Linux systems all files have a user, a group and a set of privileges which determines what resources a user can access. Every file has three different kind of access: read(r), write(w) and execute(x), as well as three different kind of permissions depending on if the person is the owner(u=user) of the file, in the same group(g) or someone else(o=other).</p> <pre><code>chmod g+w file\n</code></pre> <p>Adds(+) write(w) permissions for group(g) to the file.</p> <p>There is another way to set the permissions by using numbers. Assume that each permission equals the number listed below:</p> Number Type 0 no permissions 1 execute 2 write 4 read <pre><code>chmod 753 file\n</code></pre> <p>Gives the user the read, write and execute permission(4+2+1), whereas users in the same group get read and execute permissions (4+1) while others get write and execute permissions (2+1).</p>"},{"location":"basics/introduction/#access-control-lists","title":"Access Control Lists","text":"<p>For more fine grained control of access to files, so called POSIX ACLs (Access Control Lists) can be used. An ACL allows the owner of a file or directory to control access to it on a user-by-user or group-by-group basis. To view and modify an ACL, the commands getfacl and setfacl are used.</p>"},{"location":"basics/introduction/#getfacl","title":"getfacl","text":"<p>The command getfacl is used to get file access control lists (ACLs)</p> <pre><code>getfacl dir\n# file  dir1\n# owner  lama tst\n# group  users\nuser::rwx\ngroup::r-x\nother::r-x\n</code></pre> <p>the directory dir1 is owned by the user lama-tst with permissions read(r), write(w) and execute(x). The permissions of the group and others are set to r-x which means that the group and others can read and execute.</p> <p>Note</p> <p>Use option -c to skip the comment header.</p>"},{"location":"basics/introduction/#setfacl","title":"setfacl","text":"<pre><code>setfacl -m user:jon:rwx dir1\ngetfacl -c dir1\nuser::rwx\nuser:jon:rwx\ngroup::r-x\nmask::rwx\nother::r-x\n</code></pre> <p>Now the user jon has been added and can access to dir1 with permission read(r), write(w) and execute(x).</p> <p>Note</p> <p>The options -x is used to remove entries from the ACLs of file and -R to recurse into subdirectories.</p>"},{"location":"basics/introduction/#bash-history-and-tab-completion","title":"Bash  history and tab completion","text":"<ul> <li>history preserve commands used   <pre><code>history\n689  cd ..\n691  cd Documents/\n692  cp -r introduction_PDC /cfs/klemming/home/h/hzazzi/Documents/Presentations\n693  ssh dardel.pdc.kth.se\n694  cd introduction_PDC/\n695  ls -l\n696  pwd\n697  history\n</code></pre></li> <li>If I want to repeat\u2026   <pre><code>!696\npwd\n~/Documents/introduction_PDC\n</code></pre></li> <li>Use also the TAB key for completion</li> <li>CTRL/R to search for previous commands</li> <li>Arrows up/down to scroll for earlier commands</li> </ul>"},{"location":"basics/introduction/#bash-finding-things","title":"Bash  finding things","text":"<ul> <li>Extract lines which contain an expression with grep <pre><code># extract all lines that contain searchme\ngrep searchme draft.txt\n</code></pre></li> <li>If you do not know what a UNIX command does, examine it with man <pre><code>man [command]\n</code></pre></li> <li>Find files with find <pre><code>find ~ | grep lostfile.txt\n</code></pre></li> <li>We can pipe commands and filter results with |   <pre><code>grep energy results.out | sort | uniq\n</code></pre></li> </ul>"},{"location":"basics/introduction/#bash-redirecting-output","title":"Bash  Redirecting output","text":"<ul> <li>Print content of a file to screen   <pre><code>cat test.out\n</code></pre></li> <li>Redirect output to a file   <pre><code>cat test.out &gt; myfile.txt\n</code></pre></li> <li>Append output to a file   <pre><code>cat test.out &gt;&gt; myfile.txt\n</code></pre></li> </ul>"},{"location":"basics/introduction/#bash-writing-shell-scripts","title":"Bash  Writing shell scripts","text":"<pre><code>#! bin bash\n# here we loop over all files that end with * out\nfor file in *.out; do\n  echo $file\n  cat $file\ndone\n</code></pre> <p>We make the script executable and then run it</p> <pre><code># Make it executable\nchmod u+x my_script\n# run it\n./my_script\n</code></pre>"},{"location":"basics/introduction/#arguments-to-script-can-be-passed-by-using","title":"Arguments to script can be passed by using $","text":"<p>File example</p> <pre><code>#! bin bash\necho \"Hi\" $1 $2\n</code></pre> <pre><code>./myscript Nils Nilsson\nHi Nils Nilsson\n</code></pre> <ul> <li>$1..$X:   First\u2026Xth argument</li> </ul> <p>To start executing such scripts, you would need to start with a text-editor. Choosing a text-editor is a matter of personal choice, with Vim and Emacs being traditional and popular programs. There are also many other new and interesting editors to choose from. Open your favorite text-editor and copy-paste the file example above and save with file as"},{"location":"basics/quickstart/","title":"Quick Start","text":""},{"location":"basics/quickstart/#brief-description","title":"Brief description","text":"<p>You can find documentation on our clusters at https://www.pdc.kth.se/hpc-services/computing-systems</p>"},{"location":"basics/quickstart/#how-to-log-in","title":"How to log in","text":"<p>In order to be able to login into PDC system, you need a PDC account linked with your SUPR account.</p> <ol> <li>First follow the instructions on how to generate an SSH key pair, see Generating SSH keys.</li> <li>Goto PDC login portal and follow instructions.</li> <li>Login to PDC resources\u2026    <pre><code>ssh &lt;user name&gt;@dardel.pdc.kth.se\n</code></pre></li> </ol> <p>For more details on how to log in, see How to log in with SSH keys.</p> <p>In case you do not have a SUPR account you can also How to log in with kerberos</p>"},{"location":"basics/quickstart/#storage","title":"Storage","text":"<p>The user home directories, project directories, and scratch file area are stored on a Lustre file system which is mounted to the Dardel compute and login nodes.</p> <p>The home directories have a quota of 25 GB.</p> <p>The project directories are not backed up. You are advised to always move data, and keep additional copies of the most important input and output files.</p> <p>The scratch area is intended for temporary large files that are used during calculations. The scratch area is automatically cleaned by removing files that has not been changed within a certain time. You are advised to always move data, and keep additional copies of the most important input and output files.</p> <p>For more details on storage, see Storage areas.</p>"},{"location":"basics/quickstart/#transfer-of-files","title":"Transfer of files","text":"<p>Files and directories can be transferred to and from Dardel with the scp (secure copy) command.</p> <pre><code># To transfer a file from local computer to Dardel\nscp localfile &lt;user name&gt;@dardel.pdc.kth.se:&lt;directory&gt;/.\n# To transfer a file from Dardel to local computer\nscp &lt;user name&gt;@dardel.pdc.kth.se:&lt;directory&gt;/&lt;file&gt; .\n</code></pre> <p>For more details on how to transfer files, see Using scp/rsync.</p>"},{"location":"basics/quickstart/#the-lmod-module-system","title":"The Lmod module system","text":"<p>Dardel uses the Lmod environment module system. Lmod allows for the dynamic adding and removal of installed software packages to the running environment. To access, list and search among the installed application programs, libraries, and tools, use the module command (or the shortcut ml)</p> <pre><code>ml\n# lists the loaded software modules\n\nml avail\n# lists the available software modules\n\nml avail &lt;program name&gt;\n# lists the available versions of a given software\n</code></pre> <p>Many softwares are not directly available using the above command as they are built within different Cray programming environments. To find all software and all its dependencies.</p> <pre><code>ml spider &lt;program name&gt;\n# lists the available versions of a given software and what dependent modules need to be loaded\n</code></pre> <p>When you have found the program you are looking for, use</p> <pre><code>ml &lt;program&gt;\n# to load the program module\n</code></pre> <p>Many software modules become available after loading the latest <code>PDC</code> module.</p> <pre><code>ml PDC\n</code></pre> <p>For more details on how to use modules, see How to use module to load different softwares into your environment.</p>"},{"location":"basics/quickstart/#the-cray-programming-environment","title":"The Cray programming environment","text":"<p>The Cray Programming Environment (CPE) provides a consistent interface to multiple compilers and libraries. On Dardel you can load the <code>cpe</code> module to enable a specific version of the CPE. For example</p> <pre><code>ml cpe/23.12\n</code></pre> <p>In addition to the <code>cpe</code> module, there are also the <code>PrgEnv-</code> modules that provide compilers for different programming environment</p> <ul> <li><code>PrgEnv-cray</code>: loads the Cray compiling environment (CCE) that provides compilers for Cray systems.</li> <li><code>PrgEnv-gnu</code>: loads the GNU compiler suite.</li> <li><code>PrgEnv-aocc</code>: loads the AMD AOCC compilers.</li> </ul> <p>By default the <code>PrgEnv-cray</code> is loaded upon login. You can switch to different compilers by loading another <code>PrgEnv-</code> module</p> <pre><code>ml PrgEnv-gnu\nml PrgEnv-aocc\n</code></pre> <p>After loading the <code>cpe</code> and the <code>PrgEnv-</code> modules, you can now build your parallel applications using compiler wrappers for C, C++ and Fortran</p> <pre><code>cc -o myexe.x mycode.c      # cc is the wrapper for C compiler\nCC -o myexe.x mycode.cpp    # CC is the wrapper for C++ compiler\nftn -o myexe.x mycode.f90   # ftn is the wrapper for Fortran compiler\n</code></pre> <p>The compiler wrappers will choose the required compiler version, target architecture options, and will automatically link to the math and MPI libraries. There is no need to add any <code>-I</code>, <code>-l</code> or <code>-L</code> flags for the Cray-provided libraries.</p> <ul> <li>Math libraries:</li> </ul> <p>There are the <code>cray-libsci</code> and <code>cray-fftw</code> modules   that are designed to provide optimal performance from Cray systems.  The   <code>cray-libsci</code> module provides BLAS/LAPACK/ScaLAPACK and supports OpenMP.   The number of threads can be controlled by the <code>OMP_NUM_THREADS</code>   environment variable. - MPI library:</p> <p>There is the <code>cray-mpich</code> module, which is based on ANL   MPICH and has been optimized for Cray programming environment.</p> <p>All softwares at PDC are installed using a specific CPE. The software installed using the latest CPE can be accessed by</p> <pre><code>ml PDC\n</code></pre> <p>The <code>PDC</code> modules are directly related to the CPE version and a number of older software modules can also be viewed by looking at older <code>PDC</code> modules.</p>"},{"location":"basics/quickstart/#build-your-first-program","title":"Build your first program","text":"<p>Example 1: Build an MPI parallelized Fortran code within the PrgEnv-cray environment</p> <p>In this example we build and test run a Hello World code <code>hello_world_mpi.f90</code>.</p> <pre><code>program hello_world_mpi\ninclude \"mpif.h\"\ninteger myrank,size,ierr\ncall MPI_Init(ierr)\ncall MPI_Comm_rank(MPI_COMM_WORLD,myrank,ierr)\ncall MPI_Comm_size(MPI_COMM_WORLD,size,ierr)\nwrite(*,*) \"Processor \",myrank,\" of \",size,\": Hello World!\"\ncall MPI_Finalize(ierr)\nend program\n</code></pre> <p>The build is done within the PrgEnv-cray environment using the Cray Fortran compiler, and the testing is done on a Dardel CPU node reserved for interactive use.</p> <pre><code># Check which compiler the compiler wrapper is pointing to\nftn --version\n# returns Cray Fortran   Version 15 0 1\n\n# Compile the code\nftn hello_world_mpi.f90 -o hello_world_mpi.x\n\n# Test the code in interactive session \n# First queue to get one node reserved for 10 minutes\nsalloc -N 1 -t 0:10:00 -A &lt;project name&gt; -p main\n# wait for the node  Then run the program using 128 MPI ranks with\nsrun -n 128 ./hello_world_mpi.x\n# with program output to standard out\n#    \n# Processor  123  of  128   Hello World\n#    \n# Processor  47  of  128   Hello World\n#    \n</code></pre> <p>Having here used the ftn compiler wrapper, the linking to the cray-mpich library was done without the need to specify linking flags. As is expected for this code, in runtime each MPI rank is writing its Hello World to standard output without any synchronization with the other ranks.</p> <p>Example 2: Build a C code with PrgEnv-gnu. The code requires linking to a Fourier transform library.</p> <pre><code># Download a C program that illustrates use of the FFTW library\nmkdir fftw_test\ncd fftw_test\nwget https://people.math.sc.edu/Burkardt/c_src/fftw/fftw_test.c\n\n# Change from the PrgEnv cray to the PrgEnv gnu environment\nml PDC/23.12\nml cpeGNU/23.12\n# Lmod is automatically replacing \"cpeGNU 23 12\" with \"PrgEnv gnu 8 5 0\" \n# Lmod is automatically replacing \"cce 17 0 0\" with \"gcc 12 3\" \n# Lmod is automatically replacing \"PrgEnv cray 8 5 0\" with \"cpeGNU 23 12\" \n# Due to MODULEPATH changes  the following have been reloaded \n# 1  cray libsci 23 12 5     2  cray mpich 8 1 28\n\n# Check which compiler the cc compiler wrapper is pointing to\ncc --version\ngcc-12 (SUSE Linux) 12.3.0\n\nml\n# The listing reveals that cray libsci 23 12 5 is already loaded \n\n# In addition  the program needs linking also to a Fourier transform library \nml spider fftw\n# gives a listing of available Fourier transform libraries \n# Load a recent version of the Cray FFTW library with\nml cray-fftw/3.3.10.6\n\n# Build the code with\ncc fftw_test.c -o fftw_test.x\n\n# Test the code in interactive session \n# First queue to get one reserved core for 10 minutes\nsalloc -n 1 -t 0:10:00 -A &lt;project name&gt; -p shared\n# wait for the core  Then run the program with\nsrun -n 1 ./fftw_test.x\n</code></pre> <p>Having loaded the cray-fftw module, no additional linking flag(s) was needed for the cc compiler wrapper.</p> <p>Example 3: Build a program with the EasyBuild cpeGNU/21.09 toolchain</p> <pre><code># Load an EasyBuild user module\nml PDC/23.12\nml easybuild-user/4.9.1\n\n# Look for a recipe for the Libxc library\neb -S libxc\n# Returns a list of available EasyBuild easyconfig files \n# Choose an easyconfig file for the cpeGNU 23 12 toolchain \n\n# Make a dry run\neb libxc-6.2.2-cpeGNU-23.12.eb --robot --dry-run\n\n# Check if dry run looks reasonable  Then proceed to build with\neb libxc-6.2.2-cpeGNU-23.12.eb --robot\n\n# The program is now locally installed in the user s\n# ~  local easybuild directory and can be loaded with\nml PDC/23.12\nml easybuild-user/4.9.1\nml libxc/6.2.2-cpeGNU-23.12\n</code></pre>"},{"location":"basics/quickstart/#how-to-use-easybuild","title":"How to use EasyBuild","text":"<p>At PDC we have EasyBuild installed to simplify the installation of HPC software and several easyconfig software recipes are available via the command line. In order to use EasyBuild in your local folder</p> <pre><code>ml PDC/23.12\nml easybuild-user/4.9.1\n</code></pre> <p>EasyBuild installed software will build into  ~/.local/easybuild folder and are automatically available as modules.</p> <p>For more information regarding how to EasyBuild at PDC go to Installing software using EasyBuild</p>"},{"location":"basics/quickstart/#submit-a-batch-job-to-the-queue","title":"Submit a batch job to the queue","text":"<p>PDC uses the Slurm Workload Manager to schedule jobs.</p> <p>You are advised to always submit jobs from a directory within the project and scratch partitions of the file system.</p> <p>Keep additional copies of the most important input and output files in your home directory.</p> <p>The Dardel CPU nodes have 128 cores. Please note that if you request a full node, your project allocation will be charged for use of 128 cores, even if your program uses a smaller number of cores. You are advised to submit jobs that will use fewer cores than 128 to the shared partion.</p> <p>For more details on the partition, see Dardel partitions.</p> <p>Example 1: Submit an batch job to run on 64 cores of a node that is shared with other jobs.</p> <p>In this example we will run a batch job for the <code>hello_world_mpi.f90</code> code. To this end, we prepare a jobscript.sh</p> <pre><code>#! bin bash\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A  project name \n# The name of the script is myjob\n#SBATCH  J myjob\n# 10 minutes wall clock time will be given to this job\n#SBATCH  t 00 10 00\n# The partition\n#SBATCH  p shared\n# The number of tasks requested\n#SBATCH  n 64\n# The number of cores per task\n#SBATCH  c 1\n\necho \"Script initiated at `date` on `hostname`\"\nsrun -n 64 hello_world_mpi.x\necho \"Script finished at `date` on `hostname`\"\n</code></pre> <p>The batch job is submitted to the job queue with the <code>sbatch</code> command. After submission with</p> <pre><code>sbatch jobscript.sh\n</code></pre> <p>The status of the job (pending in queue, running, etc) can be monitored with the squeue command.</p> <pre><code>squeue -u $USER\n</code></pre> <p>The standard output of the program is written to a file slurm-.out. We inspect the output <pre><code>Script initiated at thu oct 28 14:52:26 CEST 2023 on nid001064\n..\nProcessor  25  of  64 : Hello World!\nProcessor  34  of  64 : Hello World!\n..\nScript finished at thu oct 28 14:52:28 CEST 2023 on nid001064\n</code></pre> <p>For more details on how to write batch scripts and submit batch jobs to queues, see How to Run Jobs and Queueing jobs.</p> <p>Example 2: Submit a batch job to queue for a center installed software</p> <p>In this example we will perform a calculation on two Dardel CPU compute nodes with the ABINIT package for modeling of condensed matter. The example calculation is a density functional theory (DFT) simulation of the properties of the material SrVO3. ABINIT is available as a PDC center installed software, as listed on the page Available Software.</p> <p>For ABINIT can be found general information, use information and build information.</p> <p>We activate the ABINIT software module with</p> <pre><code>ml PDC/23.12\nml abinit/9.10.3-cpeGNU-23.12\n</code></pre> <p>In order to learn more about what environment variables were set by the ml command</p> <pre><code>ml show abinit/9.10.3-cpeGNU-23.12\n#which reveals that\n#  pdc software 23 12 eb software abinit 9 10 3 cpeGNU 23 12 bin\n# was appended to the PATH\n</code></pre> <p>In order to set up the simulation for SrVO3 we need an abi input file and a set of pseudopotentials for the chemical elements. These are contained in the ABINIT 9.10.3 release</p> <p>Download and extract the ABINIT 9.6.2 release</p> <pre><code>mkdir ABINIT\ncd ABINIT\nwget https://www.abinit.org/sites/default/files/packages/abinit-9.10.3.tar.gz\ntar xf abinit-9.10.3.tar.gz\n# where the files and directories needed for this example are\nabinit-9.10.3/tests/tutoparal/Input/tdmft_1.abi\nabinit-9.10.3/tests/Psps_for_tests/\n</code></pre> <p>In order to run the calculation as a batch job on two nodes, prepare a jobscriptabinit.sh where the your-project-account should be an active compute project, and Psps_for_tests should be the path to the pseudopotentials.</p> <pre><code>#! bin bash\n# time allocation\n#SBATCH  A  your project account \n# name of this job\n#SBATCH  J abinit job\n# wall time for this job\n#SBATCH  t 00 30 00\n# number of nodes\n#SBATCH   nodes=2\n# The partition\n#SBATCH  p main\n# number of MPI processes per node\n#SBATCH   ntasks per node=128\n\nml PDC/23.12\nml abinit/9.10.3-cpeGNU-23.12\n\nexport ABI_PSPDIR=&lt;Psps_for_tests&gt;\n\necho \"Script initiated at `date` on `hostname`\"\nsrun -n 256 abinit tdmft_1.abi &gt; out.log\necho \"Script finished at `date` on `hostname`\"\n</code></pre> <p>The batch job is submitted to the job queue with the sbatch command</p> <pre><code>sbatch jobscriptabinit.sh\n</code></pre> <p>The status of the job (pending in queue, running, etc) can be monitored with the squeue command.</p> <pre><code>squeue -u $USER\n</code></pre> <p>The standard output of the program was directed to the file out.log. We inspect the last 20 lines of the output</p> <pre><code>tail -n 20 out.log\n# which prints\n--- !FinalSummary\nprogram: abinit\nversion: 9.10.3\nstart_datetime: Wed Mar 13 13:06:53 2024\nend_datetime: Wed Mar 13 13:07:03 2024\noverall_cpu_time:        2631.2\noverall_wall_time:        2691.9\nexit_requested_by_user: no\ntimelimit: 0\npseudos:\n    V   : e583d1cc132dd79ce204b31204bd83ed\n    Sr  : 02b29cc3441fa9ed5e1433b119e79fbc\n    O   : c8ba4c11dba269a1224b8b74498fed92\nusepaw: 1\nmpi_procs: 256\nomp_threads: 1\nnum_warnings: 1\nnum_comments: 0\n...\n</code></pre> <p>Exercise The final summary states one warning. Search in out.log for warning messages. What do they indicate on the matching of hardware requested for the job, and the problem size?</p> <p>More details on this particular example can be found in the ABINIT tutorial on DFT+DMFT.</p>"},{"location":"basics/quickstart/#references","title":"References","text":"<p>Dardel HPE Cray EX supercomputer at PDC</p> <p>PDC Available Software</p> <p>PDC Support web pages</p> <p>HPE Cray EX product line</p> <p>The Cray programming environment (CPE)</p>"},{"location":"contact/contact_support/","title":"Contact Support","text":"<p>Before contacting PDC Support, please have a look at the Support pages to see if your questions are already addressed, and check the current System Alerts to see if there is a related issue.</p>"},{"location":"contact/contact_support/#mail-us","title":"Mail us","text":"<p>PDC uses a support Request Tracking (RT) system to handle all support requests. All incoming support mails will be tracked via the (RT) system so we can more efficiently follow up on your request and all of the PDC staff can help you in handling your issue. To get the best possible help from us when asking for support, please read this information!</p> <p>Contact support using the link https://supr.naiss.se/support/?centre_resource=c7</p> <ul> <li>If you are a SUPR user, please login to SUPR.   This method will include your background data from SUPR database, like membership in project, groups etc.</li> <li>If you are not a member of SUPR you can still contact PDC support by pressing Cannot login to SUPR and fill in the questionnaire.</li> </ul>"},{"location":"contact/contact_support/#before-sending-mail","title":"Before sending mail:","text":"<p>Make sure you provide with adequate information when requesting support from PDC. In general, try to include the following:</p> <ol> <li>Your username at PDC (this is very important if you want changes to be made to your account)</li> <li>Operating System you are accessing PDC from</li> <li>Cluster and (or) software at PDC concering your problem</li> <li>Did it stop working suddenly or after you made some changes and when was it last working?</li> <li>Execution commands used (attaching the job script is useful)</li> <li>Output from commands (You could attach with your mail the output/error files of the batch system.    Also the output of your code could contain information about the problem in case you use redirection in your shell commands.    Please, shorten files as much as possible to a length that makes it easy to see the problems while keeping messages short.)</li> </ol>"},{"location":"contact/contact_support/#after-sending-mail","title":"After sending mail:","text":"<ol> <li>As soon as you send an e-mail, you will receive an automated reply with a subject line [NAISS support #NNNNNN] which    is an unique support request number. This number will keep track of all communication we have with you about that particular issue.</li> <li>When you respond to existing requests, we strongly reccommend using the subject line including the request number [NAISS support #NNNNNN]    (simply replying back to our response would do). In our RT system, our conversation will be threaded, and communication will flow easily    back and forth between until your problem has been resolved.</li> <li>Avoid replying back to individual PDC staff. Our RT system allows all support staff to see currently open    support requests and the ensuing conversation, making sure available staff personal with expertise can provide support.</li> <li>Finally, once you have received the e-mail with a support request number, you can be sure that your original    e-mail reached PDC safely and that we will answer you as quickly as we possibly can!</li> </ol>"},{"location":"contact/contact_support/#for-subsequent-questions","title":"For subsequent questions:","text":"<ol> <li>Please refrain from using the same request number for unrelated issues: we close your request number    once the issue is resolved. If you reply to that support request again with an unrealated question,    it will be immediately re-opened with it\u2019s entire history. It causes confusion making PDC staff think that    you have continuing problems originating from the old support request and make the entire process of solving your current problem slow.</li> <li>That said, if your problem actually concerns the previous issue or response that you got from us, you should re-use that request number!</li> </ol>"},{"location":"contact/contact_support/#visit-us","title":"Visit Us","text":"<p>PDC Support is open for visitors from 09:00-17:00 Monday to Friday (except on public holidays), however, please make an appointment first, by mailing us using the information provided above, to be sure that an appropriate person will be available to meet you when you arrive. For directions to get to PDC, click here.</p> <p>PDC, KTH  Teknikringen 14, fourth floor  SE-114 28 Stockholm  Sweden </p> <p>Note</p> <p>Please be aware that it may take longer for responses from PDC Support from midsummer to the end of August. This is because many staff members take vacation during the summer period so there are fewer people available to handle queries and provide help.</p> <p>Note</p> <p>Please note that PDC Support is completely closed during the Christmas - New Year break each year. The exact dates of the closure are posted on the PDC website each year before Christmas. Normal support services usually resume just after the Epiphany public holiday.</p> <p>Note</p> <p>If any of the described method do not work for you please contact us by mailing support at support@pdc.kth.se</p>"},{"location":"courses/deeplearning/","title":"PRACE Deep Learning workshop at PDC","text":"<p>This page contains useful information and links to course material for the PRACE Deep Learning workshop at PDC.</p> <p>The agenda for the workshop can be found at https://events.prace-ri.eu/event/412/.</p>"},{"location":"courses/deeplearning/#account-and-login","title":"Account and login","text":"<p>Please see instructions for acquiring a PDC account and logging in to PDC clusters in the General instructions for PDC courses.</p> <p>The Tegner cluster will be used for the exercises on day 2 (March 21).</p>"},{"location":"courses/deeplearning/#course-material","title":"Course material","text":"<p>All course material that will be covered during the workshop can be found on the <code>kth2019</code> branch in this GitHub repository. Participants should clone the repository to a directory on the Lustre file system. For example (replace <code>&lt;initial&gt;</code> and <code>&lt;username&gt;</code>):</p> <pre><code>$ ssh -Y &lt;username&gt;@tegner.pdc.kth.se\n$ cd /cfs/klemming/nobackup/&lt;initial&gt;/&lt;username&gt;\n$ git clone https://github.com/csc-training/intro-to-dl.git\n$ cd intro-to-dl\n$ git checkout kth2019\n</code></pre>"},{"location":"courses/deeplearning/#running-a-batch-job-on-a-gpu","title":"Running a batch job on a GPU","text":"<p>We will use both K80 nodes and K420 nodes for the workshop.</p> <p>In order to submit a Tensorflow or Keras job to a K80 node, please copy-paste the following into a batch script <code>run-k80.sh</code>:</p> <pre><code>#! bin bash\n#SBATCH  N 1  c 4   gres=gpu K80 1  t 1 00 00   mem=8G\n#SBATCH  A edu19 dlprace\n#SBATCH   reservation=dlp1903\n\nmodule load anaconda/py36/5.0.1\nsource activate tensorflow1.6\n\necho Running $*\ntime python $*\n\nrm -Rf /tmp/.keras\nsource deactivate\n</code></pre> <p>A given Python script can then be submitted by:</p> <pre><code>$ sbatch run-k80.sh keras-ted-cnn.py\n</code></pre> <p>To submit a job instead to a K420 node, change the first line to:</p> <pre><code>#SBATCH  N 1  c 4   gres=gpu K420 1  t 1 00 00   mem=8G\n</code></pre> <p>For the PyTorch exercises, use this script instead:</p> <pre><code>#! bin bash\n#SBATCH  N 1  c 4   gres=gpu K80 1  t 4 00 00   mem=8G\n#SBATCH  A edu19 dlprace\n#SBATCH   reservation=dlp1903\n\nmodule load anaconda/py36/5.0.1\nsource activate pytorch\n\necho Running $*\ntime python $*\n\nsource deactivate\n</code></pre> <p>and submit a given PyTorch job:</p> <pre><code>$ sbatch run-pytorch.sh pytorch_dvc_cnn_simple.py\n</code></pre>"},{"location":"courses/general/","title":"General instructions for PDC courses","text":""},{"location":"courses/general/#get-an-account","title":"Get an account","text":"<p>Before your course/workshop starts, all participants need to have a PDC account. Please visit https://www.kth.se/form/pdc-user-account-request and request an account at least one week before the workshop starts. Specify which course/workshop you will be participating in. Once we have received your application we will confirm that you are indeed registered for the workshop and create an account for you.</p>"},{"location":"courses/general/#how-to-use-linux","title":"How to use Linux","text":"<p>You can find a short tutorial on how to use Linux by reading Basic Linux for new HPC users.</p>"},{"location":"courses/general/#preparing-for-login","title":"Preparing for login","text":"<p>Once you have received your PDC account letter you may login at PDC. General instructions on how to configure your computer for PDC access can be found by following the instructions on How to log in with kerberos. Step-by-step guides are available for How to login from Linux, How to login from Mac OS and How to login from Windows.</p> <p>If the lab exercises you are participating in take place in a computer lab room at KTH which have Ubuntu desktop computers, you are already set up for PDC access. Specific instructions are available for How to login from KTH UBUNTU computers.</p>"},{"location":"courses/general/#login-to-dardel","title":"Login to Dardel","text":"<p>Dardel is a Cray EX supercomputer which is used for the MPI, OpenMP and Performance Engineering computer labs. Each node on Dardel is equipped with 2 AMD EPYC 2.25 GHz 64 core processors, thus providing 128 physical cores on each node.</p> <p>To log in to Dardel type</p> <pre><code>$ ssh &lt;username&gt;@dardel.pdc.kth.se\n</code></pre> <p>or, if using a KTH-Ubuntu computer,</p> <pre><code>$ pdc-ssh &lt;username&gt;@dardel.pdc.kth.se\n</code></pre>"},{"location":"courses/pdcintro/","title":"Introduction to PDC","text":""},{"location":"courses/pdcintro/#account-and-login","title":"Account and login","text":"<p>Please see instructions for acquiring a PDC account and logging in to PDC clusters in the General instructions for PDC courses.</p>"},{"location":"courses/pdcintro/#course-material","title":"Course material","text":"<p>The course material for the course \u201cIntroduction to PDC\u201d can be found at https://pdc-support.github.io/hpc-intro/</p>"},{"location":"courses/prace/","title":"PDC PRACE workshop \u201cHPC Tools for the Modern Era\u201d","text":"<p>This page contains useful information and links to course material for the PDC/PRACE workshop \u201cHPC Tools for the Modern Era\u201d.</p>"},{"location":"courses/prace/#account-and-login","title":"Account and login","text":"<p>Please see instructions for acquiring a PDC account and logging in to PDC clusters in the General instructions for PDC courses.</p>"},{"location":"courses/prace/#course-material-and-installation-instructions","title":"Course material and installation instructions","text":"<p>All software needed to participate in the workshop will be provided to course participants as a virtual machine on USB sticks which will be handed out when the workshop starts. However, most of the tools can also be installed directly on your laptop.</p> <p>Links to the course material and optional installation instructions for all workshop modules (for those who want to install some of the software on their own laptops) are provided here. Note that we strongly recommend all workshop participants to install VirtualBox, see below.</p>"},{"location":"courses/prace/#virtual-machine","title":"Virtual machine","text":"<p>USB sticks containing a virtual machine with Ubuntu 18.04 will be handed out to participants when the workshop starts. The Ubuntu image contains:</p> <ul> <li>Kerberos and SSH configuration to enable login to PDC</li> <li>Singularity version 2.5.2</li> <li>A Python environment containing Jupyter Notebooks and other packages</li> <li>Arm Forge</li> </ul> <p>The VM is also available for download on KTH Box.</p> <p>Installation instructions</p> <p>In order to use this virtual machine (VM) you need to have virtualization software installed on your laptop. We recommend VirtualBox which can be installed on Windows, MacOS and Linux and is available for download here.</p> <p>The USB sticks with the workshop VM have been formatted with exFAT file system. exFAT is supported on MacOS and Windows, but on some Linux distributions one needs to install additional packages to add exFAT file system support. On Ubuntu, you need to install these two packages to be able to mount the exFAT USB sticks:</p> <pre><code>$ sudo apt-get install exfat-utils exfat-fuse\n</code></pre> <p>In order to import the workshop VM into your VirtualBox application, do the following:</p> <ol> <li>Click <code>File</code>.</li> <li>Select <code>Import Appliance</code>.</li> <li>Click the little symbol on the right of the text box and navigate to the USB device.</li> <li>Select <code>PRACE-Course.ova</code> and click <code>Open</code>.</li> <li>Click <code>Continue</code>, and then click <code>Import</code>.</li> <li>VirtualBox should now start importing the VM, which might take a couple of minutes.</li> <li>Double-click <code>PRACE_Course</code> in the left menu in order to start the VM.</li> </ol> <p>Kerberos, SSH and Jupyter can be installed on all operating systems, but Singularity requires CentOS or Ubuntu Linux distributions. If you are have MacOS or Windows on your laptop, you will therefore need to install VirtualBox. If you have CentOS or Ubuntu, you may choose to install Singularity directly on your laptop (see below), but we still strongly recommend that Linux users also install VirtualBox as a fallback option if anything doesn\u2019t work as expected.</p>"},{"location":"courses/prace/#hpc-tips-and-tricks","title":"HPC Tips and Tricks","text":"<p>The lesson material can be found at https://pdc-support.github.io/hpc-intro/.</p> <p>Installation instructions</p> <p>For this lesson you only need a PDC account, a Kerberos installation and SSH configuration to be able to log in to PDC. Further details are provided in the section General instructions for PDC courses.</p>"},{"location":"courses/prace/#jupyter-notebooks-on-the-cluster","title":"Jupyter Notebooks on the cluster","text":"<p>All lesson material can be found on GitHub: https://github.com/PDC-support/jupyter-notebook. You will be asked to clone it to your home directory at PDC by using the command:</p> <p>$ git clone https://github.com/PDC-support/jupyter-notebook.git</p> <p>(or by downloading the zip file by clicking the green \u201cClone or download\u201d button). You can also clone the material to your own laptop if you wish to play around with the notebooks.</p> <p>Installation instructions</p> <p>It is not essential to install Jupyter Notebooks on your laptop since Jupyter will be run on PDC systems in the workshop. You will however need to set up a Jupyter configuration on the Tegner cluster, set a Jupyter password and generate a self-signed certificate for safe usage of Jupyter on Tegner. Instructions for doing this can be found on this software documentation page.</p> <p>If you\u2019re interested in using Jupyter outside PDC you can follow these official instructions for installing Jupyter.</p>"},{"location":"courses/prace/#singularity","title":"Singularity","text":"<p>All lesson material can be found in a GitHub repository: https://github.com/PDC-support/singularity-introduction. The slides are served on https://gitpitch.com/PDC-support/singularity-introduction#/.</p> <p>Installation instructions</p> <p>To install Singularity on a CentOS or Ubuntu Linux distribution, follow the instructions in the Singularity documentation.</p> <p>If you have a MacOS, Windows or a non-CentOS/non-Ubuntu Linux distribution, you will be instructed to use the workshop virtual machine for the Singularity exercises.</p>"},{"location":"courses/prace/#allinea-arm","title":"Allinea Arm","text":"<p>Course material <code>available here</code>.</p> <p>Installation instructions</p> <p>Arm Forge will be installed on the workshop virtual machine and no additional software needs to be installed on your laptop.</p> <p>However, if you prefer you can install the Arm Forge Client on your laptop as well. Ubuntu users are recommended to install the full Arm Forge suite, which also functions as a remote client. Click here to download the Arm Forge installer for Ubuntu, and click here to download the Arm Forge Client installer for Windows. MacOS users are recommended to use the workshop VM.</p>"},{"location":"courses/prace/#feedback-on-workshop-lessons","title":"Feedback on workshop lessons","text":"<p>After each lesson, please provide your (very light-weight) feedback:</p> <ol> <li>HPC tips &amp; tricks</li> <li>Singularity containers</li> <li>Jupyter Notebooks</li> <li>Arm Forge</li> <li>General workshop feedback</li> </ol>"},{"location":"courses/summerschool/","title":"PDC Summer School","text":"<p>This page contains useful information for the PDC summer school.</p>"},{"location":"courses/summerschool/#account-and-login","title":"Account and login","text":"<p>Please see instructions for acquiring a PDC account and logging in to PDC clusters in the General instructions for PDC courses.</p>"},{"location":"courses/summerschool/#virtual-machine","title":"Virtual machine","text":"<p>All participants will receive a USB memory stick with an Ubuntu virtual machine (VM) image which can be used to log in to PDC. To use the VM, you will need to install VirtualBox. The image can also be downloaded here.</p> <p>Note that you can also use the VM for the Software Engineering and Singularity lessons, so we recommend all participants to install VirtualBox.</p>"},{"location":"courses/summerschool/#compiling-code","title":"Compiling code","text":"<p>Dardel have different compiler environments which you will use to compile code for the lab exercises. Detailed information can be found at https://www.pdc.kth.se/support/documents/software_development/development_intro.html, but a summary is found below.</p> <p>Dardel uses compiler wrappers so that source code is compiled with the compiler environment currently loaded. By default the <code>PrgEnv-cray</code> module is loaded when you log in, but you can change to the AMD compiler suite by</p> <pre><code>$ module swap PrgEnv-cray PrgEnv-aocc\n</code></pre> <p>and to GNU compilers with</p> <pre><code>$ module swap PrgEnv-cray PrgEnv-gnu\n</code></pre> <p>Regardless of which compiler suite is loaded, you should use the same compiler commands <code>ftn</code>, <code>cc</code> and <code>CC</code> to compile.</p> <p>Compiling serial and MPI code (note that no MPI flags are needed):</p> <pre><code># Fortran\nftn [flags] source.f90\n# C\ncc [flags] source.c\n# C++\nCC [flags] source.cpp\n</code></pre> <p>Compiling OpenMP code:</p> <pre><code>ftn -homp source.f90\ncc -fopenmp source.c\nCC -fopenmp source.cpp\n</code></pre> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>"},{"location":"courses/summerschool/#running-jobs","title":"Running jobs","text":"<p>After logging in you are on the login node of the cluster. The login node is a shared resource and should not be used for running parallel jobs. Instead, the SLURM scheduling system should be used to either run interactive jobs or submit batch jobs to the queue (see How to Run Jobs). You can however use the login node to edit files and compile code.</p> <p>To request compute resources interactively or via a batch job, you need to specify an allocation ID, a reservation ID, how much time your job needs and how many nodes it should use. To request an interactive node for 1 hour, run the command</p> <pre><code>$ salloc -A edu19.summer --reservation &lt;reservation-name&gt; -N 1 -t 1:0:0\n</code></pre> <p>When the interactive compute node is allocated to you, a new shell session is started (in the same terminal). &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD To run a parallel job on the interactive node, type:</p> <pre><code>$ srun -n 32 ./my_executable\n</code></pre> <p>$ srun -n 32 ./my_executable</p> <p>4d729ef0c05b7932d305c428695633397b7c00d4 To run a batch job, you first need to prepare a submit script. In the simplest case, it can look like this:</p> <pre><code>#! bin bash  l\n#SBATCH  A edu19 summer\n#SBATCH   reservation= reservation name \n#SBATCH  P shared\n#SBATCH  J name of my job\n#SBATCH  t 1 00 00\n#SBATCH  N 2\n</code></pre> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p> <p>srun -n 64 ./my_executable &gt; my_output_file 2&gt;&amp;1</p> <p>4d729ef0c05b7932d305c428695633397b7c00d4 To submit the job to the scheduler, type</p> <pre><code>$ sbatch my_submit_script.bash\n</code></pre> <p>You can then monitor the job using</p> <pre><code>$ squeue -u &lt;your-username&gt;\n</code></pre> <p>and if you need to cancel the job, find its job-ID using the <code>squeue</code> command and type</p> <pre><code>$ scancel &lt;job-ID&gt;\n</code></pre>"},{"location":"courses/summerschool/#file-transfer","title":"File transfer","text":"<p>Files are transferred with scp from PDC to your local machine as follows:</p> <pre><code>$ scp &lt;username&gt;@dardel.pdc.kth.se:&lt;filename&gt; .\n</code></pre> <p>or the other way by:</p> <pre><code>$ scp &lt;filename&gt; &lt;username&gt;@dardel.pdc.kth.se:~/\n</code></pre> <p>Here is more information on Using scp/rsync</p>"},{"location":"courses/summerschool/#lab-exercises","title":"Lab exercises","text":""},{"location":"courses/summerschool/#quick-reference-guide","title":"Quick Reference Guide","text":"<p>Check out our quick reference guide to using PDC resources.</p>"},{"location":"courses/summerschool/#introduction-to-pdc","title":"Introduction to PDC","text":"<p>Lecture slides can be found here.</p>"},{"location":"courses/summerschool/#software-engineering","title":"Software engineering","text":"<p>All course material, including exercises and installation/preparation steps to be done before the summer school starts, can be found on this page.</p>"},{"location":"courses/summerschool/#singularity","title":"Singularity","text":"<p>Lecture slides can be found here.</p>"},{"location":"courses/summerschool/#openmp","title":"OpenMP","text":"<p>All exercises can be found in this GitHub repository. You can either clone it to your computer or navigate the exercises on GitHub.</p>"},{"location":"courses/summerschool/#cuda","title":"CUDA","text":"<p>All exercises can be found in this GitHub repository. You can either clone it to your computer or navigate the exercises on GitHub.</p> <p>Instructions for the two sets of exercises:</p> <ul> <li>Lab 1 in C   and guidelines for solving the lab in Fortran.</li> <li>Lab 2 in C   (and use the same guidelines as above for solving it in Fortran).</li> </ul>"},{"location":"courses/summerschool/#mpi","title":"MPI","text":"<p>All exercises can be found in this GitHub repository. You can either clone it to your computer or navigate the exercises on GitHub.</p>"},{"location":"courses/summerschool/#performance-engineering","title":"Performance Engineering","text":"<p>TBD</p>"},{"location":"data_management/data_management/","title":"Data Management","text":"<p>This section gives you information about PDC\u2019s storage solutions. Working with PDC can involve transferring data back and forth between your local machine and PDC resources, or between different systems at PDC.</p> <p>If you have NAISS Swestore allocation, please check How to use Swestore section on how to transfer files to/from Swestore.</p>"},{"location":"data_management/data_management/#where-to-store-my-data","title":"Where to store my data","text":"<p>As the speed of CPU computations keep increasing, the relatively slow rate of input/output (I/O) or data accessing operations can create bottlenecks and cause programs to slow down significantly. Therefore it is very important to pay attention to how your programs are doing I/O and accessing data as that can have a huge impact on the run time of your jobs. Here, you will find a quick guide to storing data, ideal if you have just started to use PDC resources.</p>"},{"location":"data_management/data_management/#nodes-for-file-operations","title":"Nodes for file operations","text":"<p>At PDC we have a number of transfer nodes setup. These nodes are dedicated for large file transfers but also for extensive file operations involving large amount of data or many files. It is important that you use these nodes for extensive file operations as not to overload the login node.</p> <p>Dedicated transfer nodes for large file transfers will be set up on Dardel. In the meanwhile, please use the dardel.pdc.kth.se login node for the file transfers.</p> Name Type Usage dardel.pdc.kth.se Login node (Dardel) Submitting jobs and small file transfers dardel.pdc.kth.se Login node (Dardel) Large transfers and operations on the file system"},{"location":"data_management/data_management/#pdc-environmental-variables","title":"PDC environmental variables","text":"<p>To simplify for the user how to find different folders, PDC has provided a number of specific variables which indicate in which folders data should be stored. These variables are loaded by default</p> <p>Table of the environmental variables</p> Name Function Location on Dardel PDC_BACKUP Where important data are backed up. Your klemming home directory PDC_RESOURCE Name of the cluster you are logged into Dardel PDC_SITE Name of the site PDC PDC_TMP Scratch folder for storing temporary data /cfs/klemming/scratch/[U]/[USERNAME] PDC_SHUB Singularity containers folder /pdc/software/resources/sing_hub"},{"location":"data_management/file_transfer_scp/","title":"File transfer","text":""},{"location":"data_management/file_transfer_scp/#using-scp-rsync","title":"Using scp rsync","text":""},{"location":"data_management/file_transfer_scp/#using-scp-rsync-from-ubuntu-linux","title":"Using scp rsync from Ubuntu Linux","text":"<p>SCP: (secure copy) copies files between hosts on a network. It uses SSH for data transfer, and uses the same authentication and provides the same security as <code>ssh</code>. Before using <code>scp</code>, make sure you have a working SSH setup on your local machine.</p>"},{"location":"data_management/file_transfer_scp/#transferring-from-your-local-machine-to-pdc","title":"Transferring from your local machine to PDC","text":"<p>Standing in a directory on your local computer containing the file <code>localfile</code>, you can copy it to the <code>Private</code> directory on your PDC home directory <code>~</code> using the command:</p> <pre><code>scp &lt;localfile&gt; &lt;username&gt;@dardel.pdc.kth.se:~/Private/\n</code></pre> <p>where <code>username</code> is your username at PDC. For more information about what nodes to use see Nodes for file operations</p> <p>Note</p> <p>Do not use <code>rsync -a</code> When transferring directly to a project directory or the scratch area, please do not use the <code>-a</code> flag of the <code>rsync</code> command. Doing so will incorrectly set the group of the transferred files so that they will be accounted to you personal quota, rather than the project/scratch quota. In most cases it is sufficient to use <code>rsync -r</code> to transfer a directory.</p>"},{"location":"data_management/file_transfer_scp/#transferring-from-pdc-to-your-local-machine","title":"Transferring from PDC to your local machine","text":"<p>Standing in a directory on your local computer whereto you want to copy the file pdcfile from <code>/cfs/klemming/home/&lt;1st letter username&gt;/&lt;username&gt;/</code> you can transfer it using the command:</p> <pre><code>scp &lt;username&gt;@dardel.pdc.kth.se:/cfs/klemming/scratch/&lt;1st letter username&gt;/&lt;username&gt;/&lt;pdcfile&gt; .\n</code></pre> <p>where <code>username</code> is your username at PDC.</p> <p>Note</p> <p>If your <code>.bashrc</code> or other shell configuration files produce ANY output, then <code>scp</code> can fail. You can test this with the command below. If the command produces output, then you need to fix your shell configuration files so that they do not produce output.</p> <pre><code>ssh &lt;username&gt;@dardel.pdc.kth.se /bin/true\n</code></pre> <p>For more information about what nodes to use see Nodes for file operations</p>"},{"location":"data_management/file_transfer_scp/#using-scp-psftp-from-windows","title":"Using scp psftp from Windows","text":"<p>If you\u2019re using PuTTY to login to PDC clusters, you can use PSFTP or PSCP that follows the PuTTY installation. To use PSCP, you need a saved session which is used for login on Dardel. As an example, save the session with the name dardel for dardel.pdc.kth.se</p>"},{"location":"data_management/file_transfer_scp/#using-pscp","title":"Using pscp","text":"<p>To use PSCP, open up the command prompt, i.e run cmd. Use PSCP to transfer the files using the Saved Session you previously added in PuTTY. The syntax is similar to scp (i.e. -r for recursive etc).</p> <p>To transfer a file from your local computer to Lustre (if you have saved the file transfer session in PuTTY as dardel)</p> <pre><code>\"C:\\Program Files\\PuTTY\\pscp.exe\" -load dardel C:\\&lt;file to transfer&gt; &lt;username&gt;@dardel.pdc.kth.se:/cfs/klemming/home/&lt;1st letter username&gt;/&lt;username&gt;\n</code></pre>"},{"location":"data_management/file_transfer_scp/#using-psftp","title":"Using psftp","text":"<p>To start PSFTP navigate to the folder PuTTY is installed and double click on psftp.exe, or search for PSFTP on the Windows main menu. Note that just like PuTTY, you need a kerberos ticket to use PSFTP.</p> <p>If you have multiple sessions in PuTTY clicking the PSFTP executable might load the wrong settings. If that happens you have to start PSFTP from the command prompt with an argument specifying the session you created for our cluster (e.g. dardel, see above).</p> <pre><code>\"C:\\Program Files\\PuTTY\\psftp.exe\" -load dardel\n</code></pre> <p>When you have started PSFTP, a new terminal will open. Here, you first have to connect to the cluster. You can do this by typing</p> <pre><code>open &lt;username&gt;@dardel.pdc.kth.se\n</code></pre> <p>If you have followed the step from Windows Login and saved a login session (example: dardel) you can also type `` open filetransfer`` instead. At the psftp&gt; prompt you can then use the standard ftp commands (cd, lcd, get, put, \u2026).</p> <p>Now you have logged in to the cluster and you\u2019re in your AFS Home Directory. You can change your remote directory location to your cfs directory with</p> <pre><code>cd /cfs/klemming/home/&lt;1st letter username&gt;/&lt;username&gt;/&lt;folder&gt;\n</code></pre> <p>You can also change your local directory location with</p> <pre><code>lcd c:&lt;1st letter username&gt;/&lt;username&gt;/&lt;file folder&gt;\n</code></pre> <p>Keep in mind that the location should be specified in the same way you change directory on a Windows terminal, for <code>lcd</code>.</p> <p>You can transfer files by using get or put. get will transfer files specified from remote location to current local directory, and put will transfer files from current local directory to the current remote directory.</p> <pre><code>get &lt;filename&gt;\n</code></pre> <p>For more information about PSFTP utility and commands, please look at http://the.earth.li/~sgtatham/putty/0.63/htmldoc/</p>"},{"location":"data_management/file_transfer_scp/#using-scp-rsync-from-mac-os","title":"Using scp rsync from Mac OS","text":"<p><code>scp</code> and <code>rsync</code> work the same on Mac OS as they do on Linux, see information at Using scp/rsync from Ubuntu (Linux)</p>"},{"location":"data_management/file_transfer_swestore-dcache/","title":"Swestore","text":""},{"location":"data_management/file_transfer_swestore-dcache/#some-important-consideration-before-using-swestore","title":"Some important consideration before using Swestore","text":""},{"location":"data_management/file_transfer_swestore-dcache/#introduction","title":"Introduction","text":"<p>Swestore-dCache is distributed storage infrastructure. Data is stored in two copies with each copy at a different NAISS centre. To protect against silent data corruption the dCache storage system checksums all stored data and periodically verifies the data using this checksum. The dCache system does NOT yet provide protection against user errors like inadvertent file deletions.</p> <p>For more information, see NAISS Swestore documentation</p>"},{"location":"data_management/file_transfer_swestore-dcache/#requirements","title":"Requirements","text":"<p>To access Swestore-dCache, using rclone, you need to be a member of a Swestore storage project, see Getting access to Swestore. Depending on your preference and security needs, you can use rclone with either certificate or username/password authentication. We will focus on username/password in this guide.</p>"},{"location":"data_management/file_transfer_swestore-dcache/#swestore-dcache-access-from-pdc-transfer-node","title":"Swestore dCache access from PDC transfer node","text":"<ol> <li>Login to one of the transfer nodes    <pre><code># get a 7 days forwardable ticket\n$ kinit --forwardable -l 7d &lt;user&gt;@NADA.KTH.SE\n\n#login to transfer node\n$ ssh &lt;user&gt;@dardel.pdc.kth.se\n</code></pre></li> </ol> <p>Note: Please consider time required for files to transfer, and kerberos default ticket lifetime. You can request up to 30-days forwardable kerberos ticket. Considering that it might take time to do file transfers, we suggest to do it from screen or tmux. In order to keep your access to the AFS filesystem in screen  after logging out from your original ssh session you need to have started the original screen like following:    <pre><code># $ pagsh bash  c \"export KRB5CCNAME=$KRB5CCNAME; afslog; screen  S transfer\"\n</code></pre> 1. Load the PDC and rclone module    <pre><code>$ ml PDC rclone\n</code></pre> 1. Configure rclone to connect to Swestore-dCache    <pre><code>  # start configuration of \u201cnew remote\u201d\n  $ rclone config\n    No remotes found - make a new one\n    n) New remote\n    s) Set configuration password\n    q) Quit config\n    n/s/q&gt; n\n\n  # give name to \u201cnew remote\u201d\n    name&gt; swestore\n\n  # chose webdav as type  i e  40 \n    Option Storage.\n    Type of storage to configure.\n    Enter a string value. Press Enter for the default (\"\").\n    Choose a number from below, or type in your own value.\n    1 / 1Fichier\n      \\ \"fichier\"\n    2 / Alias for an existing remote\n      \\ \"alias\"\n    3 / Amazon Drive\n      \\ \"amazon cloud drive\"\n      \u2026\u2026\u2026(etc)\u2026\u2026\u2026\n    39 / Uptobox\n      \\ \"uptobox\"\n    40 / Webdav\n      \\ \"webdav\"\n    41 / Yandex Disk\n      \\ \"yandex\"\n    42 / Zoho\n      \\ \"zoho\"\n    43 / http Connection\n      \\ \"http\"\n    44 / premiumize.me\n      \\ \"premiumizeme\"\n    45 / seafile\n      \\ \"seafile\"\n    Storage&gt; 40\n\n  # provide Swestore webdav endpoint   https   webdav swestore se\n    Option url.\n    URL of http host to connect to.\n    E.g. https://example.com.\n    Enter a string value. Press Enter for the default (\"\").\n    url&gt; https://webdav.swestore.se\n\n  # choose \u201cother\u201d as vendor option\n    Option vendor.\n    Name of the Webdav site/service/software you are using.\n    Enter a string value. Press Enter for the default (\"\").\n    Choose a number from below, or type in your own value.\n    1 / Nextcloud\n      \\ \"nextcloud\"\n    2 / Owncloud\n      \\ \"owncloud\"\n    3 / Sharepoint Online, authenticated by Microsoft account\n      \\ \"sharepoint\"\n    4 / Sharepoint with NTLM authentication, usually self-hosted on-premises\n      \\ \"sharepoint-ntlm\"\n    5 / Other site/service or software\n      \\ \"other\"\n    vendor&gt; 5\n\n  # provide your Swestore username\n    Option user.\n    User name.\n    In case NTLM authentication is used, the username should be in the format 'Domain\\User'.\n    Enter a string value. Press Enter for the default (\"\").\n    user&gt; s_dejvi\n\n  # provide your Swestore password\n    Option pass.\n    Password.\n    Choose an alternative below. Press Enter for the default (n).\n    y) Yes type in my own password\n    g) Generate random password\n    n) No leave this optional password blank (default)\n    y/g/n&gt; y\n    Enter the password:\n    Password: *****************\n    Confirm the password:\n    Password: *****************\n\n  # choose NO as bearer token\n    Option bearer_token.\n    Bearer token instead of user/pass (e.g. a Macaroon).\n    Enter a string value. Press Enter for the default (\"\").\n    bearer_token&gt;\n    Edit advanced config?\n    y) Yes\n    n) No (default)\n    y/n&gt; n\n--------------------\n[swestore]\ntype = webdav\nurl = https://webdav.swestore.se\nvendor = other\nuser = s_dejvi\npass = *** ENCRYPTED ***\n--------------------\n  y) Yes this is OK (default)\n  e) Edit this remote\n  d) Delete this remote\n  y/e/d&gt; y\nCurrent remotes:\n\nName Type\n==== ====\nswestore webdav\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n</code></pre> 1. And now you can use rclone to transfer/manage files on the system. You can test it, with \u2018rclone lsd\u2019 command, by listing directories:    <pre><code>$ rclone lsd swestore:/snic/\n</code></pre></p>"},{"location":"data_management/file_transfer_swestore-dcache/#basic-rclone-commands","title":"Basic rclone commands","text":"<p>Run \u201crclone\u201d to list available commands and \u201crclone command \u2013help\u201d to see the help for that command. You can also find more information about rclone commands here https://rclone.org/commands/ or on NAISS Swestore-dCache documentation https://docs.swestore.se/</p>"},{"location":"data_management/file_transfer_swestore-dcache/#security-considerations-when-using-rclone","title":"Security considerations when using rclone","text":"<p>Your rclone configuration file, generated by interactive configuration session, is stored in your home directory.</p> <pre><code>$ rclone config file\nConfiguration file is stored at:\n/cfs/klemming/home/v/vitlacil/.config/rclone/rclone.conf\n</code></pre> <p>Important The crypt password stored in rclone.conf is lightly obscured. That only protects it from cursory inspection. It is not secure unless configuration encryption of rclone.conf is specified.</p> <p>More information on configuration encryption is available here https://rclone.org/docs/#configuration-encryption</p>"},{"location":"data_management/klemming/","title":"Klemming","text":""},{"location":"data_management/klemming/#storage-areas","title":"Storage areas","text":"<p>Klemming is divided into three main storage areas shown in the table below. Replace <code>u/username</code> by the initial letter of your username and then the full username, and <code>projectname</code> by the name of the project. Note that only the home area is backed up.</p> Area Path Alias Size File count Backup Home <code>/cfs/klemming/home/u/username</code> <code>$PDC_BACKUP</code> 25 GB 100 K Yes Projects <code>/cfs/klemming/projects/snic/projectname</code> - Varies Varies No Scratch <code>/cfs/klemming/scratch/u/username</code> <code>$PDC_TMP</code> Unlimited Unlimited No <p>Use the <code>projinfo</code> command to show the status of your projects and quotas.</p>"},{"location":"data_management/klemming/#home","title":"Home","text":"<p>Use for files that do not belong to a specific project. This area is considered personal and PDC will not grant access to anyone if not requested by the owner.  The size is limited to 25 GB and 100,000 files. The data will be available at least 6 months after the user\u2019s last allocation ended.</p>"},{"location":"data_management/klemming/#projects","title":"Projects","text":"<p>Used for most non-temporary files that are read and written by jobs running at PDC. Each project directory belongs to a NAISS storage allocation requested through SUPR, or to the default storage of a compute allocation. The amount of data and number of files that can be stored is decided by the active allocation. Once the allocation ends, the project directory can be inherited by a subsequent allocation by the same PI.</p> <p>The PI has full access to all data in the project directory. If the file system permissions do not allow the PI to access requested data, PDC will change those permissions upon request from the PI. All members should be able to write to the project directory. It is up to each member, but under the responsibility of the PI, to create suitable subdirectories with the accurate permissions that are needed by the project.</p> <p>All the data in a project directory will be deleted 3 months after the project ends. The PI will be notified prior of deletion of data through their email address registered in SUPR.</p> <p>If the space allocated to your project is starting to run out, you should first consider if some files are no longer needed and can be removed. To find out which subdirectories of your project are using the most space run the following command. However, please do not run it more than necessary as it can cause a lot of stress on the file system.</p> <pre><code>du -sh /cfs/klemming/projects/snic/my_proj/* | sort -hr\n</code></pre> <p>If you want to see how much space each member of the project is using, run the following command. Note that this can also cause stress on the file system.</p> <pre><code>find /cfs/klemming/projects/snic/my_proj -printf \"%s %u\\n\" | awk '{arr[$2]+=$1} END {for (i in arr) {print arr[i],i}}' | numfmt --to=iec --suffix=B | sort -hr\n</code></pre>"},{"location":"data_management/klemming/#project-groups","title":"Project groups","text":"<p>The project disk usage is recorded using a separate group per project. These groups are named pg-. To find the group name of a project directory called my_proj you can do: <pre><code>ls -ld /cfs/klemming/projects/snic/my_proj\n</code></pre> <p>For files to be accounted to the correct allocation, they all need to belong to that group. New files should be taken care of automatically by the permissions set on the directory in which they are stored, i.e. the set-group-ID bit. When files are moved from another location, e.g. a users nobackup directory or another project\u2019s directory, the files need to be updated manually with the new group and the user should take responsibility to update the group association directly after the files have been moved. If you have moved files into a subdirectory called new_dir somewhere in the project directory called my_proj, changing group association can be done using:</p> <pre><code>chgrp -hR --reference=/cfs/klemming/projects/snic/my_proj new_dir\nfind new_dir -type d -exec chmod g+s {} \\;\n</code></pre> <p>The same can also be done simply using the following command:</p> <pre><code>fixgroup new_dir\n</code></pre> <p>The system will periodically make sure that all the files in a project directory are associated to the right group and that all the directories have the set-group-ID bit set. This will not be done often in order to not strain the file system.</p>"},{"location":"data_management/klemming/#scratch","title":"Scratch","text":"<p>Use for temporary files that are read and written by jobs running at PDC. Examples of this could be files that are only needed during a job or checkpoint files that become obsolete after the next job starts.</p> <p>The scratch area is automatically cleaned by removing files that have not been changed in 30 days.</p>"},{"location":"data_management/klemming/#performance-considerations","title":"Performance considerations","text":"<p>Lustre file systems perform quite differently to local disks that are common on other machines. Lustre was developed for providing fast access to the large data files needed for large parallel applications. They are particularly bad at dealing with small files and with doing many small operations on these files and those cases should be avoided as much as possible.</p>"},{"location":"data_management/klemming/#good-practice-on-a-lustre-system","title":"Good practice on a Lustre system","text":"<p>To get the best performance out of a Lustre system you should use as small a number of files as possible and each time you access a file you should read/write as much data at a time as you can. An ideal program using Lustre would read in a single data file using parallel IO (e.g. MPI IO), process the data and then at the end write out a single file again using parallel IO, with no intermediate use of the disk.</p> <p>If the software is using large files, it can be beneficial to stripe them across several file servers. A common pattern is to use the following command to stripe files in the output directory across as many servers as possible:</p> <pre><code>lfs setstripe -c -1 output/\n</code></pre> <p>More information about striping is available on the Lustre wiki: https://wiki.lustre.org/Configuring_Lustre_File_Striping.</p>"},{"location":"data_management/klemming/#bad-practice-on-a-lustre-system","title":"Bad practice on a Lustre system","text":"<p>As Lustre is designed for reading a small number of large files quickly, certain IO patterns that are perfectly fine on other systems cause very high load on a Lustre system e.g.</p> <ul> <li>Small reads</li> <li>Opening many files</li> <li>Seeking within a file to read a small piece of data</li> </ul> <p>These practices are very common in applications that were designed to run on systems where each node has its own local scratch disk. Many software packages (e.g. Quantum Espresso) have input options that reduce the disk IO.</p>"},{"location":"data_management/klemming/#general-best-practices","title":"General best practices","text":"<p>In addition to these guidelines, general storage best practices should be followed:</p> <ul> <li>Minimize the number of I/O operations, since larger input/output (I/O) operations are more efficient than small ones.   If possible reads/writes should be aggregated into larger blocks.</li> <li>Avoid creating too many files, since post-processing a large number of files can be hard on the file system.</li> <li>Avoid creating directories with a large numbers of files. Instead create directory hierarchies, which also improves interactiveness.</li> </ul>"},{"location":"data_management/klemming/#other-things-to-remember-when-using-lustre","title":"Other things to remember when using Lustre","text":"<ul> <li>Avoid all unnecessary metadata operations \u2013 once a file is opened, do as much as possible before closing it again.   Do not check the existence of files or <code>stat()</code> files too often.</li> <li>Open files as read-only if possible \u2013 read-only files require less locking and therefore put less load on the file system.</li> <li>Avoid using <code>ls</code> with flags like <code>-l</code> , <code>-F</code>, or <code>--color</code>  as this requires <code>ls</code> to <code>stat()</code>   every file to determine its type, which puts an unnecessary load on the file system. Use such flags only   when the extra information is really needed and do not have them as default.</li> </ul>"},{"location":"data_management/klemming/#after-running-your-processes","title":"After running your processes","text":"<ul> <li>After performing computations at PDC, please move important data files to your own departmental storage system or to a national storage system provided by NAISS (Swestore).   Remember, space on Lustre is currently limited, and NOT backed up. However, home directories on Dardel (residing in Lustre) are backed up.</li> </ul>"},{"location":"data_management/klemming/#managing-access-permissions","title":"Managing access permissions","text":"<p>By default, only you can access the files in your home and scratch directories, and only project members can access their project directory. But sometimes it is useful to change these defaults.</p>"},{"location":"data_management/klemming/#basic-unix-permissions","title":"Basic Unix permissions","text":"<p>Each file and directory has an owner (user) and a group. The owner can decide whether the owner, members of the group, and others should be able to read (r), write (w) and execute (x) each file. The command <code>ls -l</code> displays the permissions for these respectively. For instance <code>-rw-r-----</code> means that the owner can read and write to the file, group members can read the file, and others are denied access to the file. The command also shows who is the owner and which is the group. For more details about how to work with file permissions see this page.</p>"},{"location":"data_management/klemming/#access-control-lists","title":"Access Control Lists","text":"<p>For more advanced use cases, Lustre also supports POSIX ACLs (Access Control Lists). An ACL allows the owner of a file or directory to control access to it on a user-by-user or group-by-group basis. To view and modify an ACL, the commands <code>getfacl</code> and <code>setfacl</code> are used. Detailed documentation is available by running <code>setfacl -h</code> and \u00a0<code>getfacl -h</code>.</p> <p>To view the access for a folder in Lustre, run the command:</p> <pre><code>getfacl -a /cfs/klemming/home/u/user/test\n</code></pre> <p>You might see output like this:</p> <pre><code># file   cfs klemming home u user test\n# owner  me\n# group  users\nuser::rwx\ngroup::r-x\nother::---\n</code></pre> <p>Then you can grant the access to another user by</p> <pre><code>setfacl -m u:&lt;uid&gt;:r-x -R /cfs/klemming/home/u/user/test\n</code></pre> <p>where <code>u:&lt;uid&gt;:&lt;permission&gt;</code> sets the access ACLs for a user. You can specify a user name or UID. The <code>-R</code> flag is used for recursively modifying subdirectories with the same permissions. <code>x</code> is needed to allow traversal through directories. So for a user to access your subdirectories they need the <code>x</code> permission all the way from your top directory (<code>/cfs/klemming/home/u/user</code>).</p> <p>If you want to give another user write permissions replace <code>r-x</code> with <code>rwx</code>.</p> <p>Similarly, you can grant the access to another group by</p> <pre><code>setfacl -m g:&lt;gid&gt;:r-x -R /cfs/klemming/home/u/user/test\n</code></pre> <p>where <code>g:&lt;gid&gt;:&lt;permission&gt;</code> sets the access for a group. You can specify group name or GID.</p> <p>If you want to give another group write permissions replace <code>r-x</code> with <code>rwx</code>.</p> <p>The granted permissions can be removed with the <code>-x</code> flag. The following command will remove all permission for another user.</p> <pre><code>setfacl -x u:&lt;uid&gt; -R /cfs/klemming/home/u/user/test\n</code></pre> <p>More details about POSIX ACLs can be found on this page.</p>"},{"location":"data_management/rclone_onedrive/","title":"Using rclone to access OneDrive","text":"<p>KTH provides 25 TB of cloud storage to all employees and students through Microsoft OneDrive. The cloud storage can be used for research data not actively in use at PDC, and has options for sharing data between users etc. This page describes how to transfer files between PDC and KTH OneDrive. Other universities may provide similar services.</p>"},{"location":"data_management/rclone_onedrive/#configuring-rclone","title":"Configuring rclone","text":"<p>First rclone must be installed on your local workstation. Instructions are available here: https://rclone.org/install/.</p> <p>We will use the local client to authenticate with OneDrive, and then copy the configuration file to Dardel.</p> <p>Start by creating the OneDrive remote:</p> <pre><code>rclone config create kth_onedrive onedrive\n</code></pre> <p>This command will open your web browser for you to login. Use <code>username@ug.kth.se</code> as the email address.</p> <p>Now you must set a password to encrypt the configuration file. This step is important since it is not secure to store an unencrypted configuration file on Klemming! Run the following command:</p> <pre><code>rclone config\n</code></pre> <p>Press <code>s</code> for set password, then <code>a</code> to add a new password. Choose a strong password and then press <code>q</code> to quit.</p> <p>Now identify the location of the local configuration file:</p> <pre><code>rclone config file\n</code></pre> <p>We must also identify the location of the rclone configuration file on Dardel. Login to Dardel and run the following commands:</p> <pre><code>ml PDC\nml rclone\nrclone config file\n</code></pre> <p>This should print something like:</p> <pre><code>Configuration file doesn't exist, but rclone will use this path:\n/cfs/klemming/home/x/xyz/.config/rclone/rclone.conf\n</code></pre> <p>Now we are ready to copy the local file to Dardel. Use the paths collected from the previous commands in place of <code>LOCAL_PATH</code> and <code>REMOTE_PATH</code>, and run the following on your local workstation:</p> <pre><code>rsync LOCAL_PATH dardel.pdc.kth.se:REMOTE_PATH\n</code></pre>"},{"location":"data_management/rclone_onedrive/#warning","title":"WARNING","text":"<p>Do not copy a configuration file to Dardel if it is not encrypted! Storing an unencrypted configuration file on a shared file system may enable a malicious user to take control over your data.</p> <p>To test that the setup worked, now run the following on Dardel:</p> <pre><code>rclone ls kth_onedrive:\n</code></pre> <p>This command will ask for the password you set earlier and then show a listing of the files you have in OneDrive.</p>"},{"location":"data_management/rclone_onedrive/#transferring-files","title":"Transferring files","text":"<p>To transfer files use the <code>rclone copy</code> or <code>rclone sync</code> commands. For instance, to copy the directory <code>/cfs/klemming/scratch/x/xyz/results</code> to OneDrive, use the following command:</p> <pre><code>rclone copy /cfs/klemming/scratch/x/xyz/results kth_onedrive:\n</code></pre> <p>To transfer the directory <code>indata</code> from OneDrive to Klemming, use the following command:</p> <pre><code>rclone copy kth_onedrive:indata /cfs/klemming/scratch/x/xyz\n</code></pre> <p>For larger transfers it is recommended to use the <code>-P</code> flag to enable progress indicators in the terminal.</p> <p>See the rclone usage guide for a complete reference: https://rclone.org/docs/.</p>"},{"location":"data_management/rclone_onedrive/#limitations","title":"Limitations","text":"<p>These limitations apply when moving files to OneDrive:</p> <ul> <li>Up to 25 TB per user</li> <li>Max file size 250 GB</li> <li>Case insensitive file names</li> <li>May throttle bandwidth</li> </ul> <p>If you see excessive throttling, try to using an rclone flag like <code>--user-agent \"ISV|rclone.org|rclone/v1.55.1\"</code>. If this does not work, consider getting your own client id from Microsoft: https://rclone.org/onedrive/#getting-your-own-client-id-and-key.</p>"},{"location":"faq/faq/","title":"Frequently Asked Questions  FAQ","text":""},{"location":"faq/faq/#filesystem","title":"Filesystem","text":""},{"location":"faq/faq/#i-get-disk-quota-exceeded-in-my-project-directory-even-though-the-storage-quota-is-not-full","title":"I get \u201cDisk quota exceeded\u201d in my project directory even though the storage quota is not full","text":"<p>Check your project quota with:</p> <pre><code>lfs quota -hp `stat -c \"%g\" /cfs/klemming/projects/snic/my_project` /cfs/klemming\n</code></pre> <p>Check your personal quota with:</p> <pre><code>lfs quota -hp $UID /cfs/klemming\n</code></pre>"},{"location":"faq/faq/#how-can-i-share-files-with-pdc-support","title":"How can I share files with PDC support?","text":"<p>By default only you have access to your files. But when in contact with support it can be useful to share log files, Makefiles, source code, core files etc. There are different ways to do this\u2026</p> <ol> <li>Copy the files to your public directory. The public directory is a space in your home directory where everyone has read access. This method is best for small data that is okay to share with all users of the system.    : <code>default      cp my_file $HOME/Public      cp -R my_directory $HOME/Public</code></li> <li>Give access permission to the files directly. We provide the command <code>support-access</code> to do this automatically (by setting up Access Control Lists). This method works even for large data, and gives access only to PDC staff. It does not work in project directories.    : <code>default      support-access my_file      support-access my_directory</code></li> </ol>"},{"location":"faq/faq/#kerberos","title":"Kerberos","text":""},{"location":"faq/faq/#i-got-kinit-krb5-get-init-creds-no-enc-ts-found-error-when-trying-to-run-kinit","title":"I got \u201ckinit  krb5 get init creds  No ENC TS found\u201d error when trying to run kinit","text":"<p>Write an e-mail asking PDC support (support@pdc.kth.se) to extend your Kerberos principal. When this has been done you can continue to login again using the same password as you did before. If you do not have a valid time allocation this will not be done.</p>"},{"location":"faq/faq/#how-do-i-reset-my-kerberos-password","title":"How do I reset my Kerberos password?","text":"<p>You can reset your Kerberos password (i.e. your password to log in to PDC) using the <code>kpasswd</code> command. Just type the command into a terminal and you will be prompted for your old password, and then asked to type your new password twice.</p>"},{"location":"faq/faq/#i-got-krb5-cc-new-unique-credentials-cache-file-permissions-incorrect-error-when-trying-to-run-kinit","title":"I got \u201ckrb5 cc new unique  Credentials cache file permissions incorrect\u201d error when trying to run kinit","text":"<p>Please check the output of klist command as there may be conflict between Kerberos tickets for different realms. Try kdestroy and then kinit again to get Kerberos ticket for NADA.KTH.SE.</p> <p>For windows users: If the error persists, please use Network Identity Manager to manage Kerberos tickets. Remember to set default identity by right-clicking the Kerberos principal ending with NADA.KTH.SE, and choosing the Set as default menu item.</p>"},{"location":"faq/faq/#when-using-the-kth-ubuntu-computers-i-lose-permission-to-access-my-documents-after-getting-my-kerberos-ticket","title":"When using the KTH Ubuntu computers I lose permission to access my documents after getting my Kerberos ticket","text":"<p>Both PDC systems and KTH Ubuntu systems use Kerberos authentication, but are in different realms. Replacing the login session\u2019s credentials with PDC credentials will destroy the access to your AFS home directory, typically causing applications or the entire login session to crash.</p> <p>A workaround to the issue is to use pdc-* in front of the commands needed to access PDC systems. This includes: kinit, klist, kdestroy, ssh, scp\u2026 The new commands would then be  (pdc-kinit, pdc-klist, pdc-kdestroy, pdc-ssh, pdc-scp). These commands work in the same way as the original commands, but stores/looks for the Kerberos ticket in a different location.</p>"},{"location":"faq/faq/#kerberos-tickets-without-address","title":"Kerberos tickets without address","text":"<p>If you get the error message when you connect to PDC</p> <pre><code>kinit: krb5_get_init_creds: Incorrect net address\n</code></pre> <p>or</p> <pre><code>Kerberos V5 refuses forwarded credentials because Read forwarded creds failed: Incorrect net address\n</code></pre> <p>this is a sign that you are sitting behind NAT. NAT stands for Network Address Translation and is used when you have several computers (for instance at home) who all have addresses on an internal network but are connected through the same router/base station/broadband modem and for someone looking from the outside behave as if they had been assigned the same address from the internet provider.</p> <p>That the Kerberos-server (i.e. the Kerberos KDC: Key Distribution Center) complains about \u201cIncorrect net address\u201d is because kinit creates an address with your computers internal address - but seen from the KDC it looks like your computer has the address of the NAT that you are behind.</p> <p>To resolve this, first try asking for addressless tickets from Kerberos, that is use</p> <pre><code>kinit --forwardable --no-addresses &lt;username&gt;@NADA.KTH.SE\n</code></pre> <p>This can be made default for your computer by adding the following to your Kerberos configuration file, krb5.conf:</p> <pre><code>[appdefaults]\nno-addresses = yes\n</code></pre> <p>See more information at Firewalls and kerberos</p>"},{"location":"faq/faq/#kinit-krb5-get-init-creds-incorrect-net-address","title":"kinit  krb5 get init creds  Incorrect net address","text":"<p>This is most likely caused by a NAT firewall (such as a wideband router used for most home connections).</p> <p>Remedy: Go to Firewalls and kerberos and try the \u2013no-addresses option to kinit or \u2013extra-addresses=xyz.xyz.xyz.xyz with xyz replaced by the IP number of your external NAT interface. This page should give you the address of the external NAT interface in most (but not all) cases.</p>"},{"location":"faq/faq/#kerberos-v5-mk-req-failed-server-not-found-in-kerberos-database","title":"Kerberos V5  mk req failed  Server not found in Kerberos database","text":"<p>This is most often caused by a malfunctioning name server (such as the ones provided by some home consumer ISPs)</p> <p>Remedy: You will need to add a file krb5.conf which contains a section [domain_realm] with the correct Kerberos realm information and you will need to use an environment variable to tell Heimdal the name of your config file is (if it is not /etc/krb5.conf). Add this content in the krb5.conf file:</p> <pre><code>[domain_realm]\n  .nada.kth.se = NADA.KTH.SE\n  .pdc.kth.se = NADA.KTH.SE\n</code></pre>"},{"location":"faq/faq/#kinit-krb5-get-init-creds-unable-to-reach-any-kdc-in-realm-nada-kth-se","title":"kinit  krb5 get init creds  unable to reach any KDC in realm NADA KTH SE","text":"<p>If you get this error message you are most probably behind a firewall that blocks communication with our Kerberos servers.</p> <p>Remedy: Go to Firewalls and kerberos.</p>"},{"location":"faq/faq/#cannot-find-kdc-for-requested-realm-while-getting-initial-credentials","title":"Cannot find KDC for requested realm while getting initial credentials","text":"<p>Again this likely due to a firewall.</p> <p>Remedy: Go to Firewalls and kerberos.</p>"},{"location":"faq/faq/#time-is-out-of-bounds","title":"Time is out of bounds","text":"<p>If this happens you probably have time synchronization problem:</p> <pre><code>./kinit\n&lt;username&gt;@NADA.KTH.SE's Password:\nkinit: Time is out of bounds (krb_rd_req)\n</code></pre> <p>This problem is caused by lack of synchronization between the system you create your Kerberos ticket on and the one you try to login on using that Kerberos ticket. Kerberos demands a maximum of 5 minutes time difference between the system clocks.</p> <p>Remedy: There are a number of methods to synchronize clocks between machines. The one we recommend is NTP, a protocol for synchronizing clocks over the internet. Information and software for NTP can be found online. If everything looks right, but it does not work anyway, your computer is probably set up for the wrong timezone or the wrong daylight savings time period.</p>"},{"location":"faq/faq/#kinit-krb5-get-init-creds-time-skew-370-larger-than-max-300","title":"kinit  krb5 get init creds  time skew  370  larger than max  300","text":"<p>This is again caused by the clock on your system being out of sync with the actual time.</p> <p>Remedy: See information under Time is out of bounds.</p>"},{"location":"faq/faq/#kinit-krb5-get-init-creds-clock-skew-too-great-unable-to-negotiate-a-key-exchange-method","title":"kinit  krb5 get init creds  Clock skew too great Unable to negotiate a key exchange method","text":"<p>This is again caused by the clock on your system being out of sync with the actual time.</p> <p>Remedy: See information under Time is out of bounds.</p>"},{"location":"faq/faq/#kinit-tcp-unknown-service-using-default-port-2120","title":"kinit tcp unknown service  using default port 2120","text":"<p>This is not an error message and has no impact on the functionality of Kerberos under normal circumstances. The message informs the user that the kauth/tcp system service is not registered in the client machine as a known service with an assigned port number. The kauth client program therefore selects the default \u201cstandard\u201d connection port 2120 when talking to the PDC Kerberos server. This is the wanted behavior.</p> <p>On most systems the information where the service to port look up table is located is the file /etc/services. Note that other Kerberos client programs (kx, telnet, rsh) may produce similar messages, but may use other port numbers than 2120 as the correct default.</p>"},{"location":"faq/faq/#client-s-entry-in-database-has-expired","title":"Client s entry in database has expired","text":"<p>This message indicates that your Kerberos principal has expired. This happens automatically every other year and means that you can not get any Kerberos tickets and therefore you can not login at PDC.</p> <p>Remedy: Write an e-mail asking PDC support to extend your Kerberos principal. When this has been done you can continue to login again using the same password as you did before.</p>"},{"location":"faq/faq/#login","title":"Login","text":""},{"location":"faq/faq/#when-obtaining-kerberos-credentials-i-get-the-error-preauthentication-failed","title":"When obtaining Kerberos credentials I get the error \u201cPreauthentication failed\u201d","text":"<p>This usually means that the wrong passwords was entered. Please double check that you have entered the correct password for your PDC account. If this still fails, please see I forgot my password, how can I get a new one?</p>"},{"location":"faq/faq/#i-forgot-my-password-how-can-i-get-a-new-one","title":"I forgot my password  how can I get a new one?","text":"<p>You can request a new password using our online form. The form requires you to submit a copy of a valid ID card or passport. We will send you the new via an encrypted link.</p>"},{"location":"faq/faq/#i-cannot-login-but-have-valid-kerberos","title":"I cannot login but have valid Kerberos","text":"<p>This can either be that you are not allowed to log in since you are not part of an active time allocation at PDC, or there is an error in your configuration files. First of all you should check if you are part of a time allocation on the cluster you are trying to log in to by looking it up on SUPR (if your project is managed through SUPR), or by asking your principal investigator/Course adminstrator if you have been added as a member to the time allocation. If you are a member of an active time allocation there could be some problems with your configuration files. To troubleshoot configuration file problems please try to log in manually\u2026</p> <pre><code>ssh -vvv -o GSSAPIAuthentication=yes &lt;username&gt;@&lt;cluster&gt;.pdc.kth.se\n</code></pre> <p>If this works, the original problem was probably caused by errors in the ssh configuration file.</p> <p>In order to know what ssh you are using on your computer, you can execute</p> <pre><code>which ssh\n</code></pre>"},{"location":"faq/faq/#i-have-a-pdc-account-and-want-to-link-it-to-my-supr-account","title":"I have a PDC account and want to link it to my SUPR account","text":"<p>Most projects are now managed through SUPR, and in this case your PDC accounts should be linked to a SUPR account. This will facilitate your automatic membership at PDC to projects that are available in SUPR. If you applied for an PDC account outside of SUPR, your account might not be linked. You should check if your SUPR and PDC accounts are linked and, if necessary, arrange for the accounts to be linked.</p> <p>To link your SUPR account to your PDC account\u2026</p> <p>Contact PDC support requesting that your PDC account be linked to your SUPR account. Include the following details in your email:</p> <ol> <li>Your PDC username</li> <li>Your SUPR account ID (At the top of your personal page)</li> <li>E-mail address connected to your SUPR account</li> </ol> <p>PDC Support will send you a confirmation email when your SUPR account has been linked to your PDC account.</p>"},{"location":"faq/faq/#running","title":"Running","text":""},{"location":"faq/faq/#job-submit-allocate-failed-invalid-partition-or-qos-specification","title":"Job submit allocate failed  Invalid partition or qos specification","text":"<p>Usually this depends on user not adding time allocation, or adding it wrongfully. Using salloc you should  -A  and for batch files: <pre><code>#SBATCH  A  allocationid \n</code></pre> <p>You can check the time allocations you are a member of with</p> <pre><code>projinfo [options]\n</code></pre>"},{"location":"faq/faq/#job-submit-allocate-failed-no-partition-specified-or-system-default-partition","title":"Job submit allocate failed  No partition specified or system default partition","text":"<p>In this case you have not selected which partition your job should be running on. Using salloc you should  -p  and for batch files: <pre><code>#SBATCH  p  partition \n</code></pre> <p>Here is a complete list of all Dardel partitions</p>"},{"location":"faq/faq/#i-want-to-test-pdc-resources-and-check-if-it-suits-my-needs","title":"I want to test PDC resources and check if it suits my needs","text":"<p>If you want to try out PDC resources to make sure it suits your needs, you will need to apply for a PDC account and choose time allocation: pdc-test-xxxx , the last one being the current year. By doing that you will be able to submit your jobs for an specific period of time and check if it fits your needs. If you decide to run using PDC resources you will then need to apply for a time allocation for yourself. IMPORTANT: This is only a test allocation and has very limited amount of corehours</p>"},{"location":"faq/faq/#i-do-not-get-any-e-mails-when-my-job-is-started-finished","title":"I do not get any e mails when my job is started finished","text":"<p>By default SLURM send a an e-mail when your job has started/finished and if you do not have entered any e-mail address in your script, SLURM will automatically send it to the address in  .forward so it is important that you see that this file has an updated e-mail.</p> <p>This files resides in</p> <pre><code>/cfs/klemming/home/&lt;first letter username&gt;/&lt;username&gt;/Public/.forward\n</code></pre> <p>In order to change your e-mail address you can write</p> <pre><code>echo \"&lt;YOUR NEW E-MAIL&gt;\" &gt; /cfs/klemming/home/&lt;first letter username&gt;/&lt;username&gt;/Public/.forward\n</code></pre>"},{"location":"faq/faq/#problems-to-run-multiple-jobs-or-srun-steps-at-the-same-time-on-a-compute-node","title":"Problems to run multiple jobs or srun steps at the same time on a compute node","text":"<p>When running multiple jobs on a shared node or multiple srun-steps on a full compute node, the variable <code>FI_CXI_DEFAULT_VNI</code> needs to be set before launching an executable with srun.</p> <pre><code>export FI_CXI_DEFAULT_VNI=$(od -vAn -N4 -tu &lt; /dev/urandom)\n</code></pre>"},{"location":"faq/faq/#other","title":"Other","text":""},{"location":"faq/faq/#problems-with-the-cluster","title":"Problems with the cluster","text":"<p>In case of cluster errors, or unavailability, news regard its status are also available at https://www.pdc.kth.se/cgi-bin/flash/flash.py</p>"},{"location":"getting_access/dev_support/","title":"Software Development Assistance","text":"<p>We are excited to offer time limited assistance to support your software development needs. Our goal is to help you develop software that is not only robust and reliable but also capable of running and scaling effectively on High-Performance Computing (HPC) infrastructure.</p>"},{"location":"getting_access/dev_support/#a-collaborative-endeavor","title":"A Collaborative Endeavor","text":"<p>This project is a partnership between our team and the scientists involved in your software\u2019s development. Together, we aim to create software that is optimized for HPC environments, capable of handling large-scale computations, and ready to meet the demands of cutting-edge research and industry applications. Your active participation and collaboration are essential to the success of this initiative.</p>"},{"location":"getting_access/dev_support/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Your submission will be evaluated based on the following factors:</p> <p>Maturity of the Software: We will assess the current state of your software to determine how developed and stable it is. Projects with a clear roadmap and solid foundation will be given priority.</p> <p>HPC Scalability: A crucial aspect of this support is ensuring your software can run efficiently on HPC infrastructure. We will evaluate your software's current or potential capability to scale and perform on high-performance systems.</p> <p>Availability of Developers: The availability and expertise of your development team are crucial for collaboration. Proposals demonstrating a strong and dedicated team will be viewed favorably.</p> <p>Time Requirement: The time needed to provide meaningful assistance is a key consideration. We aim to allocate our resources where they can have the most significant impact within the available timeframe.</p> <p>Open Source: We will evaluate the openness of your software, prioritizing projects that demonstrate active community engagement, transparency, and contributions.</p>"},{"location":"getting_access/dev_support/#how-to-submit-your-proposal","title":"How to Submit Your Proposal","text":"<p>In order to be granted time limited development support you need to fill in the following form and send that form to resource-request@pdc.kth.se Ensure the document is clear, concise, and accurately reflects the state of your project.</p>"},{"location":"getting_access/dev_support/#contact-information","title":"Contact Information","text":"<p>If you have any questions regarding the submission process, the evaluation criteria, or the nature of this collaboration, please reach out to us at support@pdc.kth.se.</p> <p>We look forward to reviewing your proposals and working together to advance your software project for HPC readiness!</p>"},{"location":"getting_access/get_access/","title":"Getting Access","text":""},{"location":"getting_access/get_access/#getting-compute-time","title":"Getting compute time","text":"<p>Before running on PDC, users must belong to at least one Time Allocation.</p>"},{"location":"getting_access/get_access/#are-you-a-researcher-phd-or-higher-affiliated-to-swedish-academia-and-requiring-pdc-resources","title":"Are you a researcher  PhD or higher  affiliated to swedish academia and requiring PDC resources?","text":"<p>Research projects are handled through the SUPR portal, so applying/joining Time Allocation and applying for PDC account must be done via SUPR</p> <ul> <li>If you want to start a new Time Allocation, see Apply for a new Time Allocation via NAISS</li> <li>If you are joining a research project that already has a Time Allocation at PDC, see Joining an existing Time Allocation.</li> </ul>"},{"location":"getting_access/get_access/#are-you-a-student-taking-part-in-a-course-using-pdc-resources-or-a-member-of-a-br-project-not-managed-by-supr-https-supr-naiss-se","title":"Are you a student taking part in a course using PDC resources or a member of a br  project not managed by [SUPR] https   supr naiss se  ?","text":"<p>If you\u2019re a participant of a course using PDC resources or a member of a non-SUPR project, you don\u2019t have to send in a proposal for a Time Allocation.</p> <ul> <li>If you don\u2019t have a PDC account, go directly to Apply via PDC webpage   and fill in the form.</li> <li>Do not forget to specify the course code and the name of the course responsible.     You also need to make sure you are included in the participant list managed by your course responsible.</li> <li>If you are a researcher participating in a project not in SUPR, you should specify the project name.</li> <li>If you already have a PDC account, please send us a mail at support@pdc.kth.se   and include your\u2026</li> <li>Username at PDC</li> <li>The code for the course(project you are participating in.      You also need to make sure you are included in the participant list managed by your course responsible or Principal Investigator.</li> <li>If you are a member of a project, send us your project name</li> </ul>"},{"location":"getting_access/get_access/#are-you-a-student-bachelor-or-master-level-requiring-pdc-resources-for-your-thesis","title":"Are you a student  Bachelor or Master level  requiring PDC resources for your thesis?","text":"<p>We do not accept applications directly from students below PhD student level. However, your supervisor can apply on your behalf.</p> <ol> <li>Ask your supervisor to Apply for a new Time Allocation via NAISS.</li> <li>You can then follow Joining an existing Time Allocation as a project member.</li> </ol>"},{"location":"getting_access/get_access/#are-you-an-industrial-or-business-user","title":"Are you an industrial or business user?","text":"<p>Please, contact business-unit@pdc.kth.se to discuss conditions and contracts.</p>"},{"location":"getting_access/get_access/#apply-for-a-new-time-allocation-via-naiss","title":"Apply for a new Time Allocation via NAISS","text":"<p>Most Swedish research projects are now handled at the national level in NAISS through SUPR, so managing Time Allocations, users membership and applying for PDC account should be done from your SUPR page.</p> <p>If you are applying for a new Time Allocation, you will automatically be designated as the Principal Investigator (PI). As a PI, you should decide on the\u2026</p> <ol> <li>Compute-time per month for running jobs</li> <li>Clusters intended for usage</li> <li>Duration of the project.</li> </ol> <p>Please keep in mind that the PI will apply for a Time Allocation to cover the needs of all the members in the research project. You can decide what allocation would suit the best for your project with the help of the table below:</p> Description Small allocation Medium allocation Large allocation Limit 5K corehours/month 400K corehours/month Above 400K corehours/month Applicant requirement PhD student or higher Senior scientist in Swedish academia Senior scientist in Swedish academia Application evaluation Only technical evaluation (Assistant professor or higher)Only technical evaluation (Assistant professor or higher)Scientific and technical evaluationby SNAC twice a year.Evidence of successful work on amedium allocation. <p>Read more about time allocations and rules governing them at https://SUPR.naiss.se/round/</p> <p>Once you decide on the details of your Time Allocation, Apply via a SUPR account. You can then login/signup on SUPR and submit a proposal. When you are a member of an active project, you may apply for a PDC account, in case you do not have it, directly within SUPR.</p>"},{"location":"getting_access/get_access/#joining-an-existing-time-allocation","title":"Joining an existing Time Allocation","text":"<p>If you want to join an existing Time allocation, you have to login/signup to SUPR and send an Project Membership Request using the SUPR web interface. You may then apply for a PDC account, if you do not have a PDC account, Apply via a SUPR account.</p>"},{"location":"getting_access/get_access/#check-your-existing-time-allocation","title":"Check your existing Time Allocation","text":"<p>You can see what Time Allocations you belong to in two ways:</p> <ol> <li>If you have a SUPR account, go to your SUPR page and click on the Projects tab.</li> <li>If you have a PDC account, you can login to any of our clusters and use the <code>projinfo</code> command.    It will print the information of all the allocations you belong to and information on the recent usage of the allocation.</li> </ol>"},{"location":"getting_access/get_access/#applying-for-an-account","title":"Applying for an account","text":"<p>Once you know what allocation you will belong to, you are ready to apply for a PDC account. You can apply for an account in one of following ways depending on the type of project.</p>"},{"location":"getting_access/get_access/#apply-via-a-supr-account","title":"Apply via a SUPR account","text":"<p>If you are involved in a SUPR project, the account request is managed entirely in the SUPR website. Follow the instructions below:</p> <ol> <li>Sign up or login for SUPR</li> <li>The following personal information must be entered in SUPR in order for PDC to create your account:</li> <li>Name</li> <li>E-mail</li> <li>Citizenship</li> <li>Mobile phone number</li> <li>Become a member/PI of an active project.</li> <li>Goto Accounts and press Request an account at PDC</li> <li>Fill in one or several preferred usernames</li> <li>Fill in Swedish personal number or date of birth</li> <li>Press Request account</li> <li>Information regarding account creation will be sent to the provided e-mail address</li> <li>Account information regarding username and password will be sent to you via SMS</li> </ol> <p>Changes in SUPR are automatically applied to PDC clusters overnight.</p>"},{"location":"getting_access/get_access/#apply-via-pdc-webpage","title":"Apply via PDC webpage","text":"<p>Applying via the PDC webpage is possible for users that do not have a SUPR account. This applies to users participating in courses, or users associated with external projects.</p> <p>To apply for a PDC account, you need to fill in and submit the application form linked below. Before starting with the application, please be aware that:</p> <ol> <li>You will be asked to upload a scan of your passport or an international ID card that clearly states your nationality.    Users without passport/ID can instead submit a recent printout from Skatteverket stating their nationality (personbevis)    together with a copy of a valid identification. This can be ordered online from Skatteverket.</li> <li>If you are applying for an account for a course which will use PDC    resources, you can leave the \u201cSupervisor / Project Leader\u201d field blank    and only enter the course title.</li> <li>Complete the application form at https://blackfish.pdc.kth.se/cgi-bin/accounts/request.py    and submit it.</li> </ol>"},{"location":"getting_access/get_access/#see-also","title":"SEE ALSO","text":"<p>Nationality: Why do we bother?</p> <p>If you are concerned with what we do with your nationality information or why was ask, we at PDC do not care what nationality you have. Unfortunately, some of the computer vendors and states where these computers are manufactured do care. Therefore we have to restrict citizens of a small number of nationalities (there is a list which is updated now and then) access to whole or part of our facilities.</p>"},{"location":"getting_access/get_access/#request-class-access","title":"Request class access","text":"<p>If you are the instructor of a course affiliated to a higher school education in the Stockholm area and want to request access to PDC clusters for all course participants, please complete this application below.</p> <p>Please keep in mind that we will later ask you for a complete list of all the students participating in the course and will ask you to inform your students to apply for a PDC account.</p>"},{"location":"getting_access/get_access/#request-class-access_1","title":"Request class access","text":"<p>Complete the application and submit</p>"},{"location":"industry/industry/","title":"Practical information for industry projects","text":"<p>PDC collaborates with many businesses from different market sectors (including the life sciences, manufacturing, automotive, transport and energy industries) by providing supercomputing and storage resources, as well as consultancy services, for R&amp;D projects run by industrial researchers around the world. This section covers the procedure for accessing PDC resources for industrial users.</p>"},{"location":"industry/scania/","title":"Scania","text":"<p>This page contains useful information for users from SCANIA.</p>"},{"location":"industry/scania/#get-an-account","title":"Get an account","text":"<p>To get started using PDC resources within the Scania collaboration, please begin by applying for an account at https://blackfish.pdc.kth.se/cgi-bin/accounts/request.py Specify that you want to use the system Dardel and add a comment saying that your are part of the Scania collaboration at PDC. Once we have received your application we will confirm that you are indeed a Scania user, and will proceed in creating an account for you.</p>"},{"location":"industry/scania/#login","title":"Login","text":"<p>Once you have received your PDC account letter you may login at PDC. To connect to PDC from within Scania you first need to contact Scania IT to have your computer activated for PDC access. Instructions on how to configure your computer for PDC access can be found by following these instructions on How to log in with kerberos</p>"},{"location":"industry/scania/#dardel","title":"Dardel","text":"<p>When accessing Dardel always login to the Scania dedicated login node</p> <pre><code>dardel-scania.pdc.kth.se\n</code></pre>"},{"location":"industry/scania/#login-commands","title":"Login commands","text":"<p>First generate a Kerberos ticket as follows where you should substitute username with your username at PDC\u2026</p> <pre><code>kinit -l 30d &lt;username&gt;@NADA.KTH.SE\n</code></pre> <p>Once you have a valid ticket you can ssh to dardel-scania.pdc.kth.se using</p> <pre><code>ssh -K &lt;username&gt;@dardel-scania.pdc.kth.se\n</code></pre> <p>Read more information on how to login with How to log in with kerberos</p>"},{"location":"industry/scania/#file-transfer","title":"File transfer","text":"<p>Files are transferred with scp from PDC to Scania as follows:</p> <pre><code>scp &lt;username&gt;@dardel-scania.pdc.kth.se:&lt;filename&gt; .\n</code></pre> <p>Read more information about Using scp/rsync</p>"},{"location":"industry/scania/#storage","title":"Storage","text":"<p>Scania has its own dedicated Lustre disk system, which is available from Dardel</p> <pre><code>/cfs/scania\n</code></pre> <p>Every scania user has their homedirectory at</p> <pre><code>/cfs/scania/home/&lt;1st letter username&gt;/&lt;username&gt;\n</code></pre> <p>where u is the first letter of your PDC username. You should always execute your programs in the parallel file system (Lustre). Read more information in the following Storage areas</p>"},{"location":"industry/scania/#how-to-use-linux","title":"How to use Linux","text":"<p>You can find a short tutorial on how to use Linux and how to set file permissions by reading Basic Linux for new HPC users</p>"},{"location":"industry/scania/#software","title":"Software","text":"<p>As a SCANIA user you should mainly be interested in\u2026</p> <ul> <li>PowerFLOW</li> <li>OpenFOAM</li> <li>StarCCM+</li> </ul> <p>PDC does also other applications. Please take a look at https://www.pdc.kth.se/software for a list of softwares available at PDC.</p>"},{"location":"industry/scania/#mailing-list","title":"Mailing list","text":"<p>Scania users are automatically added to the mailing list:</p> <pre><code>scania-users@pdc.kth.se\n</code></pre> <p>Both PDC staff and Scania users may post to this list to quickly distribute important information for Scania users.</p>"},{"location":"industry/scania/#contact-support","title":"Contact support","text":"<p>Scania has its personal support queue at PDC which can be reached by sending mails to</p> <pre><code>scania-support@pdc.kth.se\n</code></pre> <p>More information on how tickets are handled and some general contact information can be found at Contact Support</p>"},{"location":"login/configuration/","title":"How to configure kerberos and SSH with kerberos","text":"<p>This section describes how to configure kerberos.</p>"},{"location":"login/configuration/#configure-kerberos","title":"Configure Kerberos","text":"<p>You need to configure Kerberos so we are able to find the PDC domain.</p> <p>The configuration file for kerberos on linux and OSX that you need to edit is /etc/krb5.conf as root. If you are not able to become root on your machine, you can create a file in your home directory called for example ~/pdckrb. After this, you need to set the path for kerberos like</p> <pre><code># For bash\nexport KRB5_CONFIG=~/pdckrb/krb5.conf\n# For tcsh\nsetenv KRB5_CONFIG  ~/pdckrb/krb5.conf\n</code></pre> <p>For Windows, the kerberos file should be located at</p> <pre><code>C:\\ProgramData\\krb5.conf\n</code></pre> <p>or</p> <pre><code>C:\\ProgramData\\Kerberos\\krb5.conf\n</code></pre> <p>krb5.conf should be defined with the following entries</p> <pre><code>[domain_realm]\n  .pdc.kth.se = NADA.KTH.SE\n\n[appdefaults]\n  forwardable = yes\n  forward = yes\n  krb4_get_tickets = no\n\n[libdefaults]\n  default_realm = NADA.KTH.SE\n  dns_lookup_realm = true\n  dns_lookup_kdc = true\n</code></pre>"},{"location":"login/configuration/#acquire-kerberos-tickets","title":"Acquire kerberos tickets","text":"<p>In order to get a kerberos ticket, you first need to startup your command shell. On Windows, search for cmd.</p> <p>To acquire tickets\u2026</p> <pre><code>kinit -f &lt;PDC username&gt;@NADA.KTH.SE\n</code></pre> <p>You will be asked for your PDC password and then you have acquired your ticket.</p> <p>On Windows it is important that you run the correct version of the software, since several version can be installed by default Windows. Execute\u2026</p> <pre><code>where kinit\nc:\\windows\\system32\\kinit.exe\nc:\\program files\\heimdal\\bin\\kinit.exe\n</code></pre> <p>\u2026to find out which executable you are running. The heimdal kerberos in the program files folder or where you have installed it. In order to execute the heimdal version you have to enter the complete path.</p> <pre><code>c:\\\"program files\"\\heimdal\\bin\\kinit.exe\n</code></pre> <p>You can see what active tickets you have using</p> <pre><code>klist -f\n</code></pre> <p>Even regarding this command, it is important that you do run the heimdal kerberos and should define the right path. (See instructions above)</p> <pre><code>where klist\nc:\\windows\\system32\\klist.exe\nc:\\program files\\heimdal\\bin\\klist.exe\n</code></pre> <p>More information about kerberos can be found at http://web.mit.edu/kerberos/krb5-current/doc/user/index.html</p>"},{"location":"login/configuration/#ssh","title":"SSH","text":"<p>This section describes how to configure SSH using Kerberos. This procedure does work only for Linux and Mac. For Windows, please read information at Setting up PuTTY</p>"},{"location":"login/configuration/#ssh-without-configuration","title":"SSH without configuration","text":"<p>In order to login you need to supply these options directly to the ssh command.</p> <pre><code>ssh -o GSSAPIAuthentication=yes &lt;username&gt;@&lt;cluster&gt;.pdc.kth.se\n</code></pre>"},{"location":"login/configuration/#ssh-with-configuration","title":"SSH with configuration","text":"<p>These are instructions on the configuration file for use of SSH with Kerberos. If you are using SSH with an SSH key pair, please refer to the page How to log in with SSH keys</p> <p>OpenSSH can be configured with command line arguments or a configuration file to simplify the login procedure. The options in the configuration file are parsed in order. Create or modify the file ~/.ssh/config</p> <pre><code># Hosts we want to authenticate to with Kerberos\nHost *.kth.se *.kth.se.\n# User authentication based on GSSAPI is allowed\nGSSAPIAuthentication yes\n# Hosts to which we want to delegate credentials  Try to limit this to\n# hosts you trust  and where you really have use for forwarded tickets \nHost *.csc.kth.se *.csc.kth.se. *.nada.kth.se *.nada.kth.se. *.pdc.kth.se *.pdc.kth.se.\n# All other hosts\nHost *\n</code></pre> <p>The file should be named config, and if this is not the case, please rename it.</p> <p>Do remember to set the right permission on the file</p> <pre><code>chmod 644 ~/.ssh/config\n</code></pre> <p>After this, you can log in by using</p> <pre><code>ssh &lt;username&gt;@&lt;cluster&gt;.pdc.kth.se\n</code></pre>"},{"location":"login/configuration/#firewalls-and-kerberos","title":"Firewalls and kerberos","text":"<p>When a firewall is installed between your workstation and the computers at PDC, a special configurations described below may be necessary to use Kerberos.</p> <ol> <li>Ports used by Kerberos. Contact your system administrators and make sure that a firewall is really the problem. Kerberos uses in its standard configuration the following ports for communication:</li> </ol> Port name Port number Port type Comment kerberos 88 UDP Default configuration kerberos 88 TCP Alternative configurationsfor usage with firewalls(see below) http(used by kerberos) 80 TCP ssh 22 TCP Usually already open ftp-data 20 TCP ftp 21 TCP kpasswd 464 UDP Only for password change <p>If possible, open UDP port 88 for bidirectional communication. This is the default (and preferred) mode of operation. Otherwise continue with the next step.    After that, try to contact our authentication server with kinit as described before. 1. If there is no contact through UDP port 88, open TCP port 88 for outgoing traffic instead (if possible), and try kinit again. If it still does not work, continue with the next step. 1. The next thing to try is to get Kerberos to communicate via http over TCP port 80. This port is often open, since it is needed for surfing the web.    1. Create the Kerberos configuration file. In addition you need to add the following       <pre><code>      [realms]\nNADA.KTH.SE = {\n        kdc = kerberos.nada.kth.se\n        kdc = http/kerberos.nada.kth.se\n        kdc = kerberos-1.nada.kth.se\n        kdc = http/kerberos-1.nada.kth.se\n        kdc = kerberos-2.nada.kth.se\n        kdc = http/kerberos-2.nada.kth.se\n        admin_server = kerberos.nada.kth.se\n        }\n</code></pre></p> <pre><code>  If kinit &lt;username&gt;@NADA.KTH.SE succeeds but ssh &lt;username&gt;@hostname does not, then you\n  might want to have a look at your crendential cache with klist. If it does not contain any\n  rows that look like host/&lt;something&gt;@NADA.KTH.SE, you need to get host credentials manually.\n  That can be done with the following command for a host named hostname:\n  ```text\n  $ host hostname | awk '$3 == \"address\" {print \"host \"$4}' | bash \\\n  | awk '{sub(\".$\",\"\"); print \"kgetcred host/\"$NF\"@NADA.KTH.SE\"}' | bash\n  ```\n\n  If hostname is dardel.pdc.kth.se, after that, the output from klist should contain something like\n  ```text\n  Apr 14 16:33:11 2015  Apr 16 10:26:05 2015  host/dardel-login2.pdc.kth.se@NADA.KTH.SE\n  ```\n</code></pre> <ol> <li> <p>In some systems, all http communication (i.e. web traffic) must go through a proxy.       If that is the case, you can probably find out it\u2019s address by looking at the settings of your web       browser. If not, ask your system administrator.</p> <p>To instruct kerberos to go through the proxy, add the following line to the [libdefaults]   section of krb5.conf:   <pre><code>http_proxy = http://address.of.proxy:port\n</code></pre></p> </li> </ol>"},{"location":"login/interactive_hpc/","title":"Interactive HPC at PDC","text":""},{"location":"login/interactive_hpc/#please-note-that-at-the-moment-thinlinc-is-not-in-production-yet-only-in-pilot-phase","title":"Please note that at the moment Thinlinc is not in production yet  only in pilot phase!","text":""},{"location":"login/interactive_hpc/#introduction","title":"Introduction","text":"<p>Interactive HPC enables scientific users to run scientific computations interactively using remote desktop environment and interactive job launcher.</p>"},{"location":"login/interactive_hpc/#what-is-thinlinc","title":"What is ThinLinc?","text":"<p>ThinLinc is a remote desktop program developed by Cendio which allows users to connect to a remote server using a ThinLinc Client. It uses ssh for authentication and VNC for graphics. ThinLinc has two main components, Server and Client. Thinlinc server is setup by PDC admins to enable remote desktop environment. ThinLinc client which is available for Linux, Mac and windows can be installed by users on their laptop or desktop.</p>"},{"location":"login/interactive_hpc/#how-thinlinc-can-help-in-my-computation","title":"How ThinLinc can help in my computation?","text":"<p>A user connecting to ThinLinc server is presented with a remote desktop graphical environment where they have access to graphical applications, utilities, terminal etc. Users can create or update their files on Lustre file system, Submit interactive jobs to SLURM and can use interactive job launcher \u2018gfxlauncher\u2019 to start various graphical applications e.g. Matlab, RStudio, Mathemtica etc. and Jupyter Labs/Notebooks on Dardel.</p>"},{"location":"login/interactive_hpc/#what-is-gfxlauncher","title":"What is gfxlauncher?","text":"<p>Gfxlauncher is an application written in Python which provides users an interactive way to launch GUI applications and submit SLURM jobs. It is developed at LUNARC, Lund University Sweden. Gfxlauncher runs on top of ThinLinc to assist users in launching interactive jobs such as Jupyter Notebooks, Matlab, RStudio etc.</p>"},{"location":"login/interactive_hpc/#getting-started-with-interactive-hpc","title":"Getting started with interactive HPC","text":"<p>Users should download ThinLinc client (tlclient) which is required to connect to remote ThinLinc server. Depending upon which operating system you are using, you can download relevant installation media from Cendio download page. Once downloaded, run the installation and follow the installation dialog box.</p>"},{"location":"login/interactive_hpc/#authentication-in-thinlinc-client","title":"Authentication in ThinLinc Client","text":"<p>ThinLinc client supports multiple authentication methods. Kerberos and SSH Key based authentication is currently supported at PDC. If you are using Windows Operating system, you can only use SSH key based authentication. For Linux and Mac users, you can use Kerberos or ssh key based authentication.</p> <p>Step 01: Launch ThinLinc Client and enter Thinlinc server address and your PDC username. Please also check the box End existing session.</p> <p></p> <p>Step 02: Click on Options -&gt; Security Tab</p> <p>Step 03: Choose type of authentication, either Kerberos or Public Key.</p> <p>Read more about Kerberos Authentication How to log in with kerberos and for ssh keybased authentication How to log in with SSH keys</p> <p>Notice for Windows system:</p> <ol> <li>You can only use Public Key  authentication to connect the Thinlinc server on Windows system.</li> <li>The ThinLinc client uses OpenSSH for ssh communication with the server, that means that it cannot support PuTTy generated private keys. You have to convert the  .ppk to OpenSSH format, this can be done with PuTTYGen by loading the private key and then clicking \u201cConversions\u201d to \u201cExport OpenSSH key\u201d. You can also use the private key generated by command ssh-keygen in MS cmd</li> </ol> <p></p>"},{"location":"login/interactive_hpc/#kerberos-authentication","title":"Kerberos Authentication","text":"<p>For Kerberos authentication, obtain Kerberos ticket in terminal and then connect to ThinLinc server.</p> <pre><code>kinit username@NADA.KTH.SE\nklist\n</code></pre> <p></p>"},{"location":"login/interactive_hpc/#ssh-key-based-authentication","title":"SSH key based authentication","text":"<p>For SSH keybased authentication, choose Public Key option in security and provide path to private key.</p> <p></p> <p>Step 04: Once you have chosen the required authenticaiton method. Click Ok and return to main window of ThinLinc client. Click Connect!</p> <p>Note</p> <p>Currently there is a software bug which prompts for password authentication once you have connected to ThinLinc. You can press cancel on this dialog.</p> <p></p>"},{"location":"login/interactive_hpc/#launching-interactive-jobs-from-menu","title":"Launching Interactive jobs from Menu","text":"<p>GUI applications available on Dardel are already integrated in Remote Desktop Menu system. Users can launch Graphical applications and Jupyter Notebooks/Lab on Dardel interactively.</p>"},{"location":"login/interactive_hpc/#graphical-application-on-dardel","title":"Graphical application on Dardel","text":"<p>Applications which have a graphical interface such as Matlab, Mathematica etc. can be launched interactively using gfxlauncher.</p> <ul> <li>Click on Applications -&gt; PDC-Apps and click on required application</li> </ul> <p> * gfxlauncher menu will appear where you can choose various options for this application</p> <p> * Click on three dots to choose resources e.g. Jobname, Tasks per node and memory</p> <p> * View logs for gfxlauncher by clicking \u2018More\u2019</p> <p> * View Job Script which will be submitted for this interactive job</p> <p> * Start job by clicking \u2018Start\u2019 button and review progress in log tab.</p> <p></p>"},{"location":"login/interactive_hpc/#launching-jupyter-lab-and-jupyter-notebook","title":"Launching Jupyter Lab and Jupyter Notebook","text":"<p>You can interactively launch Jupyter Lab and Notebook on Dardel by following the steps as below;</p> <p>Step 01: Click on Applications -&gt; PDC-Jupyter -&gt; Jupyter Lab or Jupyter Notebook</p> <p></p> <p>Step 02: Configure your job parameters in the dialog box.</p> <p></p> <p>Step 03: Click Start, wait for the job to start and in few seconds a firefox browser will open with Jupyter Lab or Notebook session. If you close the firefox browser, you can connect to same Jupyter session again by clicking \u2018Reconnect to Lab\u2019.</p> <p></p> <p>Note</p> <p>Please note that we currently do not support custom Anaconda Environments for Jupyter Notebooks and Lab launched via interactive job launcher. This is work in progress and will be made available soon.</p>"},{"location":"login/interactive_hpc/#launching-interactive-jobs-from-terminal","title":"Launching interactive jobs from Terminal","text":"<p>If your application is not integrated in menu system, you can still run such application interactively via terminal in Remote desktop environment.</p> <ul> <li>Request for node allocation and note down the allocated nid number e.g. nid00xx</li> </ul> <pre><code>salloc --ntasks=2 -t 01:00:00 -p shared --qos=normal -A &lt;project&gt;\n</code></pre> <ul> <li>Login to allocated node</li> </ul> <pre><code>ssh -X nid00xx\n</code></pre> <ul> <li>Load application module and launch</li> </ul> <pre><code>module load PDC\nmodule load matlab/2022a\nmatlab\n</code></pre>"},{"location":"login/interactive_hpc/#how-to-exit-from-thinlinc-session","title":"How to exit from ThinLinc Session","text":""},{"location":"login/interactive_hpc/#disconnect-or-logout","title":"Disconnect or Logout","text":"<p>If you don\u2019t have application running in remote desktop environment, please logout from current sesssion by selecting Logout as shown below to free up ThinLinc licenses for someone else.</p> <p></p> <p>If you simply close your ThinLinc client, your session will continue to run on server. You can then reconnect via ThinLinc client and same session will resume. This however is not recommended if you don\u2019t have any active application running on the ThinLinc node since this uses up ThinLinc licenses.</p>"},{"location":"login/interactive_hpc/#faq-s","title":"FAQ s","text":"<ol> <li>Menu structure in ThinLinc client is not working as expected, what should I do?</li> </ol> <p>When you connect to ThinLinc, a special script runs and generates up to date menu entries under your home directory. These menu entries may become outdated. You can simply issue following commands in terminal to clear up current menu entries and log out / log in again to ThinLinc session to recreate updated menu structure.</p> <pre><code>rm ~/.local/share/applications/gfx-*\nrm ~/.local/share/desktop-directories/*\n</code></pre> <ol> <li>How can I request to add additional applications in Menu Integration?</li> </ol> <p>You can always open a service request at PDC to request for additional applications to be intergrated in menu system. 1. What will happen if I close ThinLinc session?</p> <p>You current session and applications will continue to run. 1. I am getting license error, what should I do?</p> <p>We currently have 10 ThinLinc client licenses. This means 11th user connecting to ThinLinc will get License error. You can reconnect after sometime to see if license becomes available. If this happens quiet often, you can send in a support request for us to consider increasing ThinLinc client licenses. 1. How to use custom anaconda environment for Jupyter Notebooks with gfxlauncher?</p> <p>This is currently work in progress and will be implemented soon.</p>"},{"location":"login/kerberos_login/","title":"How to log in with kerberos","text":"<p>This section covers the procedure for accessing PDC resources. Before following this section, make sure you have a PDC account successfully created and you have recieved your username and password.</p> <p>In order to log in to PDC computers you require:</p> <ol> <li>A Kerberos installation</li> <li>A SSH implementation that supports Kerberos.</li> </ol> <p>Logging into PDC is a two stage process. You must first generate Kerberos credentials using kinit, which requires a password, then use those credentials together with SSH to log in to cluster on which you have an active allocation.</p>"},{"location":"login/kerberos_login/#general-information-about-kerberos","title":"General information about Kerberos","text":"<p>PDC uses Kerberos authentication protocol.</p> <p></p> <p>Kerberos tickets are stored on your local machine, and are then forwarded when you try to log in to the remote system. You\u2019ll need the following software in versions that are appropriate for your operating system:</p> <ul> <li>Kerberos v5 software (from Heimdal) - which is necessary for getting a Kerberos ticket, and</li> </ul>"},{"location":"login/kerberos_login/#commonly-used-kerberos-commands","title":"Commonly used Kerberos commands","text":"<p>Here is a list of commonly used kerberos commands for users.</p> Command Description kinit kinit obtains and caches an initial ticket-granting ticket for principal.Usage <code>kinit -f [username]@NADA.KTH.SE</code> klist klist lists the Kerberos principal and Kerberos tickets held in acredentials cache, or the keys held in a keytab file. kdestroy The kdestroy utility destroys the user\u2019s active Kerberos authorizationtickets by overwriting and deleting the credentials cache that contains them.If the credentials cache is not specified, the default credentials cache is destroyed. kpasswd The kpasswd command is used to change a Kerberos principal\u2019s password.kpasswd first prompts for the current Kerberos password,then prompts the user twice for the new password, and the password is changed."},{"location":"login/kerberos_login/#login-nodes","title":"Login nodes","text":"<p>On our clusters we have several login nodes. A main one and one node used as backup in case the first one is out of commission. The login nodenames uses the following syntax:</p> Cluster Type Address Dardel Primary dardel.pdc.kth.se"},{"location":"login/kerberos_login/#step-by-step-login-tutorial","title":"Step by step Login tutorial","text":"<p>For step-by-step tutorials on how to login, choose from below the operating system of your local computer from which you want to access PDC resources. If you face any trouble logging in or need further help, feel free to Contact Support.</p> <ul> <li>How to login from Linux</li> <li>How to login from Windows</li> <li>Install and configure Kerberos and ssh for Windows</li> <li>Install and configure Windows Subsystem for Linux (WSL)</li> <li>How to login from Mac OS</li> <li>KTH Mac OS X</li> <li>Own Mac OS X</li> <li>Additional note:</li> <li>How to reset your Kerberos password</li> <li>Linux, OSX, Windows Subsystem for Linux (WSL)</li> <li>Windows</li> <li>How to configure kerberos and SSH with kerberos</li> <li>Configure Kerberos</li> <li>Acquire kerberos tickets</li> <li>SSH</li> <li>Firewalls and kerberos</li> </ul>"},{"location":"login/kerberos_login/#troubleshooting-login-problems","title":"Troubleshooting login problems","text":"<p>A lot of solutions for login errors can be found in our FAQ section at Kerberos or at Login</p> <p>If you do not find a solution, please Contact Support with the following information:</p> <ul> <li>Username</li> <li>Which operating system and version you are using</li> <li>Any output/error message you got from   <pre><code>kinit -f &lt;username&gt;@NADA.KTH.SE\n</code></pre></li> <li>The output from   <pre><code>klist -f\n</code></pre></li> <li>The output from   <pre><code>ssh -vvv -o GSSAPIAuthentication=yes &lt;PDC username&gt;@&lt;cluster&gt;.pdc.kth.se\n</code></pre></li> </ul>"},{"location":"login/linux_login/","title":"How to login from Linux","text":"<p>There are various flavours of Linux and installing the software does differ between distributions. Configuration of kerberos is however similar in Linux distributions</p>"},{"location":"login/linux_login/#how-to-login-from-ubuntu-debian","title":"How to login from Ubuntu Debian","text":"<p>This section describes how to acquire kerberos tickets and login</p>"},{"location":"login/linux_login/#installing-kerberos","title":"Installing Kerberos","text":"<p>In Ubuntu, kinit and ssh are in the packages heimdal-clients and openssh-client. Install these packages with your favorite package manager or by executing</p> <pre><code>sudo apt-get install heimdal-clients\nsudo apt-get install openssh-client\n</code></pre> <p>For additional information goto How to configure kerberos and SSH with kerberos</p>"},{"location":"login/linux_login/#how-to-login-from-kth-ubuntu-computers","title":"How to login from KTH UBUNTU computers","text":"<p>KTH Ubuntu already has the necessary software and configuration in place, but the command are working a bit differently so that users can access both their KTH and PDC home folder.</p> <p>Most kerberos and ssh commands have a special script starting with pdc- Please not that the PDC password is different from the KTH password.</p> <p>To acquire a Kerberos ticket</p> <pre><code>pdc-kinit\n</code></pre> <p>You can see what active tickets you have by</p> <pre><code>pdc-klist -f\n</code></pre> <p>The output of this command should look something like</p> <pre><code>Credentials cache: FILE:/tmp/krb5cc_2140015_BcRCkm.pdc\nDefault principal: &lt;your-username&gt;@NADA.KTH.SE\n\nValid starting       Expires              Flags   Service principal\n07/02/2018 11:17:58  07/09/2018 11:17:50  FIA     krbtgt/NADA.KTH.SE@NADA.KTH.SE\n07/02/2018 11:43:01  07/09/2018 11:17:50  FIA     afs/pdc.kth.se@NADA.KTH.SE\n</code></pre> <p>To login into a cluster</p> <pre><code>pdc-ssh &lt;cluster&gt;.pdc.kth.se\n</code></pre> <p>Other commands</p> <pre><code># Destroy tickets\npdc-kdestroy\n# Copy files\npdc-scp &lt;localfile&gt; &lt;username&gt;@t04n28.pdc.kth.se:~/Private/\n</code></pre>"},{"location":"login/linux_login/#how-to-login-from-fedora-centos-rhel","title":"How to login from Fedora CentOs RHEL","text":"<p>This section describes how to acquire kerberos tickets and login</p>"},{"location":"login/linux_login/#installing-kerberos_1","title":"Installing Kerberos","text":"<p>For Fedora packages krb5-workstation and openssh-clients are needed. Install these packages with your favorite package manager or by executing as root</p> <pre><code>yum install krb5-workstation openssh-clients\n</code></pre> <p>These packages may already be installed. If there are updates available, we recommend you to update it (yum will ask you for it).</p> <p>Be aware that the MIT kinit shipped with Fedora differs from the Heimdal kinit and that the MIT kerberos library does not have as many features to break out of firewalls as Heimdal kerberos does.</p> <p>For additional information goto How to configure kerberos and SSH with kerberos</p>"},{"location":"login/linux_login/#how-to-login-from-suse","title":"How to login from SUSE","text":"<p>This section describes how to acquire kerberos tickets and login.</p>"},{"location":"login/linux_login/#installing-kerberos_2","title":"Installing Kerberos","text":"<p>For SUSE you need to install heimdal and a patched ssh to get GSSAPI-keyExchange to work. Download and install a suitable rpm from https://pkgserver.pdc.kth.se/pub/krb/contrib/opensuse/</p> <p>Follow instruction for How to configure kerberos and SSH with kerberos</p> <p>You might also need to run the command</p> <pre><code>/sbin/ldconfig\n</code></pre>"},{"location":"login/linux_login/#how-to-login-from-other-linux-distributions","title":"How to login from other Linux distributions","text":"<p>This section describes how to acquire kerberos tickets and login</p>"},{"location":"login/linux_login/#installing-kerberos_3","title":"Installing Kerberos","text":"<p>In order to access the computers at PDC in a secure way you have to install some variant of Kerberos binaries.</p> <ul> <li>Gentoo: Install app-crypt/heimdal and net-misc/openssh</li> <li>FreeBSD: generally comes with Kerberos pre-installed (in ports).</li> <li>Solaris: At NADA/CSC: module add heimdal/latest, otherwise use the ssh shipped with Solaris.</li> <li>Archlinux: Gssapi patch for OpenSSH needed. Either download source code and compile yourself or try this method:</li> </ul> <pre><code>install debtap from AUR //if you are using yaourt use yaourt -S debtap\nDownload https://packages.debian.org/testing/amd64/openssh-client/download\ndebtap &lt;package&gt;.deb\nsudo pacman -U &lt;package&gt;.pkg\n</code></pre> <p>If you need support for a Unix dialect that is missing, please Contact Support for additional information.</p> <p>For configure information regarding your setup goto How to configure kerberos and SSH with kerberos</p> <ul> <li>How to configure kerberos and SSH with kerberos</li> <li>Configure Kerberos</li> <li>Acquire kerberos tickets</li> <li>SSH<ul> <li>SSH without configuration</li> <li>SSH with configuration</li> </ul> </li> <li>Firewalls and kerberos</li> </ul>"},{"location":"login/mac_login/","title":"How to login from Mac OS","text":"<p>This section describes how to acquire Kerberos tickets and log in from different versions of Mac OS X.</p> <p>If you are using SSH with an SSH key pair, please refer to the page How to log in with SSH keys</p>"},{"location":"login/mac_login/#kth-mac-os-x","title":"KTH Mac OS X","text":"<p>In case you are using a Mac computer installed by KTH, everything should be installed. In case of any problems please contact it-support@kth.se</p> <p>Otherwise follow instructions below.</p>"},{"location":"login/mac_login/#own-mac-os-x","title":"Own Mac OS X","text":"<p>First get Kerberos tickets using default <code>kinit</code> (full path <code>/usr/bin/kinit</code>):</p> <pre><code>kinit your-username@NADA.KTH.SE\n</code></pre> <p>Check that valid tickets exist:</p> <pre><code>klist -f\n</code></pre> <p>You should get a similar output as the following one:</p> <pre><code>Credentials cache: API:0E4B40BC-F22B-43B8-87E2-BA13538CF042\n      Principal: your-username@NADA.KTH.SE\n\n      Issued                Expires               Principal\n      Aug 27 08:28:40 2023  Aug 27 18:28:37 2023  krbtgt/NADA.KTH.SE@NADA.KTH.SE\n</code></pre> <p>Now you are good to go:</p> <pre><code>ssh -o GSSAPIAuthentication=yes  your-username@dardel.pdc.kth.se\n</code></pre> <p>In this case, dardel prompt should appear:</p> <pre><code>dardel-login-2:~$\n</code></pre> <p>Check that tickets have been forwarded:</p> <pre><code>dardel-login-2:~$ klist\n</code></pre> <p>The output should be similar to this:</p> <pre><code>Credentials cache: FILE:/tmp/krb5cc_18118_oZ0CMh5rsk\n      Principal: your-username@NADA.KTH.SE\n\n      Issued                Expires               Principal\n      Aug 27 08:30:05 2020  Aug 27 18:28:37 2023  krbtgt/NADA.KTH.SE@NADA.KTH.SE\n      Aug 27 08:30:05 2020  Aug 27 18:28:37 2023  afs/pdc.kth.se@NADA.KTH.SE\n</code></pre> <p>Notice these are the tickets in the <code>FILE:</code> cache in Dardel.</p> <p>Other useful commands to check the state of your tickets are <code>klist -l</code>, which shows all caches, and <code>klist -v</code>, which shows more detailed information on the acquired tickets.</p>"},{"location":"login/mac_login/#additional-note","title":"Additional note","text":"<p>In order to login you need to supply the option directly to the ssh command.</p> <pre><code>ssh -o GSSAPIAuthentication=yes your-username@dardel.pdc.kth.se\n</code></pre> <p>OpenSSH can be configured with command line arguments or a configuration file to simplify the login procedure. The options in the configuration file are parsed in order. Create or modify the file ~/.ssh/config</p> <pre><code># Hosts we want to authenticate to with Kerberos\nHost *.kth.se *.kth.se.\n# User authentication based on GSSAPI is allowed\nGSSAPIAuthentication yes\n# Hosts to which we want to delegate credentials  Try to limit this to\n# hosts you trust  and where you really have use for forwarded tickets \nHost *.csc.kth.se *.csc.kth.se. *.nada.kth.se *.nada.kth.se. *.pdc.kth.se *.pdc.kth.se.\n# All other hosts\nHost *\n</code></pre> <p>Do remember to set the right permission on the file</p> <pre><code>chmod 644 ~/.ssh/config\n</code></pre> <p>After this, you can log in by using</p> <pre><code>ssh your-username@dardel.pdc.kth.se\n</code></pre>"},{"location":"login/reset_password/","title":"How to reset your Kerberos password","text":"<p>This section describes how to reset your kerberos password in different operating systems</p>"},{"location":"login/reset_password/#linux-osx-windows-subsystem-for-linux-wsl","title":"Linux OSX Windows Subsystem for Linux WSL","text":"<p>Changing your kerberos password is straightforward by using kpasswd</p> <ul> <li>Open your favorite shell</li> <li><code>default   kpasswd [username]@NADA.KTH.SE   [username]@NADA.KTH.SE's Password:   New password for [username]@NADA.KTH.SE:   Retype New password for [username]@NADA.KTH.SE:</code></li> </ul>"},{"location":"login/reset_password/#windows","title":"Windows","text":"<p>If you are not using Windows Subsystem for Linux (WSL) you must instead use the graphical Network Identify Manager</p> <ol> <li>Open Network Identity Manager (NIM)</li> <li>Select your kerberos credential</li> <li>Select menu item Credential -&gt; Change password</li> <li>Check that the top part of the window verifies your [username]@NADA.KTH.SE</li> <li>Enter current and new password</li> <li>Press Finish</li> </ol>"},{"location":"login/ssh_keys/","title":"Generating SSH keys","text":"<p>This is a guide on creating SSH keys for use with Dardel. Acceptable key types are ed25519 (EdDSA) and rsa.</p>"},{"location":"login/ssh_keys/#macos","title":"macOS","text":"<p>Open a Terminal window. You can access this by opening Finder, selecting Applications, then the Utilities folder, and double-click Terminal within there. As an alternative open Launchpad, search for Terminal and double click on the Terminal application.</p> <p></p> <p>First, make sure you have a .ssh directory in your home directory by typing these commands in Terminal:</p> <pre><code>cd\n</code></pre> <pre><code>mkdir .ssh\n</code></pre> <pre><code>chmod 700 .ssh\n</code></pre> <p>You can generate a key pair using this command in your terminal:</p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/id-ed25519-pdc\n</code></pre> <p>You may set a passphrase at your discretion, but will then have to enter it every time you use the key unless you load it in an agent. If you don\u2019t want to use a passphrase just press return on the passphrase question.</p> <p>This will generate an EdDSA key pair in .ssh under your home directory. The public key will be id-ed25519-pdc.pub in the same directory and can be registered with PDC.</p> <p>After registration you will be able to log in to Dardel with the following command:</p> <pre><code>ssh -i ~/.ssh/id-ed25519-pdc &lt;username-at-pdc&gt;@dardel.pdc.kth.se\n</code></pre> <p>If you\u2019d like to use this as your default key for Dardel, you may edit \u201c.ssh/config\u201d under your home directory and add this line:</p> <pre><code>Host dardel.pdc.kth.se\n   IdentityFile /Users/&lt;myuser&gt;/.ssh/id-ed25519-pdc\n</code></pre> <p>(If .ssh/config does not exist, simply create it.)  should be substituted with your actual username on the Mac. <p>To find your home directory, go to the Terminal and enter:</p> <pre><code>cd\n</code></pre> <pre><code>pwd\n</code></pre> <p>This should give you \u201c/Users/\u201d <p>To reach .ssh/config, go to Desktop in Finder, press Cmd+up arrow, press Cmd+Shift+. (period), then double-click the .ssh folder that appears. The command Cmd+up arrow, press Cmd+Shift+. makes dot files visible.</p> <p></p>"},{"location":"login/ssh_keys/#linux","title":"Linux","text":"<p>Open a terminal, then please use the macOS instructions as the procedure is similar, although for .ssh/config in your home directory will likely be under /home and not /Users.</p>"},{"location":"login/ssh_keys/#windows","title":"Windows","text":"<p>For PuTTY, make sure it is installed according to the instructions: Install and configure Kerberos and ssh for Windows. For WSL or Cygwin based login, please follow the macOS/Linux instructions.</p> <p>For PuTTY, run PuTTYgen, which if you used a PuTTY installer should be searchable from the start menu.</p> <p></p> <p>Make sure EdDSA is selected at the bottom with 255 bits. Then click \u201cGenerate\u201d.</p> <p></p> <p>Move the mouse randomly within the blank field as instructed until done.</p> <p></p> <p>Optionally set a key comment, so you can identify the key later. You can also set a passphrase but will then have to enter it every time you use the key or load it into an agent. You can copy the public key from the \u201cPublic key for pasting\u2026\u201d field to register with PDC.</p> <p>Make sure to save your private key in a location where you can find it later and is not accessible to other users. You can also load this private key into PuTTYgen later to access the public key in OpenSSH format. The format saved by \u201cSave public key\u201d is not compatible with PDC, so you must use the key from \u201cPublic key for pasting\u2026\u201d</p> <p></p> <p>To use your key with PuTTY, you can either do as in the image above (the \u201cPrivate key file for authentication\u201d field - this will save it with your session) to load it directly into PuTTY\u2026</p> <p>or you can start Pageant, the PuTTY agent.</p> <p></p> <p>Right-click on the computer with the hat in the system tray, select \u201cAdd key, \u201c and then load your private key. As long as the agent is running, the key will be accessible to PuTTY without further configuration.</p>"},{"location":"login/ssh_login/","title":"How to log in with SSH keys","text":"<p>In order to be able to login into PDC system, the user needs a PDC account, and this PDC account must be linked to your SUPR account. Users can upload their SSH (Secure Shell) public keys with the PDC login portal and then use them to log in to PDC resources. Before following this section, make sure you have a valid PDC account (Getting Access). Login to PDC systems will be denied without a valid allocation.</p> <p>Academic and industrial users connected to the Swedish academia can upload their SSH (Secure Shell) public keys on the PDC login portal and authenticate themselves with the help of the Swedish User and Project Repository SUPR. Registered SSH key pair login is restricted by user-defined IPs, and SSH keys have to be renewed regularly. After setting up all details in the login portal the user will be able to login from the chosen IP addresses with SSH for the selected validity period.</p>"},{"location":"login/ssh_login/#how-ssh-key-pairs-work","title":"How SSH key pairs work","text":"<p>Authentication using SSH asymmetric key pairs is very common. Each SSH key pair includes two keys: a public key and a secret key. The public key should be copied to the SSH server. Anyone with a copy of the public key can encrypt data which can then only be read by the person who holds the corresponding private key. The private key must remain with the user and should be kept secret. The server uses the public key to encrypt a message and send it to the client. If the client, on the user side, has the correct private key, it can decrypt the message and send it back to the server for verification.</p>"},{"location":"login/ssh_login/#how-to-create-ssh-key-pairs","title":"How to create SSH key pairs","text":"<p>It is recommended to create a new SSH key pair for every new purpose/resource. Keep in mind to protect the private part of your SSH key pair and only share the public key. Supported SSH key types are ed25519 (EdDSA Elliptic Curve, recommended) and rsa.</p> <p>Follow the instructions on how to generate an SSH key pair, see Generating SSH keys.</p> <p>Once you have successfully produced a working SSH key pair you let your browser open the PDC login portal.</p>"},{"location":"login/ssh_login/#authentication-process-details","title":"Authentication process details","text":"<p>SUPR authentication is used to initially set up the communication between the user\u2019s computer and the PDC system, making sure that security is maintained at all times. After the initial setup, the login portal will only be needed to manage and change the user\u2019s connection information, e.g. after the expiration date had passed or if the user logs in from an IP-address outside the user\u2019s registered IP ranges.</p> <ul> <li>Go to PDC login portal. You will see a start page with a link to the Swedish User and Project Repository (SUPR).</li> <li>Click on the SUPR link.</li> <li>Login to SUPR. SUPR does not use the same password as the one you received from PDC. If you have not previously set up a SUPR password, you can log in with SWAMID.</li> <li>By default PIs, staff and administrators will be asked to authenticate themselves with a second factor, a Time-based One-time Password (TOTP).   : Any TOTP authentication app such as e.g. andOTP, FreeOTP, Authy, Google Authenticator or Duo Mobile can be used.     Use the same app as you used when you registered for the first time for SUPR two-factor authentication.</li> <li>Click on the blue \u2018Prove My Identity to PDC\u2019 button. If you get a \u201cPermission Denied\u201d error it may be because your PDC and SUPR accounts are not linked. Please contact PDC support.</li> </ul>"},{"location":"login/ssh_login/#in-the-login-portal","title":"In the login portal","text":"<p>Once you have successfully authenticated yourself you should be back to the login portal. Your personal information should be displayed together with the details of the login credentials you already registered. If you want to register a new key:</p>"},{"location":"login/ssh_login/#click-on-add-new-key","title":"Click on \u2018Add new key","text":"<p> * Upload or copy and paste the public! part of your SSH key pair (usually with extension .pub). (Tip: If you don\u2019t see the  .ssh directory try toggling the visibility of dot files with e.g. pressing Ctrl + h in Linux or shift + cmd + \u2018.\u2019 for MacOS.) * Name the label or in case the label came from a .pub file you can also rename the label. Labels are for your personal reference only, but you can\u2019t use the same label twice. * The IP address from where you are connected as seen from the portal will already be shown in the address field as a default. A significant bit mask of between /24 and /32 or a domain-name based range can be added to the IP address. * For the expiration date, the maximum allowed value is shown. The user can chose any value between now and this maximum value. * Once you have filled in all the fields, Press Save.</p> <p>You now get back to the page with personal information in the login portal. You can also add further IP addresses, IP address ranges or domain-based address ranges to existing keys, as well as delete IP addresses or address ranges from keys. One key can have several different IP addresses and address ranges. A key will need to be restricted in the (IP) address range to be considered valid.</p> <p>Several keys can be added in the PDC login portal. You can add further keys or delete keys. If you want to add a short term entry for the same key, you can upload the same key again and restrict it to a different IP address or address range with a different expiry date.</p>"},{"location":"login/ssh_login/#log-in-to-pdc-resources","title":"Log in to PDC resources","text":"<p>Once your SSH public key is properly registered, you can login from a terminal, or by using Putty.</p>"},{"location":"login/ssh_login/#linux-and-mac-os","title":"Linux and mac OS","text":"<p>Open a terminal to login with</p> <pre><code>ssh dardel.pdc.kth.se\n</code></pre> <p>or if your user name on the system you log in from differs from your username at PDC:</p> <pre><code>ssh &lt;username-at-pdc&gt;@dardel.pdc.kth.se\n</code></pre> <p>When connecting for the first time, your ssh client will write:</p> <p>The authenticity of host \u2018dardel.pdc.kth.se (130.237.230.204)\u2019 can\u2019t be established.</p> <p>and display an ED22519, ECDSA, or a RSA key fingerprint. You should then compare that the fingerprint matches with one of the fingerprints stated on this webpage.</p>"},{"location":"login/ssh_login/#windows","title":"Windows","text":"<p>To log in with Putty, first follow the instructions on how to generate an SSH key pair, see Generating SSH keys and register the public key.</p> <p>You can then log in to Dardel by specifying dardel.pdc.kth.se as Host Name in the Putty main window.</p> <p></p> <p>When connecting for the first time, Putty will display the alert</p> <p></p> <p>You should then compare that the key fingerprint matches with one of the fingerprints stated on this webpage.</p>"},{"location":"login/ssh_login/#key-fingerprints","title":"Key fingerprints","text":"<p>The key fingerprints are hashed with either an SHA256 hash function, or an MD5 hash function.</p> <p>SHA256 hashes of key fingerprints</p> <pre><code>ED25519 key fingerprint is SHA256:mFOhdtbmDt7lT8kAtx5KJvYzUrJzyg5dbytsMODF0EE\nECDSA key fingerprint is SHA256:qisSXP26ai1ZJuTywX+6eZia7W9RgqFhrnvRSx1fyLg\nRSA key fingerprint is SHA256:7sysEJ5Gbbjjr3ITLxNZR8IUXfxTqmJ+Y3p0gDhpyCI\n</code></pre> <p>MD5 hashes of key fingerprints</p> <pre><code>ED25519 key fingerprint is MD5:84:2d:b2:67:bd:2a:0a:e7:4e:bd:9a:57:32:41:d8:8b\nRSA key fingerprint is MD5:15:e7:9e:4a:42:4d:61:29:dd:a3:2c:b9:b3:ef:e0:9d\nECDSA key fingerprint is MD5:8c:62:70:4c:b8:f2:c4:ab:14:43:b7:31:ba:c3:c9:52\n</code></pre>"},{"location":"login/ssh_login/#users-which-do-not-have-a-supr-account","title":"Users which do not have a SUPR account","text":"<p>For users without SUPR account currently only Kerberos login is officially supported. If you are in SUPR, but your SUPR account is not properly linked to PDC, please contact PDC support (support@pdc.kth.se).</p>"},{"location":"login/ssh_login/#configuring-ssh-keys-and-kerberos-login","title":"Configuring ssh keys and kerberos login","text":"<p>It is possible to have both authentication methods enabled at the same time and use whichever one you prefer when connecting. If you have configured your config file according to instructions on the SSH page you need to add the following before your kerberos configuration in the config file.</p> <pre><code>Host dardel.pdc.kth.se\n  Preferredauthentications publickey\n  IdentityFile ~/.ssh/&lt;PRIVATE SSH FILE&gt;\n</code></pre> <p>In case you have not done any changes to your config file or if you do not have a config file, then everything should work regardless of which login method you are using.</p>"},{"location":"login/ssh_login/#debugging","title":"Debugging","text":"<p>If you experience any problems logging in via your newly registered key, you should contact PDC Support (support@pdc.kth.se).</p> <p>If you already experience problems during the authentication process it might help to start a new private window in the browser with fresh settings.</p> <p>If you are interested, you can log in with Kerberos and check the current information about you in PDC\u2019s local LDAP directly on the login Node.</p> <pre><code>ldapsearch -x -b uid=`whoami`,ou=users,ou=dardel,dc=pdc,dc=kth,dc=se\n</code></pre> <p>There you can also check the cached authentication information about you:</p> <pre><code>sss_ssh_authorizedkeys `whoami`\n</code></pre>"},{"location":"login/windows_login/","title":"How to login from Windows","text":"<p>This section describes how to acquire kerberos tickets in Windows and log in to PDC.</p> <p>There are two essentially different ways to log in from Windows:</p> <ol> <li>Install and configure Kerberos and ssh for Windows (using Network Identity Manager and PuTTY)</li> <li>Install and configure Windows Subsystem for Linux (WSL) (this requires Windows 10 or higher)</li> </ol> <p>Note that if you have a KTH-Windows machine, the necessary software has to be installed by KTH IT department. For help with this, please contact it-support@kth.se.</p>"},{"location":"login/windows_login/#install-and-configure-kerberos-and-ssh-for-windows","title":"Install and configure Kerberos and ssh for Windows","text":"<p>Here are the installers that you need:</p> <p></p> <p>Links to installation packages:</p> <ol> <li>Heimdal Kerberos: Download the newest Heimdal Kerberos for Windows    (32-bit or 64-bit depending on your computer\u2019s architecture).    The latest version can be found at https://www.secure-endpoints.com/heimdal/.    Run the downloaded installer, and choose to install a complete installation of Heimdal.</li> </ol> <p>In case the download link is temporarily down, you can obtain the installer    from this link.    If you want to confirm the integrity of the installer, you can compute    the MD5SUM checksum (e.g., according to    these instructions).    The MD5SUM checksum of the 64-bit installer version 7.4 is:    <pre><code>$ md5sum Heimdal-AMD64-full-7-4-0-40.msi\n  282817ef0e1a07f0763def9713bdf995  Heimdal-AMD64-full-7-4-0-40.msi\n</code></pre> 1. Network Identity Manager (NIM): Download the newest installer from    https://www.secure-endpoints.com/netidmgr/v2/#download (choose the    version that matches your operating system). Installation    of NIM is usually straightforward.</p> <p>In case the download link is temporarily down, you can obtain the installer    from this link.    The MD5SUM checksum of the 64-bit installer version 2.5 is:    <pre><code>$ md5sum netidmgr-heimdal-AMD64-rel-2_5_0_106.msi\n  5c4945e9cc36a3b67982911b95554f5d  netidmgr-heimdal-AMD64-rel-2_5_0_106.msi\n</code></pre> 1. PuTTY: A version of PuTTY which supports GSSAPI and Heimdal Kerberos.    is needed, i.e. version 0.72 and later.    Installers can be downloaded    from this page.</p> <p>In case the download link is temporarily down, you can obtain the installer    from this link.    The MD5SUM checksum of the 64-bit installer for version 0.74 is:    <pre><code>$ md5sum putty-64bit-0.74-installer.msi\n  a76dd5040fb8bd78a4a5acbf445741a2  putty-64bit-0.74-installer.msi\n</code></pre> 1. (Optional) AFS client: Download the newest OpenAFS client for Windows from    https://www.auristor.com/openafs/client-installer/ (32-bit or 64-bit    depending on your computer\u2019s architecture). The client allows you to access    your PDC home directory from your local computer\u2019s file browser.</p> <p>In case the download link is temporarily down, you can obtain the installer    from this link.    The MD5SUM checksum of the 64-bit installer for version 1.7 is:    <pre><code>$ md5sum yfs-openafs-en_US-AMD64-1_7_3301.msi\n  7ea76eb80c8285f55c51550d593a271c  yfs-openafs-en_US-AMD64-1_7_3301.msi\n</code></pre></p> <p>After downloading these installers, follow the instructions below to configure your login.</p>"},{"location":"login/windows_login/#setting-up-network-identity-manager","title":"Setting up Network Identity Manager","text":"<p>The easiest option to create and manage Kerberos tickets is the Network Identity Manager (NIM). NIM can also manage multiple tickets from different realms.</p> <p>After opening NIM, choose the advanced NIM view:</p> <p></p> <p>Obtain new credentials dialog:</p> <p></p> <p>Choose the following options. You may need longer ticket lifetimes if you have long running batch jobs:</p> <p></p> <p>Put in password and check the [x] make default box:</p> <p></p> <p>This is how NIM looks with kerberos tickets and AFS tokens:</p> <p></p> <p>For further information on managing Kerberos tickets via NIM, please visit https://www.secure-endpoints.com/netidmgr/v2/docs/.</p>"},{"location":"login/windows_login/#alternative-to-nim-kerberos-config-file","title":"Alternative to NIM Kerberos config file","text":"<p>Follow instructions at How to configure kerberos and SSH with kerberos and how to acquire Kerberos tickets. You may need administrator privileges to be able to edit Kerberos configuration file under Windows.</p>"},{"location":"login/windows_login/#setting-up-putty","title":"Setting up PuTTY","text":"<p>PuTTY is a convenient method of connecting to PDC clusters from Windows. PuTTY is an SSH and telnet client, developed originally by Simon Tatham for the Windows platform.  PuTTY is open source software that is available with source code and is developed and supported by a group of volunteers.</p> <p>The first step is to create what is called a Session in PuTTY.  A session is basically a collection of settings for a connection to a machine.</p> <p>In the field Host Name at the top, enter <code>your-username@cluster.pdc.kth.se</code> (substituting <code>your-username</code> and <code>cluster</code> as needed). Make sure the port is 22 and that SSH is selected underneath.</p> <p>Now configure GSSAPI settings by selecting Connection &gt; SSH &gt; Auth &gt; GSSAPI and specify GSSAPI DLL:</p> <p></p> <p>The GSSAPI library is normally installed here. The User-Specified GSSAPI DLL must be on top of the list (press [Up] if neccessary:</p> <p></p> <p>Now go to Connection &gt; SSH &gt; KEX. In the Algorithm selection policy mark the line</p> <pre><code>\"--warn below here--\"\n</code></pre> <p>and press \u201cDown\u201d until it\u2019s below the warning:</p> <pre><code>Diffie-Hellman group 14\n</code></pre> <p>As seen in the image below:</p> <p></p> <p>Before logging in, save your PuTTY config as a profile named dardel:</p> <p></p> <p>Now, click Open. If you have valid Kerberos tickets as explained above you will now log in to the cluster.</p> <p>A Dardel login looks like this:</p> <p></p> <p>The path to your PDC home directory from Windows is <code>\\\\afs\\pdc.kth.se\\home\\u\\username</code> with <code>u</code> as the first letter in your username.</p> <p>When you login to Dardel, you would reach your home directory on the LUSTRE file system : `` cfs/klemming/home/u/username`` with <code>u</code> as the first letter in your username.</p> <p>While PuTTY is our recommended SSH client for Windows, it should be possible also to use Cygwin. For instructions on using Cygwin, see the section Alternative to PuTTY under Windows: Cygwin below.</p>"},{"location":"login/windows_login/#install-an-x-server-optional","title":"Install an X server optional","text":"<p>If you would like to run graphical programs, you need to additionally install an X server. There are several X servers that run on Windows, including:</p> <ul> <li>Xming</li> <li>Cygwin X</li> <li>vcXsrv</li> </ul> <p>We recommend Xming, and the process to set it up is as follows:</p> <ol> <li>Install Xming</li> <li>Start Xlaunch on Windows.</li> <li>In PuTTY, load a connection to dardel.</li> <li>In Connection &gt; SSH &gt; X11, check the box \u201cEnable X11 forwarding\u201d.</li> <li>Open the connection to dardel.</li> <li>You should now be able to use software with graphical windows locally    (test that first) and remotely through X11-forwarding. On the remote    (PDC) side the DISPLAY will be setup automatically. Before starting    advanced remote visualization tools we recommend to start xterm to    check the connection.</li> </ol>"},{"location":"login/windows_login/#install-and-configure-windows-subsystem-for-linux-wsl","title":"Install and configure Windows Subsystem for Linux WSL","text":"<p>The WSL lets users run a GNU/Linux environment, including most command-line tools, utilities, and applications, directly on Windows. WSL requires Windows 10 or newer. See the offical documentation for further information.</p> <p>A step-by-step installation process is as follows:</p> <ol> <li>Enable Windows Subsystem Linux according to first section at: https://docs.microsoft.com/en-us/windows/wsl/install-win10</li> <li>Download a Linux distro, e.g. Ubuntu, from Microsoft Store.</li> <li>Install it.</li> <li>Choose a username for this distribution. It does not need to be the same as your Windows user.</li> <li>Choose a password for this user (NB: this is not for your Windows user)</li> <li>Update the packages by typing: sudo apt update</li> <li>Upgrade packages: sudo apt upgrade</li> <li>Follow login instructions for How to login from Ubuntu/Debian (or a corresponding    page for another Linux distribution that you have chosen)</li> </ol> <p>The Windows file system is mounted under <code>/mnt/c/</code>. In other words, you can copy files between from your regular Windows file system by:</p> <pre><code>$ cp /mnt/c/&lt;path-relative-to-C&gt; &lt;Linux-file-system-path&gt;/\n</code></pre> <p>and vice versa.</p> <p>If you would like to run graphical programs on WSL, you need to additionally install an X server. There are several X servers that run on Windows, including:</p> <ul> <li>Xming</li> <li>Cygwin X</li> <li>vcXsrv</li> </ul> <p>We recommend Xming, and the process to set it up for use within WSL is as follows:</p> <ol> <li>Install Xming</li> <li>Start Xlaunch on Windows.</li> <li>In the Ubuntu terminal, type <code>export DISPLAY=localhost:0</code>. If you put this command in the file <code>.bashrc</code> in your home directory, it will be automatically set next time you start.</li> <li>You should now be able to use software with graphical windows locally    (test that first) and remotely through X11-forwarding. Use the -Y    option of ssh for that. On the remote (PDC) side the DISPLAY    will be setup automatically. Before starting advanced remote    visualization tools we recommend to start xterm to check the    connection.</li> </ol>"},{"location":"login/windows_login/#alternative-to-putty-under-windows-cygwin","title":"Alternative to PuTTY under Windows Cygwin","text":""},{"location":"login/windows_login/#warning","title":"WARNING","text":"<p>These instructions are written for older versions on Cygwin. Unfortunately they do not work on current versions.</p> <p>Cygwin can be used instead of PuTTY under Windows. The Cygwin environment gives you a lot of commands in your Windows environment that normally are available only under Unix/Linux.  Among these commands are bash, grep and less.  Cygwin is needed because it contains the libraries (i.e. DLL-files) and utility programs that Heimdal will use. After the installation you will be able to use Kerberos enabled login methods to connect to PDC.</p> <p>Installation</p> <ol> <li>If you already have an old Cygwin or Cygwin based installation on your system you have to do some manual steps as    described on http://www.cygwin.com/ before upgrading to Cygwin 2.9.0</li> <li>As most computers nowadays are 64-bit, we only suppory 64-bit Cygwin.    There is currently no version available for 32-bit Cygwin any more.    The pdclogin-0.8 package has been tested on 64-bit-Win8.</li> <li>To install Cygwin with X11 please follow the instructions    described in http://x.cygwin.com/docs/ug/setup.html.    Especially note that during step 15 of the install you want to make sure that you select the packages:</li> <li>xorg-server</li> <li>Xinit</li> <li>X-start-menu-icons (if available)</li> <li>xterm</li> <li>wget</li> <li>crypt</li> <li>libssp</li> <li>libopenssl098 (or greater)</li> <li>Start the Cygwin X11 server (Programs-&gt;Cygwin-X-&gt;XWinServer) and an xterm (Programs-&gt;Cygwin-X-&gt;xterm).    The XWin server is controlled (for example ended) by right-click on the small X icon in your taskbar.</li> <li>Windows will ask you now if you want to open the firewall for XWin.    Answer \u201cKeep Blocking\u201d as we do not want anyone to connect to your XWin from the Internet.</li> <li>Download the pdclogin-0.8 package from the cygwin xterm    <pre><code>wget https://pkgserver.pdc.kth.se/pub/heimdal/binaries/amd64-pc-cygwin/pdclogin-0.8-amd64.tar.gz\n</code></pre></li> <li>Check the checksum of the downloaded file    <pre><code>sha1sum pdclogin-0.8-amd64.tar.gz\ne17a8f79663dff2bcd4901251206ad1bffe81c2f  pdclogin-0.8-amd64.tar.gz\n</code></pre></li> <li>Extract and then open the downloaded file    <pre><code>tar xzvf pdclogin-0.8-amd64.tar.gz\n</code></pre></li> </ol> <p>This will generate a directory named pdclogin-0.8-amd64. 1. Read the README file in the above mentioned directory (any additional information specific to the version will be there)</p> <p>Using Cygwin</p> <ol> <li>Start Cygwin if necessary (as during Install above)</li> <li>Use the kinit and ssh commands directly or use the pdclogin script to login at PDC    <pre><code>./pdclogin -Y &lt;USERNAME&gt;@&lt;CLUSTER&gt;.pdc.kth.se\n</code></pre></li> </ol> <p>USERNAME should be replaced by your PDC user name.    Similarily, CLUSTER should be replaced by the PDC computer which you want to reach.    pdclogin is located in ~/pdclogin-0.8.    pdclogin takes any SSH command line options (like -Y). 1. If you get an error message containing unable to find realm of host then you    have not followed the README mentioned during the Install. 1. In order to transfer files you use the pdccopy script included in the pdclogin-0.8 folder. For instance    <pre><code>./pdccopy ./README &lt;USERNAME&gt;@&lt;CLUSTER&gt;.pdc.kth.se :~/Private/\n</code></pre></p> <p>would transfer the file README in your local cygwin directory ~/pdclogin-0.8 to the    folder Private in your home directory at PDC.</p>"},{"location":"run_jobs/job_arrays/","title":"Job arrays","text":"<p>Job arrays are a feature of the SLURM queue system to manage many similar jobs together. They make it easy to specify, submit and monitor a group of jobs all using the same sbatch options.</p> <p>Let\u2019s say you have the following script, which runs <code>myprog</code> with the arguments 0,1,\u2026,9.</p> <pre><code>#! bin bash  l\n#SBATCH  A project\n#SBATCH  N 1\n#SBATCH  t 00 10 00\n\nfor i in $(seq 0 9); do\n    srun -n 1 myprog $i\ndone\n</code></pre> <p>If each iteration takes a long time, you may want to run them in parallel instead. We can achieve this by rewriting the script so that each iteration is submitted as a separate job using a job array.</p> <pre><code>#! bin bash  l\n#SBATCH  A project\n#SBATCH  N 1\n#SBATCH  t 00 01 00\n#SBATCH  a 0 9\n\nsrun -n 1 myprog $SLURM_ARRAY_TASK_ID\n</code></pre> <p>The option <code>#SBATCH -a 0-9</code> specifies that we want to run one job for each number between 0 and 9 inclusive. For each job the number is available in the <code>SLURM_ARRAY_TASK_ID</code> environment variable. Note that the time <code>-t</code> is reduced from 10 hours to 1 hour.</p> <p>Note</p> <p>Submitting a large number of short jobs is bad for performance since there is an overhead associated with starting and stopping each job. Too many jobs can also slow down the whole queue system. Ideally each job should be at least 10 minutes long. For more information please see Short jobs.</p> <p>Once submitted, the job array is assigned an id that can be used to manipulate all jobs at once. For instance to cancel all jobs in a job array:</p> <pre><code>&gt; sbatch job_array.sh\nSubmitted batch job 6975769\n&gt; scancel 6975769\n</code></pre> <p>Pending jobs in the same array are also combined into a single line in the output of <code>squeue</code>.</p> <p>When using <code>#SBATCH --mail-type=ALL</code>, one email will be sent for the whole array. To get mail for each individual job use <code>#SBATCH --mail-type=ALL,ARRAY_TASKS</code>.</p> <p>If the parameters of your program are not just a range of numbers you can place them in a bash array. For instance, to run <code>myprog apples</code>, <code>myprog pears</code> and <code>myprog oranges</code>:</p> <pre><code>params=(apples pears oranges)\nsrun -n 1 myprog ${params[$SLURM_ARRAY_TASK_ID]}\n</code></pre> <p>Read more about job arrays on the SLURM website.</p>"},{"location":"run_jobs/job_scheduling/","title":"How to Run Jobs","text":"<p>Many researchers run their program on PDC\u2019s computer systems, often simultaneously. For this, the computer systems need a workload management and job scheduling. For job scheduling PDC uses Slurm Workload Manager .</p> <p>When you login to the supercomputer with ssh, you will login to a designated login node in your Klemming home directory. Here you can modify your scripts and manage your files.</p> <p></p> <p>To run your script/program on the computer nodes, you can do it in one of the following ways.</p>"},{"location":"run_jobs/job_scheduling/#how-jobs-are-scheduled","title":"How jobs are scheduled","text":"<p>The queue system uses two main methods to decide which jobs are run. These are called fair-share and backfill. Unlike some other centers, the time a job has been in the queue is not a factor.</p>"},{"location":"run_jobs/job_scheduling/#fair-share","title":"Fair share","text":"<p>The goal of the fair share algorithm is to make sure that all projects can use their fair share of the available resources within a reasonable time frame. The priority that a job (belonging to a particular project) is given will depend on how much of that project\u2019s time quota has been used recently in relation to the quotas of jobs belonging to other projects - the effect of this on the priority declines gradually with a half-life of 14 days. So jobs submitted by projects that have not used much of their quota recently will be given high priority, and vice versa.</p>"},{"location":"run_jobs/job_scheduling/#backfill","title":"Backfill","text":"<p>As well as having a main queue to ensure that the systems are as full as possible, the job scheduling system also implements \u201cbackfill\u201d. If the next job in the queue is large (that is, it will need lots of nodes to run), the scheduler collects nodes as they become free until there are enough to start running the large job. Backfill means that the scheduler looks for smaller jobs that could start on nodes that are free now, and which would finish before there are enough nodes free for the large job to start. For backfill to work well, the scheduler needs to know how long jobs will take. So, to take advantage of the possibility of backfill, you should set the maximum time your job needs to run as accurately as possible in your submit scripts.</p> <p>This graph shows the percentage of the nodes on Beskow (previous PDC\u2019s supercomputer) that were in use on different dates from early 2015 till late 2016. You can see how the scheduler makes good use of Beskow as nearly all of the available nodes are being used all the time.</p> <p>Note: All researchers sharing a particular time allocation have the same priority. This means that if other people in your time project have used up lots of the allocated time recently, then any jobs you (or they) submit within that project will be given the same low priority.</p>"},{"location":"run_jobs/job_scheduling/#example-of-scheduling","title":"Example of scheduling","text":"<p>Of course both Anna and Bjo\u0308rn would like their jobs to be run as soon as possible.</p> <p>However, in the current situation, the scheduler will give priority to Bjo\u0308rn\u2019s job as his project (B) has not used as much of its time allocation recently as project A has used of their allocation.</p> <p>The fact that Anna has not used any time herself does not make any difference as it is the total amount of time recently used by each project that is taken into consideration when deciding which job will be scheduled next.</p>"},{"location":"run_jobs/job_scheduling/#dardel-compute-nodes","title":"Dardel compute nodes","text":"<p>Compute nodes on Dardel have 4 different flavors with different amounts of memory. A certain amount of the memory is reserved for the operating system and system software, therefore the amount of memory available for user software is also listed in the table below. All nodes have the same dual socket processors, for a total of 128 physical cores per node.</p> Node type Node count RAM Partition Available Example used flag Thin node 700 256 GB main, shared, long 227328 MB Large node 268 512 GB main, memory 456704 MB <code>--mem=440GB</code> Huge node 8 1 TB main, memory 915456 MB <code>--mem=880GB</code> Giant node 10 2 TB memory 1832960 MB <code>--mem=1760GB</code> <p>More details on the hardware is available at https://www.pdc.kth.se/hpc-services/computing-systems/about-dardel-1.1053338.</p> <p>Different node types are allocated based on the SLURM partition (<code>-p</code>) and the <code>--mem</code> or <code>--mem-per-cpu</code> flags. By default, any free node in the partition can be allocated to the job. Using the <code>--mem=X</code> flag restricts the set of eligible nodes to those with at least X amount of available memory.</p> <p>The following configuration allocates one node with at least 250 GB of available memory in the main partition, i.e. a large or huge node:</p> <pre><code>#SBATCH   mem=250GB\n#SBATCH  p main\n</code></pre> <p>The <code>--mem</code> flag imposes a hard upper limit on the amount of memory the job can use. In the above example, even if the job is allocated a node with 1 TB of memory, it will not be able to use more than 250 GB of memory due to this limitation.</p>"},{"location":"run_jobs/job_scheduling/#dardel-partitions","title":"Dardel partitions","text":"<p>The compute nodes on Dardel are divided into four partitions. Each job must specify one of these partitions using the <code>-p</code> flag. The table below explains the difference between the partitions. See the table above for descriptions of the various node types.</p> Partition name Characteristics main Thin, large and huge nodesJob gets whole nodes (exclusive)Maximum job time 24 hours or 7 days long Thin nodesJob gets whole nodes (exclusive)Maximum job time 7 days shared Thin nodesJobs are allocated to cores, not nodesBy default granted one core, get more with <code>-n or -c</code>Job shares node with other jobsMaximum job time 7 days memory Large, huge and giant nodesJob gets whole nodes (exclusive)Maximum job time 24 hours"},{"location":"run_jobs/job_scripts/","title":"Job scripts","text":"<p>To submit a job to the queue system some details have to be specified, for instance the compute project and the number of nodes required. In a job script, this is done by adding lines starting with <code>#SBATCH</code>. These options are then passed to the SLURM, the queue system. They are not the same as options to the <code>srun</code> command, and both needed to be configured properly.</p> <p>The following list explains the most common options specified in job scripts. For a complete reference please consult the output of <code>man sbatch</code>.</p> <p>Note</p> <p>All <code>#SBATCH</code> options must be at the top of the script file.</p> <p><code>#SBATCH -A &lt;project&gt;</code> : The compute project the job is part of. Is used to prioritize jobs.   The <code>-A</code> flag must be specified, or the submission will fail:    <pre><code>Job submit/allocate failed: Invalid partition or qos specification\n</code></pre>    You can check the time allocations you are a member of by running the command:    <pre><code>projinfo\n</code></pre></p> <p><code>#SBATCH -t &lt;hh:mm:ss&gt;</code> : Maximum job run time. Allows SLURM to schedule jobs more effectively. A job with shorter run time will start quicker than a long running job. Other possible formats are , , . <p><code>#SBATCH -N &lt;nodes&gt;</code> : Number of nodes to reserve.</p> <p><code>#SBATCH -n &lt;tasks&gt;</code> : Number of tasks that the job will use. Also known as processes or MPI ranks.</p> <p><code>#SBATCH --ntasks-per-node=&lt;tasks per node&gt;</code> : Set the number of tasks per node. Useful with MPI/OpenMP hybrids where <code>--ntasks-per-node=1</code> can be used.</p> <p><code>#SBATCH --gres=gpu:X</code> : (Currently GPUs are not available on Dardel)</p> <p><code>#SBATCH -J &lt;jobname&gt;</code> : Name shown in the queue. The default is the name of the job script.</p> <p><code>#SBATCH -e &lt;file&gt;</code> : File where error messages are written (stderr). Can include special symbols starting with a percent sign. The job id %j can be used to distinguish different runs. The default is <code>slurm-%j.out</code>.</p> <p><code>#SBATCH -o &lt;file&gt;</code> : File where output messages are written (stdout). Same format as the error file above. The default is <code>slurm-%j.out</code>.</p> <p><code>#SBATCH -a 0-N</code> : Run job array with a total of N+1 jobs. Job arrays are a convienient way to start many similar jobs. Each job gets a different value in the environment variable <code>SLURM_ARRAY_TASK_ID</code>. For a more detailed explanation see Job arrays.</p> <p><code>#SBATCH --mail-type=ALL</code> : Request an email when the job starts and ends.</p> <p><code>#SBATCH -p &lt;partition name&gt;</code> : Specify the partition where the job should run. The partition can be a comma-separated list if the job can run on more than one partition.</p> <p><code>#SBATCH -C &lt;feature&gt;</code> : Specify additional constraints on the requested nodes. To list the available features of all the nodes, run:    <pre><code>sinfo -o %f\n</code></pre></p> <p>You can test if the syntax of a job script is correct by running:</p> <pre><code>sbatch --test-only my_script.sh\n</code></pre> <p>This will check the parameters and print an estimation of when the job would be started, without adding it to the queue.</p> <p>For examples of job scripts please see the following pages:</p> <ul> <li>Job script examples</li> <li>Job arrays</li> <li>Short jobs</li> <li>Pack short jobs together</li> <li>Use fewer cores</li> </ul>"},{"location":"run_jobs/job_scripts_dardel/","title":"Job script examples","text":"<p>Follow links to find information about Dardel compute nodes on Dardel and about Dardel partitions</p> <p>Note</p> <p>It is advisable to use parameter flags in the #SBATCH tags rather than as parameters in srun, to increase clarity of your scripts.</p> <p>Example 1:</p> <p>Below is a job script example for a pure MPI job. For PDC-installed programs, you can find examples in the software page.</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A naissYYYY X XX\n\n# The name of the script is myjob\n#SBATCH  J myjob\n\n# The partition\n#SBATCH  p main\n\n# 10 hours wall clock time will be given to this job\n#SBATCH  t 10 00 00\n\n# Number of nodes\n#SBATCH   nodes=4\n\n# Number of MPI processes per node\n#SBATCH   ntasks per node=128\n\n# Run the executable named myexe\n# and write the output into my output file\nsrun ./myexe &gt; my_output_file\n</code></pre> <p>Example 2:</p> <p>Below is another example for a hybrid MPI+OpenMP program. This example will place 16 MPI processes with 8 threads each on each compute node. Please note that <code>--cpus-per-task</code> needs to be set as 2x <code>OMP_NUM_THREADS</code> to ensure correct placement of the MPI processes, because simultaneous multithreading (SMT) is enabled on AMD processors. In addition, <code>OMP_PLACES=cores</code> is also necessary to ensure correct placement of the threads.</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A naissYYYY X XX\n\n# The name of the script is myjob\n#SBATCH  J myjob\n\n# The partition\n#SBATCH  p main\n\n# 10 hours wall clock time will be given to this job\n#SBATCH  t 10 00 00\n\n# Number of Nodes\n#SBATCH   nodes=4\n\n# Number of MPI tasks per node\n#SBATCH   ntasks per node=16\n# Number of logical cores hosting OpenMP threads\n# Note that cpus per task is set as 2x OMP NUM THREADS\n#SBATCH   cpus per task=16\n\n# Number and placement of OpenMP threads\nexport OMP_NUM_THREADS=8\nexport OMP_PLACES=cores\n\n# for slurm 22 05\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Run the executable named myexe\n# and write the output into my output file\nsrun ./myexe &gt; my_output_file\n</code></pre> <p>Example 3:</p> <p>Below is an example of a job array, in which 100 separate jobs are executed in one shot in the corresponding directories. Job arrays are useful to manage large numbers of jobs which run on the same number of nodes and take roughly the same time to complete. For more information see Job arrays.</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A naissYYYY X XX\n\n# The name of the script is myjob\n#SBATCH  J myjobarray\n\n# The partition\n#SBATCH  p main\n\n# 10 hours wall clock time will be given to this job\n#SBATCH  t 10 00 00\n\n# Number of nodes used for each individual job\n#SBATCH   nodes=1\n\n# Number of MPI tasks per node\n#SBATCH   ntasks per node=128\n\n# Indices of individual jobs in the job array\n#SBATCH  a 0 99\n\n# Fetch one directory from the array based on the task ID\n# Note  index starts from 0\nCURRENT_DIR=data${SLURM_ARRAY_TASK_ID}\necho \"Running simulation in $CURRENT_DIR\"\n\n# Go to job folder\ncd $CURRENT_DIR\necho \"Simulation in $CURRENT_DIR\" &gt; result\n\n# Run individual job\nsrun ./myexe &gt; my_output_file\n</code></pre> <p>Example 4:</p> <p>Below are a job script example for MPI executed on a shared node using 10 cores. Please note that the <code>shared</code> partition is used, since this job cannot saturate a single node on Dardel.</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A naissYYYY X XX\n\n# The name of the script is myjob\n#SBATCH  J myjob\n\n# The partition\n#SBATCH  p shared\n\n# The number of cores requested\n#SBATCH  n 10\n\n# 10 hours wall clock time will be given to this job\n#SBATCH  t 10 00 00\n\n# Run the executable named myexe\n# and write the output into my output file\nsrun -n 10 ./myexe &gt; my_output_file\n</code></pre> <p>Example 5:</p> <p>Below are a job script example for an OpenMP job executed on a shared node using 16 cores. Please note that the <code>shared</code> partition is used, since this job cannot saturate a single node on Dardel.</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A naissYYYY X XX\n\n# The name of the script is myjob\n#SBATCH  J myjob\n\n# The partition\n#SBATCH  p shared\n\n# Number of tasks\n#SBATCH  n 1\n\n# Number of cpus per task\n#SBATCH  c 16\n\n# 10 hours wall clock time will be given to this job\n#SBATCH  t 10 00 00\n\nexport OMP_NUM_THREADS=8\n\n# Run the executable named myexe\n# and write the output into my output file\nsrun ./myexe &gt; my_output_file\n</code></pre> <p>In case you are running more than the granted number of cores on the shared node, a similar error will occur</p> <pre><code>srun: error: Unable to create step for job &lt;JOBID&gt;: More processors requested than permitted\n</code></pre> <p>Note</p> <p>The amount of RAM you are getting on shared nodes is directly proportional to the number of processors you are asking for. For example, if you are asking for half of the processors on one node, you will also get a max of half the amount of RAM on that node.</p> <p>Example 6:</p> <p>Below is a job script example for a job running on a Dardel GPU node.</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A naissYYYY X XX\n\n# The name of the script is myjob\n#SBATCH  J myjob\n\n# The partition\n#SBATCH  p gpu\n\n# 1 hour wall clock time will be given to this job\n#SBATCH  t 01 00 00\n\n# Number of nodes\n#SBATCH   nodes=1\n\n# Number of MPI processes per node\n#SBATCH   ntasks per node=1\n\n# Load a ROCm module and set the accelerator target\nml rocm/5.0.2\nml craype-accel-amd-gfx90a\n\n# Run the executable named myexe\n# and write the output into my output file\nsrun ./myexe &gt; my_output_file\n</code></pre>"},{"location":"run_jobs/job_statistics/","title":"Job Statistics","text":""},{"location":"run_jobs/job_statistics/#viewing-information-about-account-usage","title":"Viewing information about account usage","text":"<p>The PDC system usage page lets you view information about how much time different projects have used on PDC\u2019s systems - either on a daily or monthly basis. The information can be displayed as a graph or as a table and can be viewed at https://pdc-web.eecs.kth.se/cluster_usage/ This information is also partially available using the SUPR portal, if you are a member of a NAISS project.</p>"},{"location":"run_jobs/job_statistics/#specifying-what-information-is-shown-on-the-pdc-system-usage-page","title":"Specifying what information is shown on the PDC system usage page","text":"<p>The top row of the system page is where you click on various options to specify the type of information that you want to view. The currently selected options are shown in BLUE. Options in GREY can be selected by clicking on them.</p>"},{"location":"run_jobs/job_statistics/#to-find-information-about-a-particular-project-account","title":"To find information about a particular project account","text":"<p>To specify the particular project that you are interested in, do either of the following.</p> <ul> <li>On the top row of the PDC system usage page, click on \u201cAllocation\u201d and scroll down to the particular project you are interested in.   Then click on the project name to select that project.</li> <li>On the top row of the PDC system usage page, start typing in the project name in the \u201cSearch..\u201d field, and then click on the name of the particular project from the list that will open up under the \u201cAllocation\u201d heading.   Note that the names of most projects start with \u201cprj\u201d.</li> </ul> <p>Usage information for that account will then be displayed. You can adjust the information</p> <ul> <li>to view it as a graph or as a table,</li> <li>to see usage statistics on a daily or monthly basis, and</li> <li>to see usage for different systems.</li> </ul>"},{"location":"run_jobs/job_statistics/#displaying-account-usage-information-as-a-graph-or-table","title":"Displaying account usage information as a graph or table","text":""},{"location":"run_jobs/job_statistics/#to-view-account-usage-information-as-a-graph","title":"To view account usage information as a graph","text":"<p>On the top row of the PDC system usage page, click on Graph.</p>"},{"location":"run_jobs/job_statistics/#to-view-account-usage-information-as-a-table","title":"To view account usage information as a table","text":"<p>On the top row of the PDC system usage page, click on Table.</p>"},{"location":"run_jobs/job_statistics/#viewing-usage-information-per-day-or-per-month","title":"Viewing usage information per day or per month","text":""},{"location":"run_jobs/job_statistics/#to-view-account-usage-information-on-a-daily-basis","title":"To view account usage information on a daily basis","text":"<p>On the top row of the PDC system usage page, click on Daily.</p>"},{"location":"run_jobs/job_statistics/#to-view-account-usage-information-on-a-daily-basis_1","title":"To view account usage information on a daily basis","text":"<p>On the top row of the PDC system usage page, click on Monthly.</p>"},{"location":"run_jobs/job_statistics/#viewing-usage-information-for-specific-pdc-systems","title":"Viewing usage information for specific PDC systems","text":"<p>If you want to know how much time the selected project has used on a particular PDC system, click on the name of that particular system on the top row of the PDC system usage page.</p>"},{"location":"run_jobs/job_statistics/#to-clear-all-current-selections","title":"To clear all current selections","text":"<p>Click on Reset.</p>"},{"location":"run_jobs/queueing_jobs/","title":"How to submit jobs","text":"<p>Here\u2019s a simplified workflow of queueing jobs to the supercomputer.</p> <p></p> <p>For running large time consuming programs, sending the job to the queue system is preferred.</p> <ul> <li>You can submit a job script to the Slurm queue system from the login node with:   <pre><code>sbatch &lt;filename&gt;\n</code></pre></li> </ul> <p>By default any output messages from the job are written to the file <code>slurm-XXX.out</code> where <code>XXX</code> is the job id.   More information on how to create an job script can be found in Job scripts.</p>"},{"location":"run_jobs/queueing_jobs/#warning","title":"WARNING","text":"<p>Note that programs should ONLY be run with sbatch, or following the instruction in Run interactively Running programs in any other way will result in the program running on the login node and not on the super computer.</p> <ul> <li>You can remove your job from queue with:   <pre><code>scancel &lt;jobid&gt;\n</code></pre></li> <li>Information about the jobs running in the queue can be obtained with:   <pre><code>squeue\n</code></pre></li> <li>You can also see your job in the queue by adding a flag:   <pre><code>squeue -u &lt;username&gt;\n</code></pre></li> </ul> <p>The state of job is listed in the ST column. The most common job state codes are:   * R: Running   * PD: Pending   * CG: Completing   * CA: Cancelled</p> <p>For more job state codes please visit Slurm Job State Codes. * To get further information about your jobs:   <pre><code>scontrol show job &lt;jobid&gt;\n</code></pre> * To get detailed status information of a running job/step:   <pre><code>sstat --jobs=your_job-id\n</code></pre></p> <p>The sstat command displays information about CPU usage, task information, node information,   Resident Set Size (RSS) and virtual Memory (VM).</p> <p>By the default sstat outputs much more information than what is normally needed. However, this information can be filtered using the flag <code>--format</code> together with a list of the fields we want to have. For example:   <pre><code>sstat --jobs=your_job-id --format=JobID,aveCPU,MaxRRS,NTasks\n</code></pre> * To get detailed information on past jobs:   <pre><code>sacct\n</code></pre></p> <p>This command is very similar to sstat, but instead of pulling information from jobs that are currently running, it provides information on past jobs.</p> <p>To get information on a certain job:   <pre><code>sacct --jobs=your_job-id\n</code></pre></p> <p>Jobs can also be queried using your username:   <pre><code>sacct --user=your_username\n</code></pre></p> <p>By default sacct show jobs executed after 00:00:00 of the current day. To search for older jobs user the flag <code>--starttime</code>:   <pre><code>sacct --starttime=YYYY-MM-DD\n</code></pre></p> <p>The output of sacct can be formated with the flag <code>--format</code> in the same manner than with sstat. The command can provide very detailed information, and thus, the list of possible fields that can be requested is long. Please use the flag <code>--helpformat</code> or check the man page of sacct for more information.</p> <p>As an example, the following command would show information on jobs executed after June 23 2019. The information will contain job name, CPU time used, number of nodes used, maxrss, and elapsed time:   <pre><code>sacct --starttime=2019-06-23 --format=JobName,CPUTime,NNodes,MaxRSS,Elapsed\n</code></pre></p> <p>The full list of fields and an explanation for each one of them can be found by using the flag <code>--helpformat</code> or by visiting the man page for sstat. * To get a view on nodes and partitions (logically-grouped nodes) offered by the cluster:   <pre><code>sinfo\n</code></pre></p> <p>More information on all these SLURM commands can be found on their respective man pages.</p> <ul> <li>Job scripts</li> <li>Job script examples</li> <li>Job arrays</li> <li>Short jobs<ul> <li>Pack short jobs together</li> <li>Use fewer cores</li> </ul> </li> </ul>"},{"location":"run_jobs/run_interactively/","title":"Run interactively","text":"<p>Here\u2019s a simplified workflow of booking a node.</p> <p></p> <p>Booking a node might be suitable if you want to test and verify your code in a parallell environment, or when the code is not time consuming but frequent adjustment is needed.</p> <p>Compute nodes can be booked from the queue system for interactive use. This means that you can run your program similar to how you run it on a local computer through terminal.</p> <p>Booking an interactive node can be useful when you want to test, verify or debug your code in a parallell environment. It\u2019s also suitable when the program is not time consuming but is in need of frequent adjustment. For a large scale program we recommend Queueing jobs instead, since waiting for an interactive node booked with large amount of run time can take a lot of time.</p> <p>The command to book an interactive node is salloc</p> <pre><code>salloc --nodes=2 -t 1:00:00 -A &lt;project&gt;\n</code></pre> <p>The above command would then book two nodes for one hour. The -A  should be the time allocation you are a part of. The <code>-A</code> flag must be specified, or the command will receive an error. <pre><code>Job submit/allocate failed: Invalid partition or qos specification\n</code></pre> <p>You can check the time allocations you are a member of with</p> <pre><code>projinfo [options]\n</code></pre> <p>Depending on how busy the supercomputer is, it might take a while before the interactive node is booked. The terminal would then be loading while it waits in the queue. When an node is ready you will recieve a message like</p> <pre><code>salloc: Granted job allocation &lt;jobid&gt;\n</code></pre> <p>When a node is booked, the program must be run with specific cluster commands. Commands are detailed below for our current clusters at PDC.</p> <p>On some of our cluster you can also login directly to the interactive compute node from a separate window. After login you can run the software you like normally. The name for the booked node be found using</p> <pre><code>echo $SLURM_NODELIST\n</code></pre> <p>If you\u2019re running a specific software, please see the How to use module to load different softwares into your environment and Available software .</p> <p>!!! note Keep in mind that after salloc you\u2019re still in the Login Node! if you execute a program without the specific commands the program will be running in the login node!</p> <p>Keep in mind that the node is booked as long as you have not shut down the terminal you typed salloc, or typing the <code>exit</code> command, or the time runs out.</p>"},{"location":"run_jobs/run_interactively/#running-on-dardel","title":"Running on Dardel","text":"<p>Follow links to find information about Dardel compute nodes on Dardel and about Dardel partitions</p> <p>The command to book an interactive node is salloc as described above, but the <code>-p</code> flag must be specified.</p> <pre><code>salloc --nodes=2 -t 1:00:00 -A &lt;project&gt; -p &lt;partition&gt;\n</code></pre> <p>A job can be started using srun. Commands like aprun and mpirun are not available on the system.</p> <pre><code>srun -n 64 ./program\n</code></pre> <p>On this cluster you can also login directly into the compute node when running interactively, as described above. Although login to a compute node, can only be carried out from the login node at the moment.</p> <pre><code>salloc -t 10:00 -A &lt;project&gt; -p &lt;partition&gt;\necho $SLURM_NODELIST\n&lt;NODE_NAME&gt;\nssh &lt;NODE_NAME&gt;\n</code></pre>"},{"location":"run_jobs/short_jobs/","title":"Short jobs","text":"<p>Submitting a large number of short jobs is bad for performance since there is an overhead associated with starting and stopping each job. Too many jobs can also slow down the whole queue system. Ideally each job should be at least 10 minutes long.</p> <p>Running the occasional short job is of course fine. It is often necessary for testing. The problem occurs when there is a large amount of short jobs.</p> <p>There is also an overhead associated with running many short job steps. A job step is for example an invocation of <code>srun</code>. Job steps are cheaper than whole jobs, but too many of them can still cause slowdowns.</p> <p>Below is some advice for how to reduce the number of short jobs. If you need help with this please Contact Support.</p> <p>If you are submitting many similar jobs, also consider using Job arrays.</p>"},{"location":"run_jobs/short_jobs/#pack-short-jobs-together","title":"Pack short jobs together","text":"<p>Several short jobs may be packed together into a single larger job where they run serially to increase the length of the job and reduce the number of jobs.</p> <p>Let\u2019s say you have the following job script:</p> <pre><code>#! bin bash  l\n#SBATCH  A project\n#SBATCH  N 1\n#SBATCH  t 00 01 00\n\nsrun -n 1 myprog $1\n</code></pre> <p>The jobs are submitted with the following command, where <code>x</code> is the data for the job:</p> <pre><code>sbatch short_job.sh x\n</code></pre> <p>By increasing the time limit and supplying multiple data we can run the program more times in the same job. The following script will run <code>myprog</code> once for each of the arguments to <code>sbatch</code>.</p> <pre><code>#! bin bash  l\n#SBATCH  A project\n#SBATCH  N 1\n#SBATCH  t 00 10 00\n\nfor arg in \"$@\"; do\n    srun -n 1 myprog $arg\ndone\n</code></pre> <p>The special symbol <code>\"$@\"</code> means all arguments to the script. The jobs are now submitted using:</p> <pre><code>sbatch packed_job.sh x0 x1 x2 x3 x4 x5 x6 x7 x8 x9\n</code></pre>"},{"location":"run_jobs/short_jobs/#use-fewer-cores","title":"Use fewer cores","text":"<p>Reducing the level of parallelism will make the jobs take longer, while allowing more of them to run at the same time. An added benefit is that this often increases efficiency as there is less communication.</p> <p>If your job runs on multiple nodes, then it is usually simple to just decrease the number of nodes used.</p> <p>If you job runs on one node, then it is usually possible to run multiple instances of the program at the same time, each using a fewer number of cores. Let\u2019s say you have the following OpenMP job.</p> <pre><code>#! bin bash  l\n#SBATCH  A project\n#SBATCH  N 1\n#SBATCH  t 00 01 00\n\nexport OMP_NUM_THREADS=32\n\nsrun myprog $1\n</code></pre> <p>Using an approach similar to Pack short jobs together, we can pack multiple jobs into one. This time the packed jobs can run in parallel on different cores. Reducing the number of threads from 32 to 4 means that we can run 32/4=8 instances of the program at the same time. The script below starts a job step running another script called <code>inner.sh</code>.</p> <pre><code>#! bin bash  l\n#SBATCH  A project\n#SBATCH  N 1\n#SBATCH  t 00 08 00\n\nexport OMP_NUM_THREADS=4\n\nsrun ./inner.sh \"$@\"\n</code></pre> <p>The inner script shown below starts starts several parallel instances of <code>myprog</code>. The <code>&amp;</code> is used to not wait for one command to finish before running the next one, and <code>wait</code> is used to ensure that all instances are done before exiting.</p> <pre><code>#! bin bash\n\nfor arg in \"$@\"; do\n    myprog $arg &amp;\ndone\n\nwait\n</code></pre>"},{"location":"software/module/","title":"How to use module to load different softwares into your environment","text":"<p>Modules are used to load a specific software, and versions, into your environment. In this manner we can uphold a pletora of different softwares and different versions of softwares.</p> <p>This documentation will primarily focus on using modules on Dardel but many of the commands are also working on other clusters.</p> <p>On Dardel we are using the Lmod module system and many softwares can be loaded within different Cray Programming Environments.</p>"},{"location":"software/module/#cray-programming-environment","title":"Cray Programming Environment","text":"<p>The software we have installed are are under different Cray Programming Environment (CPE) This is reflected by using the module PDC which by default loads the latest CPE into your environment.</p> <p>Therefore, right after login in a new session on Dardel, the listing of software will show only a subset of all the programs and libraries that are installed on the system. In order to view and access a larger set of programs and libraries, you need to load one of the <code>PDC</code> modules.</p> <pre><code># Loads the most recent PDC module\nml PDC\n# Loads a specific PDC module\nml PDC/23.12\n</code></pre> <p>where 23.12 is the most recent version as of May 2024.</p>"},{"location":"software/module/#what-softwares-are-installed","title":"What softwares are installed","text":"<p>At PDC we install many softwares and to examine what softwares are available for you. Both of these commands are case insentitive.</p> <pre><code>module avail &lt;SOFTWARE&gt;\n# For short\nml avail &lt;SOFTWARE&gt;\n</code></pre> <p>You can also use\u2026</p> <pre><code>module spider &lt;SOFTWARE&gt;\n# For short\nml spider &lt;SOFTWARE&gt;\n</code></pre>"},{"location":"software/module/#what-softwares-are-present-in-my-environment","title":"What softwares are present in my environment","text":"<p>In order to list the softwares in your environment.</p> <pre><code>module list\n# For short\nml\n</code></pre>"},{"location":"software/module/#how-to-manage-software-into-your-environment","title":"How to manage software into your environment","text":"<p>Softwares can be loaded into your environment if you have found which one you prefer. Do remember that this command is case sensitive so you need to spell  with the appropriate lowercase/uppercase letters. <pre><code>module load &lt;SOFTWARE&gt;[/&lt;VERSION&gt;]\n# For short\nml &lt;SOFTWARE&gt;[/&lt;VERSION&gt;]\n</code></pre>"},{"location":"software/module/#attention","title":"ATTENTION","text":"<p>In the old module.tcl on Cray environments you had to use swap for going from one compiler wrapper to the one you prefer. This is not needed on Dardel as it will be handled automatically using the load command.</p> <p>To remove a software from your environment. Do remember that this command is case sensitive so you need to spell  with the appropriate lowercase/uppercase letters. <pre><code>module unload &lt;SOFTWARE&gt;[/&lt;VERSION&gt;]\n# For short\nml -&lt;SOFTWARE&gt;[/&lt;VERSION&gt;]\n</code></pre>"},{"location":"software/module/#what-parameters-are-set-by-a-specific-module","title":"What parameters are set by a specific module","text":"<p>To list what parameters, pathes, variables are set when loading a specific software.</p> <pre><code>module show &lt;SOFTWARE&gt;[/&lt;VERSION&gt;]\n# For short\nml show &lt;SOFTWARE&gt;[/&lt;VERSION&gt;]\n</code></pre>"},{"location":"software/singularity/","title":"Instructions for using singularity at PDC","text":"<p>Singularity enables users to have full control of their environment. Singularity containers can be used to package entire scientific workflows, software and libraries, and even data. This means that you don\u2019t have to ask your cluster admin to install anything for you - you can put it in a Singularity container and run. Did you already invest in Docker? The Singularity software can import your Docker images without having Docker installed or being a superuser. Need to share your code? Put it in a Singularity container and your collaborator won\u2019t have to go through the pain of installing missing dependencies. Do you need to run a different operating system entirely? You can \u201cswap out\u201d the operating system on your host for a different one within a Singularity container. As the user, you are in control of the extent to which your container interacts with its host. There can be seamless integration, or little to no communication at all.</p> <p>More information about singularity can be found at https://docs.sylabs.io/guides/latest/user-guide/</p>"},{"location":"software/singularity/#security","title":"Security","text":"<p>Singularity is more secure on a HPC than other similar solutions like docker or shifter. Read a comparison about them at http://geekyap.blogspot.se/2016/11/docker-vs-singularity-vs-shifter-in-hpc.html</p> <p>important to remember is that if you download images they should be trusted since any container you run will have full access tor your account and your data.</p> <p>The same goes for images you build yourself that they are built upon trusted images.</p>"},{"location":"software/singularity/#performance","title":"Performance","text":"<p>Executing software in containers is very efficient as well as you create a sandbox of all applications that you do need. Very little performance loss has been seen.</p>"},{"location":"software/singularity/#installation-of-singularity","title":"Installation of singularity","text":"<p>Singularity is installed as a nosuid on the cluster, meaning that you are unable to use singularity files, but are able to use singularity sandboxes instead. Otherwise there should be no difference. At PDC you cannot write within your containers while being logged in on the cluster. Operation for creating containers is better performed on your local computer.</p>"},{"location":"software/singularity/#how-to-use-singularity-on-your-local-computer","title":"How to use singularity on your local computer","text":""},{"location":"software/singularity/#how-to-install-singularity","title":"How to install singularity","text":"<p>Singularity can be installed on your computer, with root access, so you can build your own images. Installation instructions are available at\u2026 https://docs.sylabs.io/guides/latest/user-guide/quick_start.html#quick-installation-steps</p>"},{"location":"software/singularity/#download-images-from-singularity-hub","title":"Download images from singularity hub","text":"<p>You can find numerous images at https://singularity-hub.org/ Just download them directly to our file system and use them for your analysis. The build command in singularity, besides downloading the image, also converts the image to the latest singularity version.</p> <pre><code>singularity build --sandbox &lt;sandbox name&gt; shub://&lt;name of image&gt;\n</code></pre> <p>You can also use images from docker https://hub.docker.com</p> <pre><code>singularity build --sandbox &lt;sandbox name&gt; docker://&lt;name of image&gt;\n</code></pre>"},{"location":"software/singularity/#building-your-own-images","title":"Building your own images","text":"<p>Different Linux OS are available for you to download. At this time the best Ubuntu distribution is available as a docker image and can be downloaded</p> <pre><code>sudo singularity build --sandbox &lt;sandbox name&gt; docker://ubuntu:latest\n</code></pre> <p>Which will download this docker image and convert it into a sandbox.</p>"},{"location":"software/singularity/#building-your-own-image-from-recipes","title":"Building your own image from recipes","text":"<p>You can also create an image using a recipe from our PDC github. Singularity recipes with MPI support are available at https://github.com/PDC-support/PDC-SoftwareStack/tree/master/other/singularity In order to create these</p> <pre><code>sudo singularity build --sandbox &lt;sandbox name&gt; &lt;recipe name&gt;\n</code></pre> <p>Here is a full example on how to create an image from one of our recipes.</p> <pre><code>wget https://raw.githubusercontent.com/PDC-support/PDC-SoftwareStack/master/other/singularity/ubuntu-mpich-full.def\nsudo singularity build --sandbox ubuntu-full ubuntu-mpich-full.def\n</code></pre>"},{"location":"software/singularity/#test-singularity","title":"Test singularity","text":"<p>With this command you will automatically login into your singularity image shell.</p> <pre><code>singularity shell &lt;sandbox name&gt;\n</code></pre> <p>In the shell you can do the usual Linux shell commands.</p>"},{"location":"software/singularity/#install-software","title":"Install software","text":"<p>So now that you do have a sandbox you can start by installing software in the image. By login into the writable sandbox you can use all the commands that are normal in the operating system, and your internetaccess is also available in the container, so you can use wget or other commands to download data and softwares. By default on a writable sandbox you login with the user root, therefore this should be executed on your personal computer where you have root access.</p> <pre><code>sudo singularity shell -w &lt;sandbox name&gt;\n</code></pre> <p>Read mode: You can read/write to file system outside container and read inside container.</p> <p>write mode: You can read/write inside container. In write mode you are user ROOT, home folder: /root</p>"},{"location":"software/singularity/#installing-the-essentials-in-your-image","title":"Installing the essentials in your image","text":"<p>Although you have downloaded the latest OS, it still needs some basic software, compilers and libraries. This can either be installed using a recipe like explained above, or you can login into your image using write mode and install everything you need, for example updating a sandbox</p> <pre><code>sudo singularity shell -w ubuntu_sandbox\nSingularity&gt; apt-get install update\nSingularity&gt; exit\n</code></pre>"},{"location":"software/singularity/#copying-data-from-local-file-system-to-singularity-sandbox","title":"Copying data from local file system to singularity sandbox","text":"<p>You can copy data to your singularity sandbox in several ways. Either by adding your files into the /root folder in singularity, and then they will automatically be available in the /root folder in singularity.</p> <pre><code>sudo cp &lt;your file&gt; /root\n</code></pre> <p>You can also bind your folder in singularity. In order to do that you must bind a folder in singularity to your local file system. Also the folder in singularity must be created first. The following example\u2026</p> <ol> <li>creates a new folder in the sandbox</li> <li>binds a local folder to that folder</li> <li>logins into the container,</li> <li>copies  into your sandbox <pre><code>sudo singularity exec -w ubuntu_write mkdir &lt;singularity folder&gt;\nsudo singularity shell -B &lt;local folder&gt;:/root/&lt;singularity folder&gt; -w ubuntu_write/\nSingularity&gt; cp &lt;singularity folder&gt;/&lt;files&gt; .\n</code></pre>"},{"location":"software/singularity/#where-to-store-runtime-files","title":"Where to store runtime files","text":"<p>As the root folder you are login into when using a writable container is not available using exec from outside the container, if you plan to run software that you compiled yourself, you must put the executable somewhere that is accessible by PATH.  /usr/local/bin is a good example. Installed software can then be executed by</p> <pre><code>singularity exec &lt;sandbox name&gt; &lt;myexe&gt;\n</code></pre> <p>You can also add a software path in the containers runscript which is a shell file which will be executed when you run the container. The runscript file should be stored in /.singularity.d/ folder The script is then executed using</p> <pre><code>singularity run &lt;sandbox name&gt;\n</code></pre>"},{"location":"software/singularity/#storing-help","title":"Storing help","text":"<p>help documents on the image should be saved as runscript.help in the folder  /.singularity.d This can then be read by</p> <pre><code>singularity run-help &lt;sandbox name&gt;\n</code></pre>"},{"location":"software/singularity/#saving-your-sandbox-to-pdc-cluster","title":"Saving your sandbox to PDC cluster","text":"<p>When you are ready installing all the software, paths, folders you need to transfer your sandbox to the cluster. To achieve this, it is advisable to first compress it and then transfer it.</p> <pre><code>sudo tar czf &lt;sandbox name&gt;.tar.gz &lt;sandbox name&gt;\nscp &lt;sandbox name&gt;.tar.gz &lt;username&gt;@dardel.pdc.kth.se:/cfs/klemming/home/&lt;u&gt;/&lt;username&gt;\n# On Dardel\ntar xfp &lt;sandbox name&gt;.tar.gz\n</code></pre> <p>Another possibility is to transfer a  .SIF file to Dardel and convert it to a sandbox with the build command</p> <pre><code>scp &lt;name&gt;.sif &lt;username&gt;@dardel.pdc.kth.se:/cfs/klemming/home/&lt;u&gt;/&lt;username&gt;\n# On Dardel\nsingularity build --sandbox &lt;sandbox name&gt; &lt;name&gt;.sif\n</code></pre> <p>Do remember that you do need OpenMPI to execute your software in parallel on HPC systems. We provide images with OpenMPI installed in the PDC Hub, but you can also build your own.</p>"},{"location":"software/singularity/#how-to-remote-build-a-singularity-image-on-the-cluster","title":"How to remote build a singularity image on the cluster","text":"<p>Instead of running singularity on your local computer as a mean to create a singularity sandbox, you can instead use the remote build function with a singularity recipy. Examples of recipies are available at https://github.com/PDC-support/PDC-SoftwareStack/tree/master/other/singularity In the description below we use the free service from sylabs for creating sandboxes at PDC.</p>"},{"location":"software/singularity/#create-a-sylabs-account","title":"Create a sylabs account","text":"<p>In order to be able to get access to the remote building facility you need to create a free account at https://cloud.sylabs.io/builder</p>"},{"location":"software/singularity/#create-sylabs-token","title":"Create sylabs token","text":"<ol> <li>Login into sylabs https://cloud.sylabs.io/builder</li> <li>Press USERNAME -&gt; Access tokens</li> <li>Enter a name for your token and press Create Access Token</li> <li>Copy or download the token.</li> </ol>"},{"location":"software/singularity/#adding-an-access-token-to-the-cluster","title":"Adding an access token to the cluster","text":"<p>First of all login to the PDC cluster and access the module singularity</p> <pre><code>ml PDC\nml singularity\n</code></pre> <p>After this run command</p> <pre><code>singularity remote login\n</code></pre> <p>The first time you run this command on the cluster, it will save your access token to your profile. Running the same command again will erase the old token and you can add a new one.</p>"},{"location":"software/singularity/#remote-build-a-container-on-the-cluster","title":"Remote build a container on the cluster","text":"<p>After having a valid token you can build your singularity sandbox remotely using the command\u2026</p> <pre><code>singularity build --remote --sandbox &lt;sandbox name&gt; &lt;recipe name&gt;\n</code></pre>"},{"location":"software/singularity/#running-singularity-images-at-pdc","title":"Running singularity images at PDC","text":"<p>Singularity works on Dardel by loading the singularity module. There are however restrictions. As explained above you are not able to write into singularity container. You are however able to build singularity sandboxes from containers available online, but not from recipes.</p> <pre><code>singularity build --sandbox &lt;sandbox name&gt; docker://ubuntu:latest\n</code></pre> <p>Also, you are able to run, run-help, exec, shell, as well as other non-write commands, on singularity sandboxes at PDC.</p> <p>Below are a couple of example on how to run a singularity container on PDC systems. As the Lustre filesystem is not recognised by default it is important to bind said filesystem, which is done in the example scripts below.</p>"},{"location":"software/singularity/#ready-made-containers-at-pdc","title":"Ready made containers at PDC","text":"<p>PDC provides some default containers for testing and certain software which are placed at /pdc/software/sing_hub These can also be reached by invoking  $PDC_SHUB</p> <pre><code>ls $PDC_SHUB\n</code></pre> <p>As highlighted before these containers are built as a sandbox and can be run with our clusters. In order to get information about what they contain please run</p> <pre><code>singularity run-help $PDC_SHUB/&lt;sandbox name&gt;\n</code></pre> <p>Recipes on how several of these container are bult is available at https://github.com/PDC-support/PDC-SoftwareStack/tree/master/other/singularity</p>"},{"location":"software/singularity/#job-scripts","title":"Job scripts","text":"<p>Follow information on how to create Job scripts at this page, which is not covered below. Although here are a couple of examples on how to run jobs on the dardel cluster.</p>"},{"location":"software/singularity/#batch-job-without-mpi","title":"Batch job without MPI","text":"<p>In case you would like to run on one node you do not need MPI support in your image and you can send in a job using\u2026</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A 201X X XX\n# The name of the script is myjob\n#SBATCH  J myjob\n# Only 1 hour wall clock time will be given to this job\n#SBATCH  t 1 00 00\n# Number of nodes\n#SBATCH   nodes=1\n# Using the shared partition as we are not using all cores\n#SBATCH  p shared\n# Number of MPI processes per node\n#SBATCH   ntasks per node=24\n# Run the executable named myexe\nml PDC singularity\nsrun -n 24 singularity exec -B /cfs/klemming &lt;sandbox folder&gt; &lt;myexe&gt;\n</code></pre>"},{"location":"software/singularity/#batchjob-with-mpi","title":"Batchjob with MPI","text":"<p>In case you need to parallelize your software across nodes you should use one of the recipes with Cray MPI support mentioned earlier which do reside in the https://github.com/PDC-support/PDC-SoftwareStack</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A 201X X XX\n# The name of the script is myjob\n#SBATCH  J myjob\n# Only 1 hour wall clock time will be given to this job\n#SBATCH  t 1 00 00\n# Number of nodes\n#SBATCH   nodes=2\n# Using the shared partition as we are not using all cores\n#SBATCH  p shared\n# Number of MPI processes per node\n#SBATCH   ntasks per node=12\n# Run the executable named myexe\nml PDC singularity\nsrun -n 24 --mpi=pmi2 singularity exec -B /cfs/klemming &lt;sandbox folder&gt; &lt;myexe&gt;\n</code></pre>"},{"location":"software/singularity/#batch-job-with-gpu-support","title":"Batch job with GPU support","text":"<p>In case you would like to run on one node with AMD GPUs. Be aware that you need a container with software compilated using GPU support. Look at our https://github.com/PDC-support/PDC-SoftwareStack for recipes on how to build. in your image and you can send in a job using\u2026</p> <pre><code>#! bin bash  l\n# The  l above is required to get the full environment with modules\n# Set the allocation to be charged for this job\n# not required if you have set a default allocation\n#SBATCH  A 201X X XX\n# The name of the script is myjob\n#SBATCH  J myjob\n# Only 1 hour wall clock time will be given to this job\n#SBATCH  t 1 00 00\n# Number of nodes\n#SBATCH   nodes=1\n# Using the GPU partition which is at the moment is under testing\n#SBATCH  p gpu\n# Run the executable named myexe\nml PDC singularity\nsrun -n 1 singularity exec --rocm -B /cfs/klemming &lt;sandbox folder&gt; &lt;myexe&gt;\n</code></pre>"},{"location":"software_development/development/","title":"Compilers and libraries","text":""},{"location":"software_development/development/#the-cray-programming-environment","title":"The Cray Programming Environment","text":"<p>The Cray Programming Environment (CPE) provides consistent interface to multiple compilers and libraries. On Dardel you can load the <code>cpe</code> module to enable a specific version of the CPE. For example</p> <pre><code>module load cpe/23.12\n</code></pre> <p>The <code>cpe</code> module will make sure that the corresponding versions of several other Cray libraries are loaded, such as <code>cray-libsci</code> and <code>cray-mpich</code>. You can check the details by <code>module show cpe/23.12</code>.</p> <p>In addition to the <code>cpe</code> module, there are also the <code>PrgEnv-</code> modules that provide compilers for different programming environment</p> <ul> <li><code>PrgEnv-cray</code>: loads the Cray compiling environment (CCE) that provides compilers for Cray systems.</li> <li><code>PrgEnv-gnu</code>: loads the GNU compiler suite.</li> <li><code>PrgEnv-aocc</code>: loads the AMD AOCC compilers.</li> </ul> <p>By default the <code>PrgEnv-cray</code> is loaded upon login. You can switch to different compilers using <code>module swap</code>:</p> <pre><code>module swap PrgEnv-cray PrgEnv-gnu\nmodule swap PrgEnv-gnu PrgEnv-aocc\n</code></pre>"},{"location":"software_development/development/#compiler-wrappers","title":"Compiler wrappers","text":"<p>After loading the <code>cpe</code> and the <code>PrgEnv-</code> modules, you can now build your parallel applications using compiler wrappers for C, C++ and Fortran:</p> <pre><code>cc -o myexe.x mycode.c      # cc is the wrapper for C compiler\nCC -o myexe.x mycode.cpp    # CC is the wrapper for C++ compiler\nftn -o myexe.x mycode.f90   # ftn is the wrapper for Fortran compiler\n</code></pre> <p>The compiler wrappers will choose the required compiler version, target architecture options, and will automatically link to the scientific libraries, as well as the MPI and OpenSHMEM libraries. No additional MPI flags are needed as these are included by compiler wrappers, and there is no need to add any <code>-I</code>, <code>-l</code> or <code>-L</code> flags for the Cray provided libraries. For libraries and include files covered by module files, you need not add anything to your Makefile. If a Makefile needs an input for -L to work correctly, try using \u201c<code>.</code>\u201d.</p> <p>For code development, testing, and performance analysis, it is good practice to build code with two different tool chains. On Dardel a starting point is to use the <code>PrgEnv-cray</code> and the <code>PrgEnv-gnu</code> environments.</p>"},{"location":"software_development/development/#cray-scientific-and-math-libraries","title":"Cray scientific and math libraries","text":"<p>The Cray scientific and math libraries (CSML) provide the <code>cray-libsci</code> and <code>cray-fftw</code> modules that are designed to provide optimial performance from Cray systems.</p> <ul> <li><code>cray-libsci</code>: provides BLAS, LAPACK, ScaLAPACK, etc.</li> <li><code>cray-fftw</code>: provides fastest fourier transform.</li> </ul> <p>The <code>cray-libsci</code> module supports OpenMP and the number of threads can be controlled by the <code>OMP_NUM_THREADS</code> environment variable.</p> <p>The <code>cray-libsci</code> module is loaded upon login, and its version can be changed by the <code>cpe</code> module. The <code>cray-fftw</code> module needs to be loaded by user.</p>"},{"location":"software_development/development/#cray-message-passing-toolkit","title":"Cray message passing toolkit","text":"<p>The Cray message passing toolkit (CMPT) provides the <code>cray-mpich</code> module, which is based on ANL MPICH and has been optimized for Cray programming environment.</p> <p>The <code>cray-mpich</code> module is loaded upon login, and its version can be changed by the <code>cpe</code> module. Once <code>cray-mpich</code> is loaded the compiler wrapper will automatically include MPI headers and link to MPI libraries.</p> <p>If you would like to use SHMEM you can check the availability of the <code>cray-openshmemx</code> module by \u201c<code>module avail cray-openshmemx</code>\u201d.</p>"},{"location":"software_development/development/#compiler-and-linker-flags","title":"Compiler and linker flags","text":"<p>Verbose printing of the flags and settings that are active when using the compiler wrappers</p> <pre><code>-craype-verbose\n</code></pre> <p>A suggested starting point for code optimization on AMD EPYC Zen 2 processors are</p> <ul> <li>for the Cray compilers</li> </ul> <pre><code># C C++ flags\n-Ofast              # Aggresive optimization\n-flto               # link time optimization\n-ffp=3              # optimization of floating-point math operations. Supported values are 0, 1, 2, 3, and 4.\n-fcray-mallopt      # use Cray's mallopt parameters, can improve performance\n-fno-cray-mallopt   # no use of Cray's mallopt parameters, can reduce memory usage\n-fopenmp            # enable OpenMP\n\n# Fortran flags\n-02                 # default optimization\n-O3                 # aggresive optimization\n-O ipaN             # level of inline expansion N=0-5, default N=3\n-hlist=a            # write optimization info to listing file\n-hlist=a            # create source listing with loopmark information\n-homp               # enable OpenMP\n-hthread            # level of optimization of OpenMP directive, N=0-3, default N=2\n</code></pre> <ul> <li>for the GCC compilers</li> </ul> <pre><code># General flags\n\n# C C++  Fortran flags\n-O3                 # aggresive optimization\n-march=znver2       # name of the target architecture\n-mtune=znver2       # name of the target processor for which code performance will be tuned\n-mfma               # enable fma instructions\n-mavx2              # enable avx2 instructions\n-m3dnow             # enable 3dnow instructions\n-fomit-frame-pointer  # omit the frame pointer in functions that do not need one\n-fopenmp            # enable OpenMP\n\n# Fortran flags\n-std=legacy         # specify legacy Fortran standard\n-fallow-argument-mismatch  # allow for mismatches between calls and procedure definitions\n</code></pre> <ul> <li>for the AOCC compilers</li> </ul> <pre><code># C C++ Fortran flags\n-02                 # default optimization\n-O3                 # aggresive optimization\n-O ipaN             # level of inline expansion N=0-5, default N=3\n-flto               # link time optimization\n-funroll-loops      # loop unrolling\n-unroll-aggressive  # advance loop optimization\n-fopenmp            # enable OpenMP\n\n# Fortran flags\n-ffree-form         # support for free form Fortran\n</code></pre>"},{"location":"software_development/development/#build-examples","title":"Build examples","text":"<p>Example 1: Build an MPI parallelized Fortran code within the PrgEnv-cray environment</p> <p>In this example we build and test run a Hello World code <code>hello_world_mpi.f90</code>.</p> <pre><code>program hello_world_mpi\ninclude \"mpif.h\"\ninteger myrank,size,ierr\ncall MPI_Init(ierr)\ncall MPI_Comm_rank(MPI_COMM_WORLD,myrank,ierr)\ncall MPI_Comm_size(MPI_COMM_WORLD,size,ierr)\nwrite(*,*) \"Processor \",myrank,\" of \",size,\": Hello World!\"\ncall MPI_Finalize(ierr)\nend program\n</code></pre> <p>The build is done within the PrgEnv-cray environment using the Cray Fortran compiler, and the testing is done on a Dardel CPU node reserved for interactive use.</p> <pre><code># Check which compiler the compiler wrapper is pointing to\nftn --version\n# returns Cray Fortran   Version 17 0 0\n\n# Compile the code\nftn hello_world_mpi.f90 -o hello_world_mpi.x\n\n# Test the code in interactive session \n# First queue to get one node reserved for 10 minutes\nsalloc -N 1 -t 0:10:00 -A &lt;project name&gt; -p main\n# wait for the node  Then run the program using 128 MPI ranks with\nsrun -n 128 ./hello_world_mpi.x\n# with program output to standard out\n#    \n# Processor  123  of  128   Hello World\n#    \n# Processor  47  of  128   Hello World\n#    \n</code></pre> <p>Having here used the ftn compiler wrapper, the linking to the cray-mpich library was done without the need to specify linking flags. As is expected for this code, in runtime each MPI rank is writing its Hello World to standard output without any synchronization with the other ranks.</p> <p>Example 2: Build a C code with PrgEnv-gnu. The code requires linking to a Fourier transform library.</p> <pre><code># Download a C program that illustrates use of the FFTW library\nmkdir fftw_test\ncd fftw_test\nwget https://people.math.sc.edu/Burkardt/c_src/fftw/fftw_test.c\n\n# Change from the PrgEnv cray to the PrgEnv gnu environment\nml PDC/23.12\nml cpeGNU/23.12\n# Lmod is automatically replacing \"cpeGNU 23 12\" with \"PrgEnv gnu 8 5 0\" \n# Lmod is automatically replacing \"cce 17 0 0\" with \"gcc 12 3\" \n# Lmod is automatically replacing \"PrgEnv cray 8 5 0\" with \"cpeGNU 23 12\" \n# Due to MODULEPATH changes  the following have been reloaded \n# 1  cray libsci 23 12 5     2  cray mpich 8 1 28\n\n# Check which compiler the cc compiler wrapper is pointing to\ncc --version\ngcc-12 (SUSE Linux) 12.3.0\n\nml list\n# The listing reveals that cray libsci 23 02 1 1 is already loaded \n\n# In addition  the program needs linking also to a Fourier transform library \nml spider fftw\n# gives a listing of available Fourier transform libraries \n# Load a recent version of the Cray FFTW library with\nmodule add cray-fftw/3.3.10.6\n\n# Build the code with\ncc fftw_test.c -o fftw_test.x\n\n# Test the code in interactive session \n# First queue to get one reserved core for 10 minutes\nsalloc -n 1 -t 0:10:00 -A &lt;project name&gt; -p shared\n# wait for the core  Then run the program with\nsrun -n 1 ./fftw_test.x\n</code></pre> <p>Having loaded the cray-fftw module, no additional linking flag(s) was needed for the cc compiler wrapper.</p> <p>Example 3: Build a program with the EasyBuild cpeGNU/23.12 toolchain</p> <pre><code># Load an EasyBuild user module\nml PDC/23.12\nml easybuild-user/4.9.1\n\n# Look for a recipe for the Libxc library\neb -S Libxc\n# Returns a list of available EasyBuild easyconfig files \n# Choose an easyconfig file for the cpeGNU 23 12 toolchain \n\n# Make a dry run\neb libxc-6.2.2-cpeGNU-23.12.eb --robot --dry-run\n\n# Check if dry run looks reasonable  Then proceed to build with\neb libxc-6.2.2-cpeGNU-23.12.eb --robot\n\n# The program is now locally installed in the user s\n# ~  local easybuild directory and can be loaded with\nml PDC/23.12\nml easybuild-user/4.9.1\nml libxc/6.2.2-cpeGNU-23.12\n</code></pre>"},{"location":"software_development/development/#references","title":"References","text":"<p>HPE Cray user manuals and reference information</p> <p>The Cray programming environment (CPE)</p> <p>HPE Cray Programming Environment User Guide</p> <p>HPE Cray reference information</p> <p>HPE Cray Clang C and C++ Quick Reference (13.0) (S-2179)</p> <p>HPE Cray Fortran Reference Manual (13.0) (S-3901)</p> <p>HPE Performance Analysis Tools User Guide</p> <p>References on AMD processors</p> <p>AMD Optimizing C/C++ and Fortran Compilers (AOCC)</p> <p>Using MKL efficiently</p> <p>Best practice Guide AMD EPYC</p> <p>AMD EPYC product line</p> <p>AMD EPYC wiki page</p>"},{"location":"software_development/development_allineaforge/","title":"Allinea Forge","text":"<p>Allinea tools can be used for debugging and performance analysis. More information at https://www.pdc.kth.se/software/software/allinea-forge/index_general.html</p>"},{"location":"software_development/development_allineaforge/#arm-allinea-map","title":"ARM  Allinea  map","text":""},{"location":"software_development/development_example/","title":"Downloadable example for compiling and submitting","text":"<p>This is a simple example showing how you can compile and submit parallel jobs on our clusters. For instructions please read the \u00efncluded README file. The example can be donwloaded from https://github.com/PDC-support/introduction-to-pdc/tree/master/example</p>"},{"location":"software_development/development_gpu/","title":"Building for AMD GPUs","text":""},{"location":"software_development/development_gpu/#the-amd-rocm-development-platform","title":"The AMD ROCm development platform","text":"<p>The AMD Radeon Open Compute (ROCm) platform is a software stack for programming and running of programs on GPUs. The ROCm platform has support for different programming models such as heterogeneous interface for portability (HIP), offloading to GPU with OpenMP directives, and the SYCL programming model.</p> <p>Programs on Dardel are installed using a specific Cray Parallel Environment (CPE). The main version of the Cray Parallel Environment on Dardel is currently 23.12 which can be loaded with</p> <pre><code>ml PDC/23.12\n</code></pre> <p>To load the ROCm module version 5.7.0 and set the accelerator target to amd-gfx90a (AMD MI250X GPU)</p> <pre><code>ml rocm/5.7.0\nml craype-accel-amd-gfx90a\n</code></pre> <p>Programs can then be built with different toolchains (Cray, Gnu, AOCC), as are available in the different versions of the Cray Programming Environments Compilers and libraries.</p> <p>For running programs as batch jobs on the GPU nodes, see job script example 6 on Job script examples.</p>"},{"location":"software_development/development_gpu/#compiler-and-linker-flags-environment-variables","title":"Compiler and linker flags environment variables","text":"<p>For executables that are built with the compilers of the Cray Compiler Environment (CCE), verbose runtime information can be enabled with the environment variable <code>CRAY_ACC_DEBUG</code> which takes values 1, 2 or 3. For the highest level of information</p> <pre><code>export CRAY_ACC_DEBUG=3\n</code></pre>"},{"location":"software_development/development_gpu/#build-and-run-examples","title":"Build and run examples","text":"<p>Example 1: Build and run a C++ code with offloading to GPU with HIP</p> <p>In this example we build and test run a Hello World C++ code in which offloading to GPU is done with the heterogeneous interface for portability (HIP). The program is built with the AMD hipcc compiler.</p> <pre><code># Download the source code\nwget https://raw.githubusercontent.com/PDC-support/introduction-to-pdc/master/example/hello_world_gpu.cpp\n\n# Load the ROCm module and set the accelerator target to amd gfx90a  AMD MI250X GPU \nml rocm/5.7.0\nml craype-accel-amd-gfx90a\n\n# We use the AMD hipcc compiler  Check the full path of the command hipcc\nwhich hipcc\n# returns\n/opt/rocm-5.7.0/bin/hipcc\n\n# Compile the code on the login node\nhipcc --offload-arch=gfx90a hello_world_gpu.cpp -o hello_world_gpu.x\n\n# Test the code in an interactive session \n# First queue to get one GPU node reserved for 10 minutes\nsalloc -N 1 -t 0:10:00 -A &lt;project name&gt; -p gpu\n# wait for a node \n\n# then run the program\nsrun -n 1 ./hello_world_gpu.x\n\n# with program output to standard out\nYou can access GPU devices: 0-7\nGPU 0: hello world\n...\n</code></pre> <p>Example 2: Build and run a Fortran code with offloading to GPU with OpenMP</p> <p>In this example we build and test run a Fortran program that calculates the dot product of two long vectors by means of offloading to GPU with OpenMP. The build is done within the PrgEnv-cray environment using the Cray Compiler Environment.</p> <pre><code># Download the source code\nwget https://github.com/ENCCS/openmp-gpu/raw/main/content/exercise/ex04/solution/ex04.F90\n\n# Load the ROCm module and set the accelerator target to amd gfx90a  AMD MI250X GPU \nml rocm/5.7.0\nml craype-accel-amd-gfx90a\n\n# Check which compiler the compiler wrapper is pointing to\nftn --version\n# returns\nCray Fortran : Version 17.0.0\n\n# Compile the code on the login node\nftn -fopenmp ex04.F90 -o ex04.x\n\n# Test the code in interactive session \n# First queue to get one GPU node reserved for 10 minutes\nsalloc -N 1 -t 0:10:00 -A &lt;project name&gt; -p gpu\n# wait for a node \n\n# then run the program\nsrun -n 1 ./ex04.x\n\n# with program output to standard out\nThe sum is:  1.25\n\n# Alternatively  login to the node with  for example \nssh nid002792\n# where nid002792 is one of the Dardel GPU nodes \n\n# Load the rocm module\nml rocm/5.7.0\n\n# then run the program\n./ex04.x\n\n# with program output to standard out\nThe sum is:  1.25\n\n# For CCE build executables  enable verbose runtime information on\n# the offloading to GPU with the environment variable\nexport CRAY_ACC_DEBUG=3\n\n# When rerunning the program\n./ex04.x\n\n# a detailed listing of data transfer to and from the host memory to the\n# device memory is displayed\nACC: Version 5.0 of HIP already initialized, runtime version 50013601\nACC: Get Device 0\n...\n...\nACC: End transfer (to acc 0 bytes, to host 4 bytes)\nACC:\nThe sum is:  1.25\nACC: __tgt_unregister_lib\n</code></pre>"},{"location":"software_development/development_gpu/#references-general-information","title":"References  general information","text":"<p>AMD ROCm Information Portal</p> <p>ENCCS and AMD training material for ROCm</p> <p>LUMI software development</p> <p>LUMI training materials</p> <p>Frontier user guide</p> <p>AMD Instinct product line</p> <p>AMD Instinct Wikipedia page</p> <p>ENCCS general GPU programming course</p>"},{"location":"software_development/development_gpu/#introductory-videos-from-amd","title":"Introductory videos from AMD","text":"<p>Introduction to HIP Programming</p> <p>Introduction to AMD GPU Hardware</p> <p>GPU Programming Concepts</p> <p>GPU Programming Software</p> <p>Porting CUDA to HIP</p>"},{"location":"software_development/development_gpu/#heterogeneous-interface-for-portability-hip","title":"Heterogeneous interface for portability  HIP","text":"<p>PRACE training GPU Programming with HIP</p> <p>AMD\u2019s HIP Programming Guide</p>"},{"location":"software_development/development_gpu/#openmp","title":"OpenMP","text":"<p>Michael Klemm, Intro to GPU Programming with the OpenMP API (2021-10-20)</p> <p>AMD\u2019s ROCm documentation, chapter OpenMP support</p> <p>ENCCS and CSC, OpenMP for GPU offloading</p>"},{"location":"software_development/development_gpu/#sycl","title":"SYCL","text":"<p>Codeplay\u2019s introduction to SYCL (videos)</p> <p>Introduction to SYCL</p> <p>Topology Discovery and Queue Creation</p> <p>SYCL Kernel Functions</p> <p>Managing Data in SYCL</p> <p>ENCCS workshop, Heterogeneous programming with SYCL</p> <p>Aksel Alpay, Universit\u00e4t Heidelberg, SYCL Tutorial: An Introduction to hipSYCL (video)</p> <p>hipSYCL blog, benchmarking hipSYCL with HeCBench on AMD hardware (2022-07-20)</p>"},{"location":"software_development/development_references/","title":"References","text":"<p>The Cray programming environment (CPE)</p> <p>Using MKL efficiently</p> <p>Best practice Guide AMD EPYC</p> <p>AMD EPYC product line</p> <p>AMD EPYC wiki page</p>"},{"location":"software_development/development_sycl/","title":"Development sycl","text":"<p>Example 3: Build and run a C++ code with offloading to GPU with SYCL</p> <p>In this example we build and test run a SYCL C++ program that performs axpy arithmetic operations. The build is done within the PrgEnv-gnu environment using the syclcc compiler and the CMake build system.</p> <pre><code># Download the source code and its accompanying CMakeLists txt\nwget https://raw.githubusercontent.com/ENCCS/sycl-workshop/main/content/code/day-1/06_axpy-usm/solution/axpy.cpp\nwget https://raw.githubusercontent.com/ENCCS/sycl-workshop/main/content/code/day-1/06_axpy-usm/solution/CMakeLists.txt\n\n# Load the AdaptiveCpp module and set the accelerator target to amd gfx90a  AMD MI250X GPU \nml craype-accel-amd-gfx90a\nml PDC/23.03\nml adaptivecpp/23.10.0-cpeGNU-23.03-rocm-5.3.3-llvm\nml cmake/3.27.7\n\n# We use the syclcc compiler together with CMake  Check the full path of the command syclcc\nwhich syclcc\n# returns\n/pdc/software/23.03/eb/software/adaptivecpp/23.10.0-cpeGNU-23.03-rocm-5.3.3-llvm/bin/syclcc\n\n# Configure and compile the code with Cmake\nmkdir build\ncd build\ncmake ..\nmake\n\n**Note:** If needed, replace in ``CMakeLists.txt`` the line ``find_package(hipSYCL CONFIG REQUIRED)``\nwith the line ``find_package(AdaptiveCpp CONFIG REQUIRED)``.\n\n# Test the code in interactive session \n# First queue to get one GPU node reserved for 10 minutes\nsalloc -N 1 -t 0:10:00 -A &lt;project name&gt; -p gpu\n# wait for a node  Then login to the node with  for example \nssh nid002860\n# where nid002860 is one of the Dardel GPU nodes \n\n# Load the PDC and hipsycl modules\nml PDC/23.03\nml adaptivecpp/23.10.0-cpeGNU-23.03-rocm-5.3.3-llvm\n\n# then run the program\n./axpy\n\n# with program output to standard out\nRunning on:\nChecking results...\nNice job!\n</code></pre>"},{"location":"software_development/easybuild/","title":"Installing software using EasyBuild","text":"<p>EasyBuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. EasyBuild is available on PDC clusters to facilitate the system wide installation of software, or to install software in your home folder. You can find more information about Easybuild at https://easybuild.io/ and all documentations for easybuild reside at https://docs.easybuild.io/en/latest/</p> <p>There are two versions of EasyBuild installed. The easybuild-prod module is only intended for users with privileged access to PDC clusters. The easybuild-user module is intended for users to make local installs within their home folders.</p>"},{"location":"software_development/easybuild/#for-local-installations","title":"For local installations","text":"<p>This module will facilitate the installation of the desired software into your home folder and will build software and modules into  ~/.local/easybuild/ by default. If you would like to store your easybuild installation elsewhere please set its environment variable prior of loading the easybuild-user module. At this time this works only for easybuild/4.9.1 and PDC/23.12.</p> <pre><code>export EB_USER_PREFIX=&lt;MyPath&gt;\n</code></pre> <p>To activate easybuild-user.</p> <pre><code>ml PDC/23.12\nml easybuild-user/4.9.1\n</code></pre>"},{"location":"software_development/easybuild/#how-is-easybuild-configured","title":"How is EasyBuild configured","text":"<p>Easybuild module at PDC is configured to take advantage of existent local resources.</p> <ul> <li>Temporary files stored in  /tmp</li> <li>EasyBuild searches for easyconfig recipes locally and   has access to many recipes from various HPC centers.</li> <li>EasyBuild searches for easyblocks and   has access to many recipes from various HPC centers.</li> <li>EasyBuild includes toolchains that are specific to the resources at hand</li> </ul> <p>You can look up the current configuration of EasyBuild by\u2026</p> <pre><code>eb --show-config\n</code></pre>"},{"location":"software_development/easybuild/#how-to-install-software-using-easybuild","title":"How to install software using EasyBuild","text":"<p>Files ending with eb are called easyconfig files and are used as recipes for installation. In general their names have the format</p> <pre><code>eb &lt;software&gt;-&lt;version&gt;-&lt;toolchain&gt;-&lt;version&gt;.eb\n</code></pre>"},{"location":"software_development/easybuild/#dry-run-installation-of-software","title":"dry run installation of software","text":"<p>Performing a dry-run is a handy procedure for testing the installation of a software</p> <pre><code>eb boost-1.83.0-cpeGNU-23.12.eb --dry-run\n</code></pre> <p>You can also use  -x,\u2013extended-dry-run for more information.</p>"},{"location":"software_development/easybuild/#how-to-search-for-software-using-easybuild","title":"How to search for software using EasyBuild","text":"<p>The installation at PDC includes many easyconfigs that can be searched for and use as a base for new easyconfig recipes. On Dardel git repositories of easyconfigs can be found within the directory</p> <pre><code>/pdc/software/eb_repo\n</code></pre> <p>The PDC-Software-stack is also hosted as a public git repository on https://github.com/PDC-support/PDC-SoftwareStack. In order to search for easyconfigs for a specific program, use the command</p> <pre><code>eb -S  &lt;software&gt;\n</code></pre> <p>where   is the case insensitive name of the program that you would like to install. For example, for a listing of available easyconfigs for the molecular dynamics program GROMACS, use <pre><code>eb -S gromacs\nCFGS1=/pdc/software/eb_repo/PDC-SoftwareStack/easybuild/easyconfigs/g\nCFGS2=/pdc/software/eb_repo/CSCS-production/easybuild/easyconfigs/g/GROMACS\n* $CFGS1/GROMACS-2020.5-cpeCray-21.09.eb\n* $CFGS1/GROMACS-2021.3-cpeCray-21.09.eb\n* $CFGS1/GROMACS-2021.3-cpeCray-21.11.eb\n* $CFGS1/GROMACS-2022-beta1-cpeCray-21.09.eb\n* $CFGS2/GROMACS-2018.6-CrayGNU-20.08-cuda-pat.eb\n* $CFGS2/GROMACS-2018.6-CrayGNU-20.08-cuda.eb\n...\n</code></pre> <p>Also you can directly install these softwares by just entering the easyconfig name</p>"},{"location":"software_development/easybuild/#how-install-dependent-software","title":"How install dependent software","text":"<p>In many cases a specific installation has many dependencies. These dependencies can be automatically installed using easyconfigs that are available in robot paths</p> <pre><code>eb boost-1.83.0-cpeGNU-23.12.eb --robot\n</code></pre> <p>To check what dependencies are missing</p> <pre><code>eb boost-1.83.0-cpeGNU-23.12.eb --missing\n</code></pre>"},{"location":"software_development/easybuild/#how-to-build-easyconfig-files","title":"How to build easyconfig files","text":"<p>The easyconfig file is central for installing a software with EasyBuild. This is a short tutorial on how to achieve that. More information can be found at https://docs.easybuild.io/en/latest/Writing_easyconfig_files.html</p>"},{"location":"software_development/easybuild/#parameters","title":"Parameters","text":"<p>Parameters are good as variables to provide information for EasyBuild. A full overview of all known easyconfig parameter can be acquired using</p> <pre><code>eb --avail-easyconfig-params\n</code></pre> <p>More information can be found at https://docs.easybuild.io/en/latest/version-specific/easyconfig_parameters.html#vsd-avail-easyconfig-params</p>"},{"location":"software_development/easybuild/#templates","title":"Templates","text":"<p>A set of templates that can be used in easyconfig files and function as small commands.</p> <pre><code>eb --avail-easyconfig-templates\n</code></pre> <p>More information can be found at https://docs.easybuild.io/en/latest/version-specific/easyconfig_templates.html#avail-easyconfig-templates</p>"},{"location":"software_development/easybuild/#name","title":"Name","text":"<p>This specifies the name and version of the software. The created module will be named accordingly.</p> <pre><code>name = 'Blast+'\nversion = '2.12.0'\nhomepage = 'https://blast.ncbi.nlm.nih.gov/'\ndescription = \"\"\"Blast for searching sequences\"\"\"\n</code></pre>"},{"location":"software_development/easybuild/#toolchains","title":"Toolchains","text":"<p>A toolchain provides information for EasyBuild about compilers libraries etc\u2026 The idea is to compose and maintain a limited set of specific compiler toolchains. At PDC we have several toolchains that should be used in case you would like to parallelize your software</p> <ul> <li>cpeCray</li> <li>cpeGNU</li> <li>cpeAMD</li> </ul> <p>In your easyconfig file you enter this information using the following command.</p> <pre><code>toolchain = {'name': 'cpeGNU', 'version': '23.12'}\n</code></pre> <p>Remember that what toolchain should be used when building your software will also have an impact on the dependencies your installation has.</p> <p>For small software or supporting libraries it is not always important that you use the cpeXXX toolchains but are happy with using whatever system toolchain there is.</p> <pre><code>toolchain = SYSTEM\n</code></pre>"},{"location":"software_development/easybuild/#sources","title":"Sources","text":"<p>Specify where EasyBuild can download your source code. Also look at templates described earlier for functions to simplify this process.</p> <pre><code>sources = [{\n    'source_urls': ['https://example.com'],\n    'filename': '%(name)s-%(version)s.tar.gz',\n    'extract_cmd': \"tar xf %s\",  # Optional\n}]\n</code></pre>"},{"location":"software_development/easybuild/#easyblock","title":"Easyblock","text":"<p>An easyblock is a python code to address special procedure for the installation. For example, adresses that you should first run configure &gt; make &gt; make install or cmake &gt; make &gt; make install</p> <pre><code>easyblock = 'type'\n</code></pre> <p>Many EasyBlock are generic as to describe standard installation patterns. Easyconfigs without an easyblock entry are specific and Easybuild will search for EasyBlocks named EB_[software] in the local EasyBuild repository.</p> <p>To find which easyblock are available for you\u2026</p> <pre><code>eb --list-easyblocks\n</code></pre> <p>See information about which generic easyblock are available at https://docs.easybuild.io/en/latest/version-specific/generic_easyblocks.html</p>"},{"location":"software_development/easybuild/#dependencies","title":"Dependencies","text":"<p>Will be taken into consideration during the installation procedure if an easyconfig is found or if a module already exists</p> <p>Ordinarily a dependency will follow whatever main application toolchain has been defined</p> <pre><code>dependencies = [\n    ('Software', 'version'),\n]\n</code></pre> <p>If you, however, would like to build a specific dependency with the SYSTEM toolchain, the format is as follows\u2026</p> <pre><code>dependencies = [\n    ('Software', 'version', '', ('system', '')),\n]\n</code></pre>"},{"location":"software_development/easybuild/#sanity-check","title":"Sanity check","text":"<p>Prior of finalizing the installation, EasyBuild performs a sanity check to ensure that everything was installed correctly.</p> <pre><code>sanity_check_paths = {\n    'files': ['bin/reframe',\n              'share/completions/file1',\n              'share/completions/file2'],\n    'dirs': ['bin', 'lib', 'share', 'tutorials']\n}\nsanity_check_commands = [\n    'software --version',\n    'software --help',\n]\n</code></pre>"},{"location":"software_development/easybuild/#final-results","title":"Final results","text":"<p>After the installation is completed, EasyBuild will have built the software in question and provided a module for it.</p>"},{"location":"software_development/spack/","title":"Installing software using Spack","text":"<p>Spack is a package manager for supercomputers. It can be used for installing scientific software and libraries for your own software development project in an easy way.</p> <p>Below you can find short information on how to use spack at PDC, but the full documentation for spack is available online at https://spack.readthedocs.io/en/latest/index.html</p> <p>Please note that while there is a central Spack installation used by PDC staff to install software in <code>/pdc/software/...</code>, you cannot use this yourself to install software as it is write-protected. The Spack philosophy of working is instead that you work with your own Spack installation yourself in your private space. It is possible to link your own installation to the PDC central one and reduce the number of package you need to compile by using the so-called \u201cchaining\u201d functionality, see more information here https://spack.readthedocs.io/en/latest/chain.html.</p>"},{"location":"software_development/spack/#setting-the-environment","title":"Setting the environment","text":"<p>This module will facilitate the installation of the desired software into your home folder and will build software and modules into  ~/.local/spack/ by default. If you would like to store your easybuild installation elsewhere please set its environment variable prior of loading the spack-user module.</p> <pre><code>export SPACK_USER_PREFIX=&lt;MyPath&gt;\n</code></pre> <p>To activate spack-user.</p> <pre><code>ml PDC/23.12\nml spack-user/0.21.2\n</code></pre>"},{"location":"software_development/spack/#finding-and-listing-available-software","title":"Finding and listing available software","text":"<p>If you would like to find out what software you can install, use the <code>list</code> command:</p> <pre><code>$ spack list kokkos\n==&gt; 6 packages.\nkokkos  kokkos-kernels  kokkos-kernels-legacy  kokkos-legacy  kokkos-nvcc-wrapper  py-pykokkos-base\n</code></pre> <p>More details about a specific packages, such as configuration flags and build options can be seen with <code>spack info</code></p> <pre><code>$ spack info kokkos\n$ spack info kokkos\nCMakePackage:   kokkos\n\nDescription:\n    Kokkos implements a programming model in C++ for writing performance\n    portable applications targeting all major HPC platforms.\n\nHomepage: https://github.com/kokkos/kokkos\n\nMaintainers: @DavidPoliakoff @jciesko\n\nExternally Detectable:\n    False\n\nTags:\n    e4s\n\nPreferred version:\n    3.4.01     https://github.com/kokkos/kokkos/archive/3.4.01.tar.gz\n...\n</code></pre>"},{"location":"software_development/spack/#installing-software-using-spack_1","title":"Installing software using spack","text":"<p>Installing software using spack in the most basic way is simply:</p> <pre><code>spack install &lt;software&gt;\n</code></pre> <p>If there are other packages or libraries that needs to be installed for this software to work, they are also installed automatically. In general, though, you would like to be more specific, and request a certain version of a software, or maybe a specific compiler to be used. There is a specific syntax for doing that:</p> Command Option @ Which version to install/use % Which compiler to use ^ Add a specific dependency <p>This is an example of an installation of Amber 18 where it is compiled with GCC version 9.3.0 and a specific ncurses package</p> <pre><code>spack install amber@18%gcc@9.3.0^ncurses@6.2\n...\n==&gt; amber: Successfully installed amber-18-js3n7jolbdh7gknbfuiwf5vutoaljckd\nFetch: 1.09s.  Build: 46.92s.  Total: 48.01s.\n[+] $HOME/spack/opt/spack/cray-sles15-zen2/gcc-9.3.0/amber-18-js3n7jolbdh7gknbfuiwf5vutoaljckd\n</code></pre> <p>It is often useful to do a dry run before installing to see what Spack thinks it needs to install. This can be done with <code>spec</code> command. For example, what will happen when we request LINPACK?</p> <pre><code>$ spack spec -I hpl\nInput spec\n--------------------------------\n-   hpl\n\nConcretized\n--------------------------------\n-   hpl@2.3%gcc@11.2.0~openmp arch=cray-sles15-zen2\n[+]      ^cray-libsci@21.08.1.2%gcc@11.2.0~mpi~openmp+shared arch=cray-sles15-zen2\n[+]      ^mpich@8.1.11%gcc@11.2.0~argobots+fortran+hwloc+hydra+libxml2+pci+romio~slurm~two_level_namespace~verbs+wrapperrpath device=ch4 netmod=ofi pmi=pmi arch=cray-sles15-zen2\n</code></pre> <p>Here Spack will compile and install LINPACK using gcc version 11.2.0, MPICH for MPI, and Cray\u2019s LibSci for BLAS and LAPCK. Only the LINPACK package itself will be compiled. Cray Libsci and MPICH are already installed, as indicated by the <code>[+]</code> symbols at the beginning of the lines.</p>"},{"location":"software_development/spack/#installing-non-downloadable-software","title":"Installing non downloadable software","text":"<p>In some cases it is not possible to directly download the software you are interested in as the software is restricted by license or other.</p> <p>In this case you have to download it manually and create folder for it. See instructions at https://spack.readthedocs.io/en/latest/basic_usage.html#non-downloadable-tarballs</p>"},{"location":"software_development/spack/#garbage-collection","title":"Garbage collection","text":"<p>Installing and uninstalling softwares will in the end use up your disk space so it is good practice to do some garbage collection</p> <pre><code>spack gc\n</code></pre>"},{"location":"software_development/spack/#how-to-execute-your-software","title":"How to execute your software","text":"<p>Software installed by Spack are put in the directory <code>spack/opt/spack/cray-sles15-zen2/&lt;compiler&gt;/&lt;software&gt;</code>. This can be a bit tedious to use every time, so there are two ways to make it more convenient. Spack automatically makes module files for all installed software, which shows up when you do <code>module avail</code>, provided that you can run the Spack initilization script mentioned above. You can then use the <code>module load</code> command in the way as you would do for other software at PDC, but in this case you load a software from your own Spack installation.</p> <pre><code>$ module avail hpl\n\n--------------------------- /cfs/klemming/scratch/y/ypetla/spack/spack/share/spack/modules/cray-sles15-zen2 ---------------------------\nhpl-2.3-gcc-11.2.0-5v2u3sq\n</code></pre> <p>Alternatively, you can use Spack\u2019s built-in mechanism for loading software packages, which works similarily to <code>module load</code>:</p> <pre><code>$ spack load hpl\n$ which xhpl\n/cfs/klemming/pdc/software/dardel/21.11/spack/spack/opt/spack/cray-sles15-zen2/gcc-11.2.0/hpl-2.3-5v2u3sqa3huu2djhxdtjxbfaoysdwfpy/bin/xhpl\n</code></pre> <p>\u2026and the software will be directly available via your command line.</p>"}]}